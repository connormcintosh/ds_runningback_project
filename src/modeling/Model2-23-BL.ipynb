{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../../data/processed/norookies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('year',axis=1)\n",
    "df = df.drop('playername',axis=1)\n",
    "df = df.drop('team',axis=1)\n",
    "df = df.drop('basesalarycap (10^8)',axis=1)\n",
    "df = df.drop('cashspent (M)',axis=1)\n",
    "df = df.drop('caphit (M)',axis=1)\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.drop('Unnamed: 0.1',axis=1)\n",
    "df = df.drop('catchpercent',axis=1)\n",
    "df = df.drop('draft_yr',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>age</th>\n",
       "      <th>draft_pos</th>\n",
       "      <th>attempts</th>\n",
       "      <th>yards_run</th>\n",
       "      <th>tds_run</th>\n",
       "      <th>longgain_run</th>\n",
       "      <th>yardsperatt</th>\n",
       "      <th>yardspergame_run</th>\n",
       "      <th>Percenthit (%)</th>\n",
       "      <th>...</th>\n",
       "      <th>tds_rec</th>\n",
       "      <th>firstdowns</th>\n",
       "      <th>longgain_rec</th>\n",
       "      <th>yardspertarget</th>\n",
       "      <th>recpergame</th>\n",
       "      <th>yardspergame_rec</th>\n",
       "      <th>fumbles</th>\n",
       "      <th>team_adjusted_line_yards</th>\n",
       "      <th>team_running_back_yards</th>\n",
       "      <th>team_stuffed_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>182</td>\n",
       "      <td>75</td>\n",
       "      <td>389</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>5.2</td>\n",
       "      <td>77.8</td>\n",
       "      <td>1.100909</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>314</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>3.9</td>\n",
       "      <td>52.3</td>\n",
       "      <td>0.529768</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>211</td>\n",
       "      <td>898</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.3</td>\n",
       "      <td>59.9</td>\n",
       "      <td>0.945802</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>251</td>\n",
       "      <td>1042</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>4.2</td>\n",
       "      <td>65.1</td>\n",
       "      <td>0.355530</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.728473</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>327</td>\n",
       "      <td>1485</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "      <td>4.5</td>\n",
       "      <td>92.8</td>\n",
       "      <td>10.748185</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.9</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3.6</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9.268023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "      <td>1266</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>4.5</td>\n",
       "      <td>90.4</td>\n",
       "      <td>11.300813</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>348</td>\n",
       "      <td>2097</td>\n",
       "      <td>12</td>\n",
       "      <td>82</td>\n",
       "      <td>6.0</td>\n",
       "      <td>131.1</td>\n",
       "      <td>10.534826</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>208</td>\n",
       "      <td>970</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "      <td>4.7</td>\n",
       "      <td>80.8</td>\n",
       "      <td>10.587500</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>250</td>\n",
       "      <td>31</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2.7</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.239555</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>250</td>\n",
       "      <td>90</td>\n",
       "      <td>425</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>4.7</td>\n",
       "      <td>42.5</td>\n",
       "      <td>0.365679</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>250</td>\n",
       "      <td>41</td>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>4.5</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.559959</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>250</td>\n",
       "      <td>221</td>\n",
       "      <td>1015</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>4.6</td>\n",
       "      <td>72.5</td>\n",
       "      <td>3.731343</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>17.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>250</td>\n",
       "      <td>171</td>\n",
       "      <td>659</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>3.9</td>\n",
       "      <td>54.9</td>\n",
       "      <td>2.291667</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>22.3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>181</td>\n",
       "      <td>150</td>\n",
       "      <td>499</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>0.406321</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>181</td>\n",
       "      <td>71</td>\n",
       "      <td>262</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>3.7</td>\n",
       "      <td>23.8</td>\n",
       "      <td>0.430828</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>75</td>\n",
       "      <td>364</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>4.9</td>\n",
       "      <td>60.7</td>\n",
       "      <td>2.056606</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>58.7</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.447780</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>54</td>\n",
       "      <td>23</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.390542</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44</td>\n",
       "      <td>27</td>\n",
       "      <td>129</td>\n",
       "      <td>139</td>\n",
       "      <td>492</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>3.5</td>\n",
       "      <td>61.5</td>\n",
       "      <td>1.644715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>12.9</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>129</td>\n",
       "      <td>73</td>\n",
       "      <td>385</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>5.3</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.342405</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>47</td>\n",
       "      <td>27</td>\n",
       "      <td>187</td>\n",
       "      <td>34</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.092913</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>49</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5.1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.522704</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>60</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>220</td>\n",
       "      <td>756</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>3.4</td>\n",
       "      <td>47.3</td>\n",
       "      <td>2.642276</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>278</td>\n",
       "      <td>1094</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>3.9</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.487562</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>62</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>181</td>\n",
       "      <td>667</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>3.7</td>\n",
       "      <td>41.7</td>\n",
       "      <td>1.529167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>84</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.460637</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>69</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>80</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>4.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.755832</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>126</td>\n",
       "      <td>178</td>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>4.3</td>\n",
       "      <td>51.5</td>\n",
       "      <td>2.774451</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>529</td>\n",
       "      <td>27</td>\n",
       "      <td>194</td>\n",
       "      <td>51</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>4.8</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.625471</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>9.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>530</td>\n",
       "      <td>25</td>\n",
       "      <td>194</td>\n",
       "      <td>214</td>\n",
       "      <td>921</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>4.3</td>\n",
       "      <td>65.8</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>31.9</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>533</td>\n",
       "      <td>29</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.355530</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>534</td>\n",
       "      <td>26</td>\n",
       "      <td>73</td>\n",
       "      <td>36</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.765546</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>535</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "      <td>94</td>\n",
       "      <td>340</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>3.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.706579</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>539</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>190</td>\n",
       "      <td>707</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>3.7</td>\n",
       "      <td>47.1</td>\n",
       "      <td>3.132832</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>540</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>157</td>\n",
       "      <td>543</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>45.3</td>\n",
       "      <td>2.371273</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>541</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>258</td>\n",
       "      <td>1045</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>4.1</td>\n",
       "      <td>65.3</td>\n",
       "      <td>7.379076</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>542</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>260</td>\n",
       "      <td>1145</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>4.4</td>\n",
       "      <td>76.3</td>\n",
       "      <td>7.582638</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>22.2</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>543</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>324</td>\n",
       "      <td>1416</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>4.4</td>\n",
       "      <td>94.4</td>\n",
       "      <td>6.015582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>544</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>253</td>\n",
       "      <td>1042</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "      <td>4.1</td>\n",
       "      <td>86.8</td>\n",
       "      <td>2.628591</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>31.6</td>\n",
       "      <td>5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>545</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>237</td>\n",
       "      <td>1002</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>4.2</td>\n",
       "      <td>83.5</td>\n",
       "      <td>1.112385</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>22.6</td>\n",
       "      <td>5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>549</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.690755</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>20.7</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>550</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>104</td>\n",
       "      <td>414</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>1.061883</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>34.8</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>554</td>\n",
       "      <td>25</td>\n",
       "      <td>119</td>\n",
       "      <td>14</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5.3</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1.534455</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>558</td>\n",
       "      <td>27</td>\n",
       "      <td>73</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.456483</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>559</td>\n",
       "      <td>26</td>\n",
       "      <td>73</td>\n",
       "      <td>137</td>\n",
       "      <td>544</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.9</td>\n",
       "      <td>1.912859</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>560</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "      <td>167</td>\n",
       "      <td>800</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>4.8</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.551884</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>564</td>\n",
       "      <td>27</td>\n",
       "      <td>199</td>\n",
       "      <td>40</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12.2</td>\n",
       "      <td>2.334932</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>27.4</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>565</td>\n",
       "      <td>26</td>\n",
       "      <td>199</td>\n",
       "      <td>84</td>\n",
       "      <td>286</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>3.4</td>\n",
       "      <td>17.9</td>\n",
       "      <td>1.437126</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>63</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>566</td>\n",
       "      <td>25</td>\n",
       "      <td>199</td>\n",
       "      <td>92</td>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3.9</td>\n",
       "      <td>35.7</td>\n",
       "      <td>1.149771</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>573</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>133</td>\n",
       "      <td>548</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4.1</td>\n",
       "      <td>34.3</td>\n",
       "      <td>0.540993</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>574</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>96</td>\n",
       "      <td>375</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>3.9</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.216154</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>575</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>122</td>\n",
       "      <td>485</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>2.774975</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>576</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>223</td>\n",
       "      <td>857</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>3.8</td>\n",
       "      <td>57.1</td>\n",
       "      <td>4.888417</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>13.8</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>577</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>1251</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>4.9</td>\n",
       "      <td>89.4</td>\n",
       "      <td>4.079179</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>56</td>\n",
       "      <td>7.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>41.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>590</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.195122</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>591</td>\n",
       "      <td>26</td>\n",
       "      <td>140</td>\n",
       "      <td>25</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>4.4</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.469179</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>595</td>\n",
       "      <td>25</td>\n",
       "      <td>153</td>\n",
       "      <td>22</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.382572</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>599</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>138</td>\n",
       "      <td>377</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.398135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1.1  age  draft_pos  attempts  yards_run  tds_run  \\\n",
       "0                 0   26        182        75        389        5   \n",
       "1                 4   35          7        80        314        2   \n",
       "2                 5   34          7       211        898        5   \n",
       "3                 6   33          7       251       1042        7   \n",
       "4                 8   31          7        37         72        0   \n",
       "5                 9   30          7       327       1485       11   \n",
       "6                10   29          7        21         75        0   \n",
       "7                11   28          7       279       1266       10   \n",
       "8                12   27          7       348       2097       12   \n",
       "9                13   26          7       208        970       12   \n",
       "10               17   29        250        31         85        0   \n",
       "11               18   28        250        90        425        2   \n",
       "12               19   27        250        41        186        2   \n",
       "13               20   26        250       221       1015        6   \n",
       "14               21   25        250       171        659        9   \n",
       "15               30   27        181       150        499        2   \n",
       "16               31   26        181        71        262        1   \n",
       "17               35   25         67        75        364        4   \n",
       "18               39   27         54         2          7        0   \n",
       "19               40   26         54        23        115        0   \n",
       "20               44   27        129       139        492        3   \n",
       "21               45   26        129        73        385        8   \n",
       "22               47   27        187        34         96        0   \n",
       "23               53   25         -1        49        248        1   \n",
       "24               60   28         -1       220        756        7   \n",
       "25               61   27         -1       278       1094        6   \n",
       "26               62   26         -1       181        667       11   \n",
       "27               65   24         84         6         11        0   \n",
       "28               69   30        126        80        343        0   \n",
       "29               70   29        126       178        772        5   \n",
       "..              ...  ...        ...       ...        ...      ...   \n",
       "267             529   27        194        51        246        2   \n",
       "268             530   25        194       214        921        3   \n",
       "269             533   29         73        29         80        1   \n",
       "270             534   26         73        36         90        0   \n",
       "271             535   25         73        94        340        2   \n",
       "272             539   31         24       190        707        6   \n",
       "273             540   30         24       157        543        6   \n",
       "274             541   29         24       258       1045        4   \n",
       "275             542   28         24       260       1145        5   \n",
       "276             543   26         24       324       1416        4   \n",
       "277             544   25         24       253       1042        7   \n",
       "278             545   24         24       237       1002        5   \n",
       "279             549   26         36        17         63        0   \n",
       "280             550   25         36       104        414        1   \n",
       "281             554   25        119        14         74        0   \n",
       "282             558   27         73        18         30        0   \n",
       "283             559   26         73       137        544        6   \n",
       "284             560   25         73       167        800        4   \n",
       "285             564   27        199        40        171        0   \n",
       "286             565   26        199        84        286        3   \n",
       "287             566   25        199        92        357        1   \n",
       "288             573   30         -1       133        548        4   \n",
       "289             574   29         -1        96        375        4   \n",
       "290             575   26         10       122        485        7   \n",
       "291             576   25         10       223        857       12   \n",
       "292             577   24         10       256       1251       17   \n",
       "293             590   32         -1        11         23        0   \n",
       "294             591   26        140        25        110        1   \n",
       "295             595   25        153        22         81        0   \n",
       "296             599   32         23       138        377        2   \n",
       "\n",
       "     longgain_run  yardsperatt  yardspergame_run  Percenthit (%)  \\\n",
       "0              75          5.2              77.8        1.100909   \n",
       "1              27          3.9              52.3        0.529768   \n",
       "2              32          4.3              59.9        0.945802   \n",
       "3              90          4.2              65.1        0.355530   \n",
       "4              13          1.9              24.0        7.728473   \n",
       "5              80          4.5              92.8       10.748185   \n",
       "6              17          3.6              75.0        9.268023   \n",
       "7              78          4.5              90.4       11.300813   \n",
       "8              82          6.0             131.1       10.534826   \n",
       "9              54          4.7              80.8       10.587500   \n",
       "10             23          2.7              14.2        0.239555   \n",
       "11             29          4.7              42.5        0.365679   \n",
       "12             27          4.5              62.0        1.559959   \n",
       "13             37          4.6              72.5        3.731343   \n",
       "14             37          3.9              54.9        2.291667   \n",
       "15             17          3.3              31.2        0.406321   \n",
       "16             48          3.7              23.8        0.430828   \n",
       "17             49          4.9              60.7        2.056606   \n",
       "18              4          3.5               1.2        0.447780   \n",
       "19             15          5.0               7.2        0.390542   \n",
       "20             23          3.5              61.5        1.644715   \n",
       "21             31          5.3              38.5        0.342405   \n",
       "22             13          2.8               6.0        1.092913   \n",
       "23             13          5.1              62.0        2.522704   \n",
       "24             25          3.4              47.3        2.642276   \n",
       "25             48          3.9              72.9        2.487562   \n",
       "26             18          3.7              41.7        1.529167   \n",
       "27              4          1.8               1.6        0.460637   \n",
       "28             38          4.3              49.0        2.755832   \n",
       "29             75          4.3              51.5        2.774451   \n",
       "..            ...          ...               ...             ...   \n",
       "267            34          4.8              18.9        0.625471   \n",
       "268            46          4.3              65.8        0.617204   \n",
       "269            12          2.8               8.0        0.355530   \n",
       "270            13          2.5              11.3        0.765546   \n",
       "271            43          3.6              56.7        0.706579   \n",
       "272            55          3.7              47.1        3.132832   \n",
       "273            50          3.5              45.3        2.371273   \n",
       "274            46          4.1              65.3        7.379076   \n",
       "275            47          4.4              76.3        7.582638   \n",
       "276            58          4.4              94.4        6.015582   \n",
       "277            56          4.1              86.8        2.628591   \n",
       "278            54          4.2              83.5        1.112385   \n",
       "279            12          3.7              10.5        0.690755   \n",
       "280            20          4.0              29.6        1.061883   \n",
       "281            17          5.3              24.7        1.534455   \n",
       "282            10          1.7              15.0        2.456483   \n",
       "283            48          4.0              38.9        1.912859   \n",
       "284            65          4.8              50.0        0.551884   \n",
       "285            19          4.3              12.2        2.334932   \n",
       "286            21          3.4              17.9        1.437126   \n",
       "287            42          3.9              35.7        1.149771   \n",
       "288            30          4.1              34.3        0.540993   \n",
       "289            26          3.9              46.9        0.216154   \n",
       "290            35          4.0              69.3        2.774975   \n",
       "291            25          3.8              57.1        4.888417   \n",
       "292            36          4.9              89.4        4.079179   \n",
       "293             6          2.1               1.4        1.195122   \n",
       "294            26          4.4              18.3        0.469179   \n",
       "295            17          3.7               5.4        0.382572   \n",
       "296            16          2.7              31.4        0.398135   \n",
       "\n",
       "           ...          tds_rec  firstdowns  longgain_rec  yardspertarget  \\\n",
       "0          ...                2          12            30             5.8   \n",
       "1          ...                0           3            18             4.9   \n",
       "2          ...                0           7            22             6.2   \n",
       "3          ...                1           9            52             8.0   \n",
       "4          ...                0           0             7             1.3   \n",
       "5          ...                0           9            49             6.2   \n",
       "6          ...                0           1             9             6.0   \n",
       "7          ...                1          11            22             4.3   \n",
       "8          ...                1          11            20             4.3   \n",
       "9          ...                1           5            22             5.8   \n",
       "10         ...                3           5            10             4.6   \n",
       "11         ...                6          20            22             6.4   \n",
       "12         ...                0           3            14             5.3   \n",
       "13         ...                0           7            59             7.9   \n",
       "14         ...                2          12            26             6.1   \n",
       "15         ...                0           4            28             5.7   \n",
       "16         ...                0           4            14             6.0   \n",
       "17         ...                4          29            52             7.8   \n",
       "18         ...                1           1            22            11.5   \n",
       "19         ...                1           5            16             4.2   \n",
       "20         ...                0           3            14             3.6   \n",
       "21         ...                0           2            17             5.1   \n",
       "22         ...                0           5            16             4.5   \n",
       "23         ...                0          10            28             6.9   \n",
       "24         ...                0           1            10             2.8   \n",
       "25         ...                0           4            13             3.5   \n",
       "26         ...                0           7            53            12.2   \n",
       "27         ...                0           0             6             1.4   \n",
       "28         ...                1           3            28             6.1   \n",
       "29         ...                0           9            31             5.2   \n",
       "..         ...              ...         ...           ...             ...   \n",
       "267        ...                0          10            31             9.7   \n",
       "268        ...                2          17            46            10.6   \n",
       "269        ...                0           1            10             4.5   \n",
       "270        ...                0           0             3            -0.4   \n",
       "271        ...                0           0             7             4.0   \n",
       "272        ...                0           6            17             5.5   \n",
       "273        ...                1          11            25             3.9   \n",
       "274        ...                0          13            22             6.1   \n",
       "275        ...                1          17            50             5.7   \n",
       "276        ...                0          16            38             4.3   \n",
       "277        ...                1          15            53             6.1   \n",
       "278        ...                1          13            37             5.2   \n",
       "279        ...                0           6            23             8.3   \n",
       "280        ...                4          20            37             6.2   \n",
       "281        ...                0           1            20             4.6   \n",
       "282        ...                0           2            18             8.5   \n",
       "283        ...                1           9            37             6.0   \n",
       "284        ...                5          15            39             6.3   \n",
       "285        ...                0          16            20             5.2   \n",
       "286        ...                2          20            63             6.3   \n",
       "287        ...                5          20            23             5.5   \n",
       "288        ...                1          11            50             7.7   \n",
       "289        ...                0           7            27             9.9   \n",
       "290        ...                0           3            14             3.4   \n",
       "291        ...                2           8            23             4.2   \n",
       "292        ...                4          27            56             7.2   \n",
       "293        ...                1           3            12             2.4   \n",
       "294        ...                0           3            18             3.4   \n",
       "295        ...                0           4            18             4.9   \n",
       "296        ...                0           0             9             1.8   \n",
       "\n",
       "     recpergame  yardspergame_rec  fumbles  team_adjusted_line_yards  \\\n",
       "0           3.7              29.0        2                       5.0   \n",
       "1           1.0               7.1        0                      19.0   \n",
       "2           1.1               9.5        3                      18.0   \n",
       "3           1.3              13.0        3                      26.0   \n",
       "4           1.0               2.7        1                      30.0   \n",
       "5           1.9              13.9        7                      10.0   \n",
       "6           2.0              18.0        0                      13.0   \n",
       "7           2.1              12.2        5                      10.0   \n",
       "8           2.5              13.6        4                      10.0   \n",
       "9           1.5              11.6        1                      18.0   \n",
       "10          1.7              10.7        0                      27.0   \n",
       "11          3.8              30.0        3                      16.0   \n",
       "12          2.3              14.0        0                      15.0   \n",
       "13          1.6              17.5        3                       2.0   \n",
       "14          2.8              22.3        1                      29.0   \n",
       "15          1.3               9.6        0                      27.0   \n",
       "16          0.6               4.9        0                      20.0   \n",
       "17          6.2              58.7        1                       4.0   \n",
       "18          0.2               2.1        0                       1.0   \n",
       "19          0.9               5.5        1                       7.0   \n",
       "20          2.5              12.9        3                      30.0   \n",
       "21          1.2               8.6        0                       2.0   \n",
       "22          0.8               5.3        0                       7.0   \n",
       "23          5.6              45.8        0                      29.0   \n",
       "24          0.3               1.4        2                      11.0   \n",
       "25          1.5               6.9        3                      11.0   \n",
       "26          0.6               9.9        0                       2.0   \n",
       "27          0.4               1.0        0                      16.0   \n",
       "28          1.6              15.7        2                      32.0   \n",
       "29          1.5              11.3        1                      29.0   \n",
       "..          ...               ...      ...                       ...   \n",
       "267         1.5              17.2        0                      16.0   \n",
       "268         2.4              31.9        4                      17.0   \n",
       "269         0.3               1.8        2                      15.0   \n",
       "270         0.5              -0.3        0                      26.0   \n",
       "271         0.7               3.3        0                       5.0   \n",
       "272         1.3               9.9        0                      14.0   \n",
       "273         2.8              15.9        0                      24.0   \n",
       "274         2.4              20.1        0                      13.0   \n",
       "275         2.8              22.2        2                      30.0   \n",
       "276         3.4              21.5        2                      28.0   \n",
       "277         3.3              31.6        5                      29.0   \n",
       "278         3.2              22.6        5                      28.0   \n",
       "279         2.2              20.7        1                      15.0   \n",
       "280         3.9              34.8        1                      21.0   \n",
       "281         2.0              13.7        0                      25.0   \n",
       "282         1.0              11.3        0                      22.0   \n",
       "283         1.5              12.9        0                       8.0   \n",
       "284         2.0              17.3        2                      24.0   \n",
       "285         4.4              27.4        0                      20.0   \n",
       "286         3.3              27.8        1                      32.0   \n",
       "287         5.3              37.1        0                      31.0   \n",
       "288         1.4              12.5        0                       1.0   \n",
       "289         1.5              16.1        0                      13.0   \n",
       "290         1.6               8.2        2                      26.0   \n",
       "291         2.1              13.8        3                      19.0   \n",
       "292         4.2              41.4        1                       1.0   \n",
       "293         0.7               2.9        1                      32.0   \n",
       "294         1.6               7.2        0                      21.0   \n",
       "295         0.6               4.3        0                      18.0   \n",
       "296         0.7               1.7        1                      18.0   \n",
       "\n",
       "     team_running_back_yards  team_stuffed_rate  \n",
       "0                        3.0                1.0  \n",
       "1                       22.0               18.0  \n",
       "2                       15.0               17.0  \n",
       "3                       23.0               28.0  \n",
       "4                       32.0               26.0  \n",
       "5                        4.0               12.0  \n",
       "6                       15.0                2.0  \n",
       "7                        2.0               20.0  \n",
       "8                        1.0               29.0  \n",
       "9                        4.0               28.0  \n",
       "10                      31.0               24.0  \n",
       "11                      23.0               15.0  \n",
       "12                      20.0               10.0  \n",
       "13                       5.0               18.0  \n",
       "14                      31.0               15.0  \n",
       "15                      27.0               21.0  \n",
       "16                      24.0               18.0  \n",
       "17                       8.0                2.0  \n",
       "18                       5.0                6.0  \n",
       "19                       7.0                5.0  \n",
       "20                      30.0               31.0  \n",
       "21                       5.0               18.0  \n",
       "22                      14.0               20.0  \n",
       "23                      28.0               27.0  \n",
       "24                      27.0                9.0  \n",
       "25                      16.0                9.0  \n",
       "26                      21.0                7.0  \n",
       "27                      25.0               21.0  \n",
       "28                      26.0               32.0  \n",
       "29                      21.0               29.0  \n",
       "..                       ...                ...  \n",
       "267                      8.0               13.0  \n",
       "268                     22.0               10.0  \n",
       "269                     18.0                5.0  \n",
       "270                     15.0               26.0  \n",
       "271                     16.0               27.0  \n",
       "272                     21.0               20.0  \n",
       "273                     22.0               25.0  \n",
       "274                     14.0               21.0  \n",
       "275                     17.0               20.0  \n",
       "276                      9.0               28.0  \n",
       "277                     24.0               21.0  \n",
       "278                     21.0               27.0  \n",
       "279                     17.0               28.0  \n",
       "280                     32.0               19.0  \n",
       "281                     14.0               29.0  \n",
       "282                     17.0               31.0  \n",
       "283                      5.0               20.0  \n",
       "284                     14.0               31.0  \n",
       "285                     25.0               27.0  \n",
       "286                     31.0               31.0  \n",
       "287                     29.0               29.0  \n",
       "288                      9.0                2.0  \n",
       "289                     18.0               14.0  \n",
       "290                     29.0               16.0  \n",
       "291                     26.0               26.0  \n",
       "292                      1.0                2.0  \n",
       "293                     32.0               32.0  \n",
       "294                     24.0               14.0  \n",
       "295                     15.0               17.0  \n",
       "296                     29.0               18.0  \n",
       "\n",
       "[297 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1.1                int64\n",
       "age                           int64\n",
       "draft_pos                     int64\n",
       "attempts                      int64\n",
       "yards_run                     int64\n",
       "tds_run                       int64\n",
       "longgain_run                  int64\n",
       "yardsperatt                 float64\n",
       "yardspergame_run            float64\n",
       "Percenthit (%)              float64\n",
       "g                             int64\n",
       "gs                            int64\n",
       "tgt                           int64\n",
       "rec                           int64\n",
       "yards_rec                     int64\n",
       "yardsperrec                 float64\n",
       "tds_rec                       int64\n",
       "firstdowns                    int64\n",
       "longgain_rec                  int64\n",
       "yardspertarget              float64\n",
       "recpergame                  float64\n",
       "yardspergame_rec            float64\n",
       "fumbles                       int64\n",
       "team_adjusted_line_yards    float64\n",
       "team_running_back_yards     float64\n",
       "team_stuffed_rate           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('Percenthit (%)',axis=1).values\n",
    "y = df['Percenthit (%)'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 12.6333 - val_loss: 12.5045\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 187us/step - loss: 12.3045 - val_loss: 12.2219\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 168us/step - loss: 12.0674 - val_loss: 12.0446\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 11.9159 - val_loss: 11.9178\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 11.8134 - val_loss: 11.8452\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 11.7641 - val_loss: 11.7973\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 11.7256 - val_loss: 11.7588\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 11.6883 - val_loss: 11.7225\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 11.6516 - val_loss: 11.6867\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 184us/step - loss: 11.6143 - val_loss: 11.6503\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 11.5762 - val_loss: 11.6125\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 11.5358 - val_loss: 11.5734\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 11.4939 - val_loss: 11.5330\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 177us/step - loss: 11.4501 - val_loss: 11.4902\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 217us/step - loss: 11.4047 - val_loss: 11.4453\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 11.3574 - val_loss: 11.3991\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 11.3088 - val_loss: 11.3519\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 11.2560 - val_loss: 11.3044\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 11.2077 - val_loss: 11.2533\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 177us/step - loss: 11.1537 - val_loss: 11.2017\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 11.0967 - val_loss: 11.1503\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 11.0421 - val_loss: 11.0972\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 10.9864 - val_loss: 11.0429\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 10.9280 - val_loss: 10.9888\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 10.8715 - val_loss: 10.9334\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 10.8117 - val_loss: 10.8774\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 10.7513 - val_loss: 10.8205\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 10.6923 - val_loss: 10.7623\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 10.6306 - val_loss: 10.7037\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 10.5676 - val_loss: 10.6455\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 10.5048 - val_loss: 10.5859\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 176us/step - loss: 10.4430 - val_loss: 10.5234\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 189us/step - loss: 10.3748 - val_loss: 10.4620\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 10.3109 - val_loss: 10.4001\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 189us/step - loss: 10.2443 - val_loss: 10.3376\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 10.1785 - val_loss: 10.2741\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 182us/step - loss: 10.1088 - val_loss: 10.2110\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 10.0435 - val_loss: 10.1453\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 180us/step - loss: 9.9730 - val_loss: 10.0814\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 174us/step - loss: 9.9042 - val_loss: 10.0175\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 180us/step - loss: 9.8358 - val_loss: 9.9530\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 9.7663 - val_loss: 9.8869\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 9.6972 - val_loss: 9.8205\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 9.6267 - val_loss: 9.7550\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 9.5557 - val_loss: 9.6906\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 9.4863 - val_loss: 9.6255\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 9.4178 - val_loss: 9.5590\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 9.3465 - val_loss: 9.4936\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 9.2742 - val_loss: 9.4289\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 9.2071 - val_loss: 9.3627\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 197us/step - loss: 9.1346 - val_loss: 9.2981\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 9.0645 - val_loss: 9.2336\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 8.9933 - val_loss: 9.1687\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 8.9245 - val_loss: 9.1023\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 8.8536 - val_loss: 9.0354\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 8.7815 - val_loss: 8.9707\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 8.7107 - val_loss: 8.9062\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 8.6426 - val_loss: 8.8416\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 174us/step - loss: 8.5740 - val_loss: 8.7782\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 200us/step - loss: 8.5041 - val_loss: 8.7148\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 8.4360 - val_loss: 8.6500\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 8.3645 - val_loss: 8.5870\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 8.3013 - val_loss: 8.5205\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 187us/step - loss: 8.2303 - val_loss: 8.4591\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 8.1647 - val_loss: 8.3984\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 8.0970 - val_loss: 8.3419\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 8.0341 - val_loss: 8.2844\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 7.9668 - val_loss: 8.2174\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 7.9051 - val_loss: 8.1529\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 7.8421 - val_loss: 8.0928\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 7.7763 - val_loss: 8.0366\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 191us/step - loss: 7.7145 - val_loss: 7.9811\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 7.6528 - val_loss: 7.9262\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 7.5904 - val_loss: 7.8743\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 7.5311 - val_loss: 7.8229\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 7.4720 - val_loss: 7.7738\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 7.4110 - val_loss: 7.7088\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 144us/step - loss: 7.3537 - val_loss: 7.6496\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 7.2929 - val_loss: 7.6001\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 7.2383 - val_loss: 7.5527\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 7.1792 - val_loss: 7.5034\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 7.1215 - val_loss: 7.4500\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 7.0644 - val_loss: 7.4009\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 7.0131 - val_loss: 7.3440\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 86us/step - loss: 6.9559 - val_loss: 7.2947\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 6.8976 - val_loss: 7.2544\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 6.8437 - val_loss: 7.1877\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 6.7871 - val_loss: 7.1362\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 6.7317 - val_loss: 7.0928\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 6.6758 - val_loss: 7.0405\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 6.6181 - val_loss: 6.9804\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 6.5554 - val_loss: 6.8908\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 6.3849 - val_loss: 6.3178\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 5.5533 - val_loss: 5.3149\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 4.6273 - val_loss: 4.6725\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 4.0949 - val_loss: 4.2306\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 3.8689 - val_loss: 3.9941\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.8256 - val_loss: 3.8895\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.7205 - val_loss: 3.9929\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.6187 - val_loss: 3.7879\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.5930 - val_loss: 3.7308\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 3.5097 - val_loss: 3.8514\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.4617 - val_loss: 3.7258\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.4238 - val_loss: 3.6797\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.3898 - val_loss: 3.6947\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.3937 - val_loss: 3.7273\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.3529 - val_loss: 3.6438\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.3369 - val_loss: 3.6461\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.3424 - val_loss: 3.6844\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.3062 - val_loss: 3.6196\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.3053 - val_loss: 3.6192\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 3.2866 - val_loss: 3.5971\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 3.2577 - val_loss: 3.6102\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 77us/step - loss: 3.2548 - val_loss: 3.6103\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.2467 - val_loss: 3.5942\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.2418 - val_loss: 3.6029\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 3.1998 - val_loss: 3.5631\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 3.2209 - val_loss: 3.5715\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.1761 - val_loss: 3.6229\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.1822 - val_loss: 3.6099\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.1774 - val_loss: 3.5553\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.1468 - val_loss: 3.5782\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.1398 - val_loss: 3.6324\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.1244 - val_loss: 3.5857\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.1042 - val_loss: 3.5862\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.0827 - val_loss: 3.5749\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 3.0857 - val_loss: 3.5756\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.1138 - val_loss: 3.5522\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 3.0419 - val_loss: 3.6407\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.1426 - val_loss: 3.6568\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 3.0733 - val_loss: 3.5432\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.0363 - val_loss: 3.5662\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.0076 - val_loss: 3.6436\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.0195 - val_loss: 3.5765\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 3.0081 - val_loss: 3.5443\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.9890 - val_loss: 3.5962\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.9773 - val_loss: 3.5804\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.9530 - val_loss: 3.5870\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.9609 - val_loss: 3.5717\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 2.9353 - val_loss: 3.6131\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.9120 - val_loss: 3.5770\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.9405 - val_loss: 3.5774\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.8954 - val_loss: 3.6030\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.8843 - val_loss: 3.6283\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.8752 - val_loss: 3.6044\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.8626 - val_loss: 3.6092\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.8328 - val_loss: 3.6607\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 2.8498 - val_loss: 3.6505\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.8351 - val_loss: 3.6262\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.8195 - val_loss: 3.6589\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 2.8060 - val_loss: 3.6520\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.7968 - val_loss: 3.6526\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.7842 - val_loss: 3.6556\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.7704 - val_loss: 3.6528\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.7614 - val_loss: 3.6518\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 129us/step - loss: 2.7549 - val_loss: 3.6607\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.7296 - val_loss: 3.6930\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.7485 - val_loss: 3.6979\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.7227 - val_loss: 3.6785\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.7084 - val_loss: 3.6947\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.6991 - val_loss: 3.6949\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.7087 - val_loss: 3.6979\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.6845 - val_loss: 3.6802\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.6703 - val_loss: 3.6855\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.6755 - val_loss: 3.7089\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.6341 - val_loss: 3.6898\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.6423 - val_loss: 3.6856\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.6408 - val_loss: 3.6865\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.6181 - val_loss: 3.7114\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.6086 - val_loss: 3.7147\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.6064 - val_loss: 3.7107\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 2.6057 - val_loss: 3.7352\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.5826 - val_loss: 3.7249\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 2.5706 - val_loss: 3.7437\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.5738 - val_loss: 3.7395\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.5649 - val_loss: 3.7616\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 2.5532 - val_loss: 3.7400\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.5446 - val_loss: 3.7456\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.5233 - val_loss: 3.7443\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.5281 - val_loss: 3.7535\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 2.5406 - val_loss: 3.7638\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.4954 - val_loss: 3.7568\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.5344 - val_loss: 3.7631\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.5021 - val_loss: 3.8014\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.4951 - val_loss: 3.7677\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.4757 - val_loss: 3.7734\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.4741 - val_loss: 3.7823\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.4760 - val_loss: 3.7733\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.4745 - val_loss: 3.7796\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.4441 - val_loss: 3.8190\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.4515 - val_loss: 3.7987\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 2.4287 - val_loss: 3.8017\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 2.4240 - val_loss: 3.8185\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.4494 - val_loss: 3.8202\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.4196 - val_loss: 3.8253\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.4052 - val_loss: 3.8263\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.3926 - val_loss: 3.8266\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.3995 - val_loss: 3.8359\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.3920 - val_loss: 3.8602\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.3699 - val_loss: 3.8274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc060719fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train.shape\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(x=X_train, y= y_train, validation_data=(X_test,y_test),batch_size=32,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  3.827412426446154\n",
      "RMSE:  1.956377373219736\n",
      "MAE:  1.4612227869106509\n",
      "ESV:  0.3371118038296572\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \",mean_squared_error(y_test,predictions) )\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,predictions)) )\n",
    "print(\"MAE: \",mean_absolute_error(y_test,predictions) )\n",
    "print(\"ESV: \", explained_variance_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./savedmodels/57dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model('./savedmodels/57dnn/')\n",
    "# predictions = loaded_model.predict(X_test)\n",
    "# print(\"MSE: \",mean_squared_error(y_test,predictions) )\n",
    "# print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,predictions)) )\n",
    "# print(\"MAE: \",mean_absolute_error(y_test,predictions) )\n",
    "# print(\"ESV: \", explained_variance_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 10.7555 - val_loss: 10.3040\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 9.9433 - val_loss: 9.6097\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 9.1838 - val_loss: 8.9192\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 184us/step - loss: 8.4557 - val_loss: 8.2412\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 7.7175 - val_loss: 7.5765\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 171us/step - loss: 7.0316 - val_loss: 6.9420\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 6.3948 - val_loss: 6.3595\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 5.8114 - val_loss: 5.8467\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 5.3321 - val_loss: 5.4246\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 4.9760 - val_loss: 5.1305\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 4.6999 - val_loss: 4.9623\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 4.5498 - val_loss: 4.8727\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 4.4999 - val_loss: 4.8197\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 4.4882 - val_loss: 4.7891\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 4.4409 - val_loss: 4.7633\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 4.4150 - val_loss: 4.7382\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 4.3986 - val_loss: 4.7191\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 4.3691 - val_loss: 4.6967\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 4.3535 - val_loss: 4.6767\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 4.3276 - val_loss: 4.6435\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 4.3058 - val_loss: 4.6130\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 4.2792 - val_loss: 4.5892\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 4.2523 - val_loss: 4.5566\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 4.2309 - val_loss: 4.5267\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 4.2050 - val_loss: 4.4948\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 4.1816 - val_loss: 4.4641\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 4.1570 - val_loss: 4.4341\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 4.1287 - val_loss: 4.4079\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 4.1104 - val_loss: 4.3813\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 4.0757 - val_loss: 4.3501\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 4.0546 - val_loss: 4.3217\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 4.0276 - val_loss: 4.2970\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.9898 - val_loss: 4.2648\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.9694 - val_loss: 4.2345\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.9419 - val_loss: 4.2100\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.9215 - val_loss: 4.1866\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 3.8965 - val_loss: 4.1704\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.8753 - val_loss: 4.1481\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 174us/step - loss: 3.8516 - val_loss: 4.1304\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.8324 - val_loss: 4.1159\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.8123 - val_loss: 4.0929\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.7904 - val_loss: 4.0763\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 180us/step - loss: 3.7709 - val_loss: 4.0505\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 3.7557 - val_loss: 4.0369\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 3.7301 - val_loss: 4.0087\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.7104 - val_loss: 3.9826\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.7048 - val_loss: 3.9637\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 3.6863 - val_loss: 3.9507\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.6806 - val_loss: 3.9641\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.6515 - val_loss: 3.9609\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 3.6367 - val_loss: 3.9435\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.6171 - val_loss: 3.9152\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.6281 - val_loss: 3.8898\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.5966 - val_loss: 3.8918\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 3.5836 - val_loss: 3.8987\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.5740 - val_loss: 3.9023\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 3.5628 - val_loss: 3.8812\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.5487 - val_loss: 3.8652\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.5344 - val_loss: 3.8598\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.5309 - val_loss: 3.8511\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.5142 - val_loss: 3.8471\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.5086 - val_loss: 3.8411\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.4938 - val_loss: 3.8192\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 3.4854 - val_loss: 3.8062\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.4774 - val_loss: 3.8040\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.4729 - val_loss: 3.8065\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.4690 - val_loss: 3.7938\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 3.4744 - val_loss: 3.8153\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 3.4397 - val_loss: 3.7821\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.4367 - val_loss: 3.7630\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.4547 - val_loss: 3.7571\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 3.4180 - val_loss: 3.7756\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.4037 - val_loss: 3.8034\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 3.4178 - val_loss: 3.8076\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.4051 - val_loss: 3.7929\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 87us/step - loss: 3.3927 - val_loss: 3.7698\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.3977 - val_loss: 3.7390\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.3934 - val_loss: 3.7486\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 101us/step - loss: 3.3761 - val_loss: 3.7438\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.3670 - val_loss: 3.7502\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.3722 - val_loss: 3.7340\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.3495 - val_loss: 3.7568\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.3488 - val_loss: 3.7589\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.3441 - val_loss: 3.7636\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 3.3382 - val_loss: 3.7522\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 3.3354 - val_loss: 3.7257\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.3297 - val_loss: 3.7424\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.3152 - val_loss: 3.7346\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.3189 - val_loss: 3.7397\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.3109 - val_loss: 3.7270\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.3016 - val_loss: 3.7310\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 77us/step - loss: 3.2989 - val_loss: 3.7422\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.2899 - val_loss: 3.7278\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.2844 - val_loss: 3.7261\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 3.2825 - val_loss: 3.7393\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.2735 - val_loss: 3.7313\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.2885 - val_loss: 3.7059\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 3.2663 - val_loss: 3.7116\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 3.2521 - val_loss: 3.7372\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.2565 - val_loss: 3.7470\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.2504 - val_loss: 3.7503\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 3.2455 - val_loss: 3.7351\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 73us/step - loss: 3.2361 - val_loss: 3.7171\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 3.2301 - val_loss: 3.7211\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 3.2358 - val_loss: 3.7074\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 84us/step - loss: 3.2203 - val_loss: 3.7156\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 3.2173 - val_loss: 3.7282\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.2205 - val_loss: 3.7443\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.2105 - val_loss: 3.7079\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.2004 - val_loss: 3.7088\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.1955 - val_loss: 3.7247\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.1859 - val_loss: 3.7249\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.1794 - val_loss: 3.7239\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.1775 - val_loss: 3.7343\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 3.1670 - val_loss: 3.7144\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.1607 - val_loss: 3.7178\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.1555 - val_loss: 3.7237\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.1521 - val_loss: 3.7315\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.1384 - val_loss: 3.7173\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.1331 - val_loss: 3.7026\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.1331 - val_loss: 3.7028\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.1221 - val_loss: 3.7090\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.1263 - val_loss: 3.6956\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.1084 - val_loss: 3.6985\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.1016 - val_loss: 3.7074\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.0961 - val_loss: 3.7054\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 3.0854 - val_loss: 3.7084\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.0788 - val_loss: 3.7102\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 3.0763 - val_loss: 3.7015\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.0750 - val_loss: 3.7131\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.0613 - val_loss: 3.7137\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.0505 - val_loss: 3.7033\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 3.0518 - val_loss: 3.7006\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 3.0386 - val_loss: 3.7192\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 3.0348 - val_loss: 3.7226\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.0243 - val_loss: 3.7083\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 3.0197 - val_loss: 3.7114\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.0122 - val_loss: 3.7035\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.0048 - val_loss: 3.7056\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.0087 - val_loss: 3.7255\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.9989 - val_loss: 3.7036\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.9952 - val_loss: 3.6942\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.9748 - val_loss: 3.6984\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.9673 - val_loss: 3.6964\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.9602 - val_loss: 3.6919\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.9661 - val_loss: 3.7133\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.9529 - val_loss: 3.6843\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 2.9393 - val_loss: 3.6857\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.9283 - val_loss: 3.6803\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 2.9254 - val_loss: 3.6855\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.9101 - val_loss: 3.6733\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.8996 - val_loss: 3.6748\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.8907 - val_loss: 3.6780\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 167us/step - loss: 2.8833 - val_loss: 3.6711\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.8711 - val_loss: 3.6734\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.8645 - val_loss: 3.6653\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 125us/step - loss: 2.8535 - val_loss: 3.6812\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.8562 - val_loss: 3.6775\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.8326 - val_loss: 3.6801\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.8390 - val_loss: 3.6859\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.8160 - val_loss: 3.6727\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.8137 - val_loss: 3.6679\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.8022 - val_loss: 3.6648\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 2.7922 - val_loss: 3.6711\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.7873 - val_loss: 3.6875\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.7820 - val_loss: 3.6684\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.7701 - val_loss: 3.6647\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.7653 - val_loss: 3.6856\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 85us/step - loss: 2.7447 - val_loss: 3.6707\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.7468 - val_loss: 3.6479\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 2.7307 - val_loss: 3.6573\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 2.7160 - val_loss: 3.6780\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 2.7184 - val_loss: 3.6721\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.7121 - val_loss: 3.6760\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.7046 - val_loss: 3.6443\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.6944 - val_loss: 3.6530\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.6760 - val_loss: 3.6708\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.6706 - val_loss: 3.6580\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.6666 - val_loss: 3.6428\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.6426 - val_loss: 3.6595\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.6474 - val_loss: 3.7014\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.6435 - val_loss: 3.6906\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.6259 - val_loss: 3.6641\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.6066 - val_loss: 3.6554\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.6094 - val_loss: 3.6679\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.5990 - val_loss: 3.6465\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.5858 - val_loss: 3.6445\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.5819 - val_loss: 3.6590\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.5591 - val_loss: 3.6478\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.5578 - val_loss: 3.6457\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 75us/step - loss: 2.5458 - val_loss: 3.6674\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.5389 - val_loss: 3.6641\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 87us/step - loss: 2.5683 - val_loss: 3.6939\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.5043 - val_loss: 3.6603\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.5398 - val_loss: 3.6837\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.5070 - val_loss: 3.7160\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 2.4924 - val_loss: 3.6977\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.4844 - val_loss: 3.6761\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.4728 - val_loss: 3.6864\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.4657 - val_loss: 3.6726\n",
      "\n",
      "\n",
      "ESV: 0.3682299159632625\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 1ms/step - loss: 11.2449 - val_loss: 10.7762\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 10.4608 - val_loss: 10.0458\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 80us/step - loss: 9.6895 - val_loss: 9.3060\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 80us/step - loss: 8.9151 - val_loss: 8.5745\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 72us/step - loss: 8.1600 - val_loss: 7.8612\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 7.4222 - val_loss: 7.1841\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 6.7241 - val_loss: 6.5577\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 6.1099 - val_loss: 6.0083\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 5.5900 - val_loss: 5.5712\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 5.1486 - val_loss: 5.2581\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 4.8903 - val_loss: 5.0455\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 4.6966 - val_loss: 4.9307\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 4.5744 - val_loss: 4.8714\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 4.5515 - val_loss: 4.8199\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 4.5012 - val_loss: 4.7859\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 4.4819 - val_loss: 4.7574\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 4.4628 - val_loss: 4.7261\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 4.4289 - val_loss: 4.6967\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 210us/step - loss: 4.3924 - val_loss: 4.6884\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 4.3406 - val_loss: 4.6938\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 4.2940 - val_loss: 4.6700\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 4.2461 - val_loss: 4.6125\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 4.2014 - val_loss: 4.5548\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 4.1540 - val_loss: 4.5120\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 4.1207 - val_loss: 4.4822\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 4.1096 - val_loss: 4.4149\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 4.0366 - val_loss: 4.4198\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 4.0130 - val_loss: 4.4331\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.9682 - val_loss: 4.3974\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 3.9204 - val_loss: 4.3229\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.8850 - val_loss: 4.2874\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.8510 - val_loss: 4.2645\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.8264 - val_loss: 4.2282\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.7761 - val_loss: 4.2574\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 64us/step - loss: 3.7586 - val_loss: 4.2513\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.7132 - val_loss: 4.1656\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.6861 - val_loss: 4.1646\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.6578 - val_loss: 4.1489\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.6195 - val_loss: 4.1225\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.5968 - val_loss: 4.1168\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.5684 - val_loss: 4.0780\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 3.5503 - val_loss: 4.0636\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 3.5348 - val_loss: 4.0174\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 88us/step - loss: 3.5123 - val_loss: 4.0405\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.4923 - val_loss: 4.0067\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.4673 - val_loss: 3.9948\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.4543 - val_loss: 4.0374\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.4432 - val_loss: 3.9818\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 3.4172 - val_loss: 3.9698\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.4092 - val_loss: 3.9870\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.4139 - val_loss: 3.9113\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.3860 - val_loss: 3.9595\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.3624 - val_loss: 3.9366\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 84us/step - loss: 3.3562 - val_loss: 3.9503\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.3408 - val_loss: 3.9198\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.3493 - val_loss: 3.8957\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.3157 - val_loss: 3.9228\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.3074 - val_loss: 3.9176\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 3.2979 - val_loss: 3.9033\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.3099 - val_loss: 3.8905\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 175us/step - loss: 3.2972 - val_loss: 3.9322\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.2675 - val_loss: 3.8822\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.2816 - val_loss: 3.8592\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 3.2544 - val_loss: 3.9083\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 3.2441 - val_loss: 3.8733\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.2354 - val_loss: 3.8397\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 3.2328 - val_loss: 3.8354\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.2374 - val_loss: 3.8744\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 3.2131 - val_loss: 3.8650\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.2054 - val_loss: 3.8553\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.2079 - val_loss: 3.8288\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 79us/step - loss: 3.1941 - val_loss: 3.8880\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 3.1837 - val_loss: 3.8583\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.1733 - val_loss: 3.8587\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.1685 - val_loss: 3.8402\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.1578 - val_loss: 3.8504\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.1615 - val_loss: 3.9258\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 80us/step - loss: 3.1483 - val_loss: 3.8422\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.1457 - val_loss: 3.8116\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 3.1388 - val_loss: 3.8269\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.1338 - val_loss: 3.8855\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.1242 - val_loss: 3.8487\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 3.1106 - val_loss: 3.8652\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.1061 - val_loss: 3.8424\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.1016 - val_loss: 3.8218\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 80us/step - loss: 3.0987 - val_loss: 3.8179\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.0959 - val_loss: 3.8155\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 87us/step - loss: 3.0792 - val_loss: 3.8539\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0813 - val_loss: 3.8345\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.0540 - val_loss: 3.7871\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.0713 - val_loss: 3.7980\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 3.0499 - val_loss: 3.8658\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0543 - val_loss: 3.8285\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 3.0349 - val_loss: 3.8413\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.0294 - val_loss: 3.8419\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 72us/step - loss: 3.0269 - val_loss: 3.8603\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 3.0241 - val_loss: 3.8422\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.0260 - val_loss: 3.8097\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.0014 - val_loss: 3.8535\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 3.0005 - val_loss: 3.8447\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.9965 - val_loss: 3.8321\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.9819 - val_loss: 3.7748\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.9877 - val_loss: 3.8191\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.9982 - val_loss: 3.8512\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.9558 - val_loss: 3.8168\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.9552 - val_loss: 3.8240\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.9503 - val_loss: 3.8259\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.9676 - val_loss: 3.7945\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.9601 - val_loss: 3.8593\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.9373 - val_loss: 3.8192\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.9162 - val_loss: 3.8587\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 2.9005 - val_loss: 3.8577\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.8965 - val_loss: 3.8690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.8901 - val_loss: 3.8610\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.8784 - val_loss: 3.8899\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.8987 - val_loss: 3.9033\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.8647 - val_loss: 3.8680\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.8688 - val_loss: 3.8691\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 2.8483 - val_loss: 3.9233\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.8674 - val_loss: 3.8972\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.8328 - val_loss: 3.9536\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.8215 - val_loss: 3.8896\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.8310 - val_loss: 3.8851\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.8312 - val_loss: 3.9897\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.8144 - val_loss: 3.9038\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.7844 - val_loss: 3.9291\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 73us/step - loss: 2.7742 - val_loss: 3.9263\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.7715 - val_loss: 3.9654\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.7549 - val_loss: 3.9254\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.7810 - val_loss: 3.9007\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 86us/step - loss: 2.7508 - val_loss: 3.9785\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 2.7576 - val_loss: 3.9081\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.7384 - val_loss: 3.9337\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.7202 - val_loss: 3.9665\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.7095 - val_loss: 3.9785\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.6980 - val_loss: 3.9359\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.6848 - val_loss: 3.9723\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.6690 - val_loss: 3.9554\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.6611 - val_loss: 3.9740\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.6838 - val_loss: 4.0199\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.7347 - val_loss: 3.9555\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 2.6413 - val_loss: 4.0797\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.6515 - val_loss: 4.0039\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 177us/step - loss: 2.6437 - val_loss: 3.9770\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.6089 - val_loss: 4.0476\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.6062 - val_loss: 4.0089\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 167us/step - loss: 2.6052 - val_loss: 4.0017\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.6371 - val_loss: 3.9819\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 2.5746 - val_loss: 4.0682\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.6012 - val_loss: 4.0864\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.5896 - val_loss: 3.9854\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.5841 - val_loss: 4.0251\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.5558 - val_loss: 4.0600\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.5756 - val_loss: 4.0251\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 2.5249 - val_loss: 4.0598\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.5129 - val_loss: 4.0597\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.5114 - val_loss: 4.0676\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 2.4975 - val_loss: 4.0567\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.5112 - val_loss: 4.0317\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.4860 - val_loss: 4.1180\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.4788 - val_loss: 4.0587\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.4774 - val_loss: 4.0510\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.4594 - val_loss: 4.0577\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.4774 - val_loss: 4.1242\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.4300 - val_loss: 4.0546\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.4462 - val_loss: 4.0627\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.4127 - val_loss: 4.1085\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.4178 - val_loss: 4.1172\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.4114 - val_loss: 4.0786\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.4057 - val_loss: 4.0791\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 2.4096 - val_loss: 4.1332\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.4008 - val_loss: 4.0699\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.3948 - val_loss: 4.0611\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 2.3838 - val_loss: 4.0919\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.3763 - val_loss: 4.0934\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.3606 - val_loss: 4.1261\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.3554 - val_loss: 4.0880\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.3394 - val_loss: 4.0841\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.3445 - val_loss: 4.0852\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.3474 - val_loss: 4.0878\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 2.3252 - val_loss: 4.1023\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.3178 - val_loss: 4.1057\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.3075 - val_loss: 4.0828\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.3058 - val_loss: 4.0785\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.3024 - val_loss: 4.0996\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.2854 - val_loss: 4.0936\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.2938 - val_loss: 4.0893\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.2892 - val_loss: 4.1288\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 180us/step - loss: 2.2873 - val_loss: 4.0973\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.2590 - val_loss: 4.0900\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 2.2638 - val_loss: 4.1024\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 113us/step - loss: 2.2498 - val_loss: 4.1026\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.2440 - val_loss: 4.1010\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.2375 - val_loss: 4.1134\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.2350 - val_loss: 4.1221\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.2341 - val_loss: 4.1015\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.3126 - val_loss: 4.1026\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 219us/step - loss: 2.2850 - val_loss: 4.1832\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.2242 - val_loss: 4.1007\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.2311 - val_loss: 4.1139\n",
      "\n",
      "\n",
      "ESV: 0.2970814642241286\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 11.7762 - val_loss: 11.6557\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 11.4837 - val_loss: 11.3368\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 11.1206 - val_loss: 10.9720\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 10.6869 - val_loss: 10.5507\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 10.1907 - val_loss: 10.0149\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 9.5317 - val_loss: 9.2281\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 8.5445 - val_loss: 8.1565\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 7.3485 - val_loss: 7.0377\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 190us/step - loss: 6.2646 - val_loss: 6.1282\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 5.4509 - val_loss: 5.5702\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 5.0049 - val_loss: 5.3220\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 4.8219 - val_loss: 5.2265\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 4.7527 - val_loss: 5.1535\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 4.7053 - val_loss: 5.0634\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 86us/step - loss: 4.6195 - val_loss: 4.9774\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 4.5432 - val_loss: 4.9115\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 4.4728 - val_loss: 4.8510\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 4.4227 - val_loss: 4.7971\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 4.3578 - val_loss: 4.7284\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 4.2904 - val_loss: 4.6350\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 4.2059 - val_loss: 4.5403\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 4.1236 - val_loss: 4.4619\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 4.0593 - val_loss: 4.3721\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.9872 - val_loss: 4.2948\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.9306 - val_loss: 4.2296\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 3.8737 - val_loss: 4.1699\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.8099 - val_loss: 4.1290\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.7655 - val_loss: 4.0996\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.7283 - val_loss: 4.0414\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 3.6749 - val_loss: 3.9835\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.6593 - val_loss: 3.9351\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.5931 - val_loss: 3.9198\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.5486 - val_loss: 3.8764\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 3.5195 - val_loss: 3.8315\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 3.4843 - val_loss: 3.8267\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 174us/step - loss: 3.4507 - val_loss: 3.8016\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 3.4190 - val_loss: 3.7845\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.3992 - val_loss: 3.7542\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.3759 - val_loss: 3.7314\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.3542 - val_loss: 3.7061\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.3353 - val_loss: 3.6929\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 3.3206 - val_loss: 3.7002\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.3279 - val_loss: 3.6558\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.2808 - val_loss: 3.6511\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 3.2717 - val_loss: 3.6507\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.2638 - val_loss: 3.6586\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.2579 - val_loss: 3.6617\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.2416 - val_loss: 3.6207\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.2505 - val_loss: 3.6018\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.2256 - val_loss: 3.6081\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.2525 - val_loss: 3.6455\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.1995 - val_loss: 3.5961\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 3.2010 - val_loss: 3.5795\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.1876 - val_loss: 3.5884\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 180us/step - loss: 3.1731 - val_loss: 3.5820\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.1836 - val_loss: 3.5863\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 3.1673 - val_loss: 3.5828\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.1570 - val_loss: 3.5584\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 3.1851 - val_loss: 3.5490\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 3.1469 - val_loss: 3.5626\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.1236 - val_loss: 3.5831\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.1204 - val_loss: 3.5839\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 3.1056 - val_loss: 3.5763\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.0974 - val_loss: 3.5692\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.0827 - val_loss: 3.5427\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.0727 - val_loss: 3.5436\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 3.0746 - val_loss: 3.5481\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.0754 - val_loss: 3.5242\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.0435 - val_loss: 3.5370\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 114us/step - loss: 3.0391 - val_loss: 3.5429\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.0354 - val_loss: 3.5513\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 3.0340 - val_loss: 3.5239\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.0291 - val_loss: 3.5311\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.0056 - val_loss: 3.5082\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.0086 - val_loss: 3.4972\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.9924 - val_loss: 3.5013\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.0194 - val_loss: 3.5467\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.9893 - val_loss: 3.4933\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.9790 - val_loss: 3.4809\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.9679 - val_loss: 3.4886\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 80us/step - loss: 2.9575 - val_loss: 3.4956\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.9574 - val_loss: 3.4974\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.9494 - val_loss: 3.4788\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.9392 - val_loss: 3.4891\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.9563 - val_loss: 3.4740\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.9280 - val_loss: 3.4931\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.9181 - val_loss: 3.4749\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.9037 - val_loss: 3.4884\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.9323 - val_loss: 3.4720\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.9036 - val_loss: 3.5074\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.8970 - val_loss: 3.4762\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.8823 - val_loss: 3.4644\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.8725 - val_loss: 3.4721\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.8723 - val_loss: 3.4636\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.8550 - val_loss: 3.4667\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.8453 - val_loss: 3.4580\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.8422 - val_loss: 3.4591\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.8788 - val_loss: 3.4504\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.8234 - val_loss: 3.4703\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.8421 - val_loss: 3.4905\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.8271 - val_loss: 3.4773\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.8208 - val_loss: 3.4602\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.8015 - val_loss: 3.4633\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 2.8005 - val_loss: 3.4624\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.8066 - val_loss: 3.4846\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.7703 - val_loss: 3.4613\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.7918 - val_loss: 3.4594\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.8027 - val_loss: 3.4633\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.7600 - val_loss: 3.4578\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.7584 - val_loss: 3.4548\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.7917 - val_loss: 3.4714\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.7550 - val_loss: 3.4566\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.7482 - val_loss: 3.4475\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.7328 - val_loss: 3.4583\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.7235 - val_loss: 3.4653\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.7135 - val_loss: 3.4571\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.7119 - val_loss: 3.4584\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.7058 - val_loss: 3.4656\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.7059 - val_loss: 3.4471\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.6860 - val_loss: 3.4495\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.6924 - val_loss: 3.4485\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.6778 - val_loss: 3.4399\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.6975 - val_loss: 3.4475\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.6753 - val_loss: 3.4451\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.6783 - val_loss: 3.4351\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.6622 - val_loss: 3.4351\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.6524 - val_loss: 3.4352\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.6650 - val_loss: 3.4437\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.6493 - val_loss: 3.4443\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.6313 - val_loss: 3.4446\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 181us/step - loss: 2.6384 - val_loss: 3.4433\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.6168 - val_loss: 3.4359\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.6382 - val_loss: 3.4319\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.6307 - val_loss: 3.4452\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.6096 - val_loss: 3.4304\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.6046 - val_loss: 3.4308\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 2.5993 - val_loss: 3.4329\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.6015 - val_loss: 3.4308\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.5845 - val_loss: 3.4363\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.5989 - val_loss: 3.4461\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 186us/step - loss: 2.5747 - val_loss: 3.4482\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.5910 - val_loss: 3.4518\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.5604 - val_loss: 3.4460\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 186us/step - loss: 2.5722 - val_loss: 3.4555\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.5941 - val_loss: 3.4658\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.5866 - val_loss: 3.4627\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.5443 - val_loss: 3.4585\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 151us/step - loss: 2.5526 - val_loss: 3.4562\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.5338 - val_loss: 3.4506\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.5442 - val_loss: 3.4651\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 2.5669 - val_loss: 3.4713\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.5444 - val_loss: 3.4595\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.5211 - val_loss: 3.4614\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.4996 - val_loss: 3.4533\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.5126 - val_loss: 3.4452\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 2.4919 - val_loss: 3.4450\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.5233 - val_loss: 3.4433\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 2.4823 - val_loss: 3.4300\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.4912 - val_loss: 3.4159\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.4768 - val_loss: 3.4248\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 2.4929 - val_loss: 3.4388\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.4516 - val_loss: 3.4313\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.4583 - val_loss: 3.4336\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.4508 - val_loss: 3.4218\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.4412 - val_loss: 3.4137\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.4355 - val_loss: 3.4147\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.4381 - val_loss: 3.4151\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.4258 - val_loss: 3.4168\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.4230 - val_loss: 3.4212\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.4229 - val_loss: 3.4173\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.4621 - val_loss: 3.4108\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.3966 - val_loss: 3.4011\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.4282 - val_loss: 3.3870\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 2.3980 - val_loss: 3.3804\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.3867 - val_loss: 3.3744\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.3910 - val_loss: 3.3740\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.3908 - val_loss: 3.3685\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.3627 - val_loss: 3.3834\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 2.3728 - val_loss: 3.3878\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.3724 - val_loss: 3.3814\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.3577 - val_loss: 3.3555\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.3808 - val_loss: 3.3491\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.3404 - val_loss: 3.3493\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.3990 - val_loss: 3.3908\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.3226 - val_loss: 3.3488\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.3429 - val_loss: 3.3429\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.3180 - val_loss: 3.3465\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.3257 - val_loss: 3.3507\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.3140 - val_loss: 3.3351\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.3021 - val_loss: 3.3337\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.2964 - val_loss: 3.3306\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.3178 - val_loss: 3.3396\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 85us/step - loss: 2.2984 - val_loss: 3.3438\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.2826 - val_loss: 3.3366\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.2904 - val_loss: 3.3401\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.2698 - val_loss: 3.3299\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.2838 - val_loss: 3.3336\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.2535 - val_loss: 3.3316\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.2714 - val_loss: 3.3353\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 176us/step - loss: 2.2450 - val_loss: 3.3231\n",
      "\n",
      "\n",
      "ESV: 0.4250230288049299\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 12.5937 - val_loss: 12.3798\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 167us/step - loss: 12.2279 - val_loss: 12.0494\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 11.8728 - val_loss: 11.7579\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 11.5258 - val_loss: 11.4645\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 11.1954 - val_loss: 11.1210\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 10.8173 - val_loss: 10.7384\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 10.3927 - val_loss: 10.2822\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 9.8870 - val_loss: 9.7289\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 9.2594 - val_loss: 9.0808\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 8.5170 - val_loss: 8.3593\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 7.7254 - val_loss: 7.5821\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 6.9501 - val_loss: 6.8059\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 176us/step - loss: 6.1700 - val_loss: 6.1270\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 5.4986 - val_loss: 5.5834\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 5.0413 - val_loss: 5.1912\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 4.6778 - val_loss: 4.9842\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 4.4988 - val_loss: 4.8873\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 4.4492 - val_loss: 4.8337\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 4.4069 - val_loss: 4.8028\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 4.3917 - val_loss: 4.7760\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 4.3699 - val_loss: 4.7433\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 4.3433 - val_loss: 4.7153\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 4.3207 - val_loss: 4.6882\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 4.2982 - val_loss: 4.6604\n",
      "Epoch 25/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 152us/step - loss: 4.2726 - val_loss: 4.6376\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 4.2481 - val_loss: 4.6108\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 4.2266 - val_loss: 4.5779\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 4.1960 - val_loss: 4.5495\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 4.1655 - val_loss: 4.5225\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 4.1245 - val_loss: 4.4855\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 4.1231 - val_loss: 4.4439\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 4.0637 - val_loss: 4.4116\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 4.0329 - val_loss: 4.3827\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 4.0016 - val_loss: 4.3481\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.9707 - val_loss: 4.3138\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.9440 - val_loss: 4.2802\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.9131 - val_loss: 4.2526\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 78us/step - loss: 3.8961 - val_loss: 4.2362\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.8566 - val_loss: 4.1988\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.8272 - val_loss: 4.1413\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.8051 - val_loss: 4.0971\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.7538 - val_loss: 4.0811\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.7428 - val_loss: 4.0712\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 3.7042 - val_loss: 4.0294\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 3.6809 - val_loss: 3.9962\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.6703 - val_loss: 3.9617\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.6544 - val_loss: 3.9327\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.6240 - val_loss: 3.9215\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.5973 - val_loss: 3.9175\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.5856 - val_loss: 3.8858\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.5628 - val_loss: 3.8624\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.5420 - val_loss: 3.8460\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.5204 - val_loss: 3.8345\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 3.5326 - val_loss: 3.8154\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.5062 - val_loss: 3.8213\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.4796 - val_loss: 3.8007\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.4622 - val_loss: 3.7833\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.4475 - val_loss: 3.7725\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.4310 - val_loss: 3.7781\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.4235 - val_loss: 3.7721\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.4178 - val_loss: 3.7412\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.3885 - val_loss: 3.7392\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.3673 - val_loss: 3.7410\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.3626 - val_loss: 3.7547\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 3.3532 - val_loss: 3.7303\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 3.3291 - val_loss: 3.7340\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.3263 - val_loss: 3.7547\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.3066 - val_loss: 3.7218\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.2921 - val_loss: 3.7096\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.2860 - val_loss: 3.7098\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.2843 - val_loss: 3.7423\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.2692 - val_loss: 3.7230\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.2468 - val_loss: 3.7128\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.2371 - val_loss: 3.7073\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.2339 - val_loss: 3.7181\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.2227 - val_loss: 3.7155\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 3.2075 - val_loss: 3.7067\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 3.2130 - val_loss: 3.6982\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 3.2020 - val_loss: 3.7109\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.1942 - val_loss: 3.7285\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 3.1797 - val_loss: 3.7173\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.1647 - val_loss: 3.7157\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.1590 - val_loss: 3.7464\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 3.1450 - val_loss: 3.7504\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 175us/step - loss: 3.1322 - val_loss: 3.7384\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.1240 - val_loss: 3.7365\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.1191 - val_loss: 3.7682\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.1195 - val_loss: 3.7574\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 174us/step - loss: 3.1069 - val_loss: 3.7639\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.0921 - val_loss: 3.7815\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 3.0959 - val_loss: 3.7852\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.0909 - val_loss: 3.7411\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 3.0818 - val_loss: 3.7673\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 220us/step - loss: 3.0933 - val_loss: 3.8251\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.0511 - val_loss: 3.7776\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.0462 - val_loss: 3.7689\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 3.0563 - val_loss: 3.7929\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.0288 - val_loss: 3.8235\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 3.0447 - val_loss: 3.8402\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 167us/step - loss: 3.0152 - val_loss: 3.7941\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 3.0008 - val_loss: 3.8136\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.9941 - val_loss: 3.8093\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.9917 - val_loss: 3.8015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.9948 - val_loss: 3.8089\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.9997 - val_loss: 3.7982\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.9669 - val_loss: 3.8348\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.9631 - val_loss: 3.8437\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.9648 - val_loss: 3.8146\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.9379 - val_loss: 3.8452\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.9850 - val_loss: 3.8990\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.9284 - val_loss: 3.8368\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.9244 - val_loss: 3.8490\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.9155 - val_loss: 3.8724\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.9086 - val_loss: 3.8805\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 2.9004 - val_loss: 3.8704\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.8946 - val_loss: 3.8760\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.8869 - val_loss: 3.8728\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.8780 - val_loss: 3.8817\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.8729 - val_loss: 3.8818\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 2.8728 - val_loss: 3.8777\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.8663 - val_loss: 3.8945\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.8481 - val_loss: 3.9019\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.8470 - val_loss: 3.9045\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.8444 - val_loss: 3.9361\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.8357 - val_loss: 3.9026\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.8354 - val_loss: 3.9397\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 2.8518 - val_loss: 3.9259\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.8147 - val_loss: 3.9414\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.7989 - val_loss: 3.9460\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.7922 - val_loss: 3.9474\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.7956 - val_loss: 3.9592\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 2.7772 - val_loss: 3.9414\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.7810 - val_loss: 3.9437\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.7807 - val_loss: 3.9986\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.7645 - val_loss: 3.9753\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.7604 - val_loss: 3.9620\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.7546 - val_loss: 3.9789\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.7488 - val_loss: 3.9839\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.7412 - val_loss: 3.9602\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.7516 - val_loss: 3.9977\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.7237 - val_loss: 3.9838\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.7188 - val_loss: 3.9837\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.7127 - val_loss: 3.9771\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.7069 - val_loss: 3.9882\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.7034 - val_loss: 4.0161\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.6946 - val_loss: 4.0256\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.7015 - val_loss: 4.0428\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.7045 - val_loss: 4.0644\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.6954 - val_loss: 4.0319\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.7082 - val_loss: 4.0383\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.7005 - val_loss: 4.0231\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.6814 - val_loss: 4.1134\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.6775 - val_loss: 4.0183\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.6554 - val_loss: 4.0177\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.6535 - val_loss: 4.0526\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 2.6476 - val_loss: 4.0539\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.6467 - val_loss: 4.0743\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.6325 - val_loss: 4.0517\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 168us/step - loss: 2.6229 - val_loss: 4.0453\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.6205 - val_loss: 4.0655\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.6268 - val_loss: 4.0569\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.6287 - val_loss: 4.0815\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.6096 - val_loss: 4.0709\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 175us/step - loss: 2.5961 - val_loss: 4.0822\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 2.5860 - val_loss: 4.0619\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 88us/step - loss: 2.5842 - val_loss: 4.0749\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 171us/step - loss: 2.5927 - val_loss: 4.0737\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.5707 - val_loss: 4.1210\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.5774 - val_loss: 4.0792\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.5609 - val_loss: 4.0876\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.5538 - val_loss: 4.0885\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.5467 - val_loss: 4.1091\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.5573 - val_loss: 4.0981\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.5664 - val_loss: 4.0556\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.5503 - val_loss: 4.0609\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 2.5541 - val_loss: 4.0939\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.5051 - val_loss: 4.0738\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 68us/step - loss: 2.5299 - val_loss: 4.1059\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.5145 - val_loss: 4.1265\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.5054 - val_loss: 4.0806\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.5079 - val_loss: 4.1265\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 140us/step - loss: 2.4869 - val_loss: 4.1171\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.4775 - val_loss: 4.1365\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.4845 - val_loss: 4.1448\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.4693 - val_loss: 4.1406\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 2.4930 - val_loss: 4.1739\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 85us/step - loss: 2.4715 - val_loss: 4.1289\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.4724 - val_loss: 4.1324\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.4586 - val_loss: 4.1324\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.4432 - val_loss: 4.1507\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.4546 - val_loss: 4.1683\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.4525 - val_loss: 4.1672\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.4287 - val_loss: 4.1749\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.4198 - val_loss: 4.1803\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.4273 - val_loss: 4.1806\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.4183 - val_loss: 4.1704\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.4055 - val_loss: 4.1760\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 2.4151 - val_loss: 4.1839\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.4178 - val_loss: 4.1754\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.3792 - val_loss: 4.1836\n",
      "\n",
      "\n",
      "ESV: 0.2777369813152578\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 12.1319 - val_loss: 12.0472\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 11.9250 - val_loss: 11.9146\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 204us/step - loss: 11.8479 - val_loss: 11.8568\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 11.7933 - val_loss: 11.8089\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 11.7431 - val_loss: 11.7602\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 11.6907 - val_loss: 11.7102\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 11.6381 - val_loss: 11.6585\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 11.5813 - val_loss: 11.6065\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 11.5259 - val_loss: 11.5520\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 11.4686 - val_loss: 11.4951\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 11.4076 - val_loss: 11.4376\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 11.3453 - val_loss: 11.3786\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 11.2830 - val_loss: 11.3170\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 11.2170 - val_loss: 11.2543\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 11.1508 - val_loss: 11.1887\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 11.0797 - val_loss: 11.1227\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 11.0091 - val_loss: 11.0542\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 10.9365 - val_loss: 10.9838\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 10.8606 - val_loss: 10.9132\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 10.7864 - val_loss: 10.8400\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 10.7088 - val_loss: 10.7668\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 10.6329 - val_loss: 10.6909\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 10.5497 - val_loss: 10.6160\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 186us/step - loss: 10.4689 - val_loss: 10.5391\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 10.3864 - val_loss: 10.4617\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 10.3060 - val_loss: 10.3817\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 10.2185 - val_loss: 10.3017\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 10.1362 - val_loss: 10.2199\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 10.0485 - val_loss: 10.1396\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 9.9622 - val_loss: 10.0586\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 9.8759 - val_loss: 9.9752\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 9.7852 - val_loss: 9.8921\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 9.6983 - val_loss: 9.8076\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 9.6071 - val_loss: 9.7242\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 9.5183 - val_loss: 9.6393\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 9.4293 - val_loss: 9.5531\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 9.3359 - val_loss: 9.4680\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 9.2424 - val_loss: 9.3824\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 9.1525 - val_loss: 9.2945\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 9.0579 - val_loss: 9.2091\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 8.9686 - val_loss: 9.1226\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 8.8749 - val_loss: 9.0364\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 8.7832 - val_loss: 8.9499\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 187us/step - loss: 8.6882 - val_loss: 8.8649\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 8.5991 - val_loss: 8.7765\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 8.5002 - val_loss: 8.6927\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 8.4128 - val_loss: 8.6063\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 8.3216 - val_loss: 8.5209\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 8.2298 - val_loss: 8.4373\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 171us/step - loss: 8.1389 - val_loss: 8.3564\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 8.0499 - val_loss: 8.2764\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 184us/step - loss: 7.9619 - val_loss: 8.1961\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 7.8805 - val_loss: 8.1133\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 176us/step - loss: 7.7888 - val_loss: 8.0342\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 7.7009 - val_loss: 7.9562\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 7.6203 - val_loss: 7.8763\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 176us/step - loss: 7.5321 - val_loss: 7.7996\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 7.4510 - val_loss: 7.7242\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 109us/step - loss: 7.3635 - val_loss: 7.6524\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 7.2876 - val_loss: 7.5796\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 7.2082 - val_loss: 7.5076\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 7.1319 - val_loss: 7.4368\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 7.0530 - val_loss: 7.3693\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 6.9807 - val_loss: 7.3027\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 6.9052 - val_loss: 7.2389\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 190us/step - loss: 6.8333 - val_loss: 7.1764\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 190us/step - loss: 6.7656 - val_loss: 7.1129\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 6.6942 - val_loss: 7.0505\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 6.6270 - val_loss: 6.9903\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 6.5588 - val_loss: 6.9341\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 6.4976 - val_loss: 6.8761\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 6.4336 - val_loss: 6.8198\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 6.3690 - val_loss: 6.7662\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 6.3111 - val_loss: 6.7112\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 6.2497 - val_loss: 6.6617\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 6.1934 - val_loss: 6.6155\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 6.1429 - val_loss: 6.5693\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 6.0894 - val_loss: 6.5255\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 6.0400 - val_loss: 6.4812\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 5.9884 - val_loss: 6.4392\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 5.9427 - val_loss: 6.3983\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 5.8958 - val_loss: 6.3607\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 5.8513 - val_loss: 6.3240\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 5.8075 - val_loss: 6.2896\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 5.7672 - val_loss: 6.2564\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 5.7313 - val_loss: 6.2229\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 5.6922 - val_loss: 6.1931\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 5.6541 - val_loss: 6.1667\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 5.6250 - val_loss: 6.1391\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 5.5916 - val_loss: 6.1130\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 5.5605 - val_loss: 6.0890\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 5.5312 - val_loss: 6.0659\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 5.5026 - val_loss: 6.0435\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 5.4766 - val_loss: 6.0217\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 5.4498 - val_loss: 6.0024\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 5.4254 - val_loss: 5.9841\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 78us/step - loss: 5.4013 - val_loss: 5.9679\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 5.3814 - val_loss: 5.9512\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 5.3598 - val_loss: 5.9357\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 5.3397 - val_loss: 5.9211\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 5.3231 - val_loss: 5.9078\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 5.3041 - val_loss: 5.8973\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 5.2907 - val_loss: 5.8862\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 5.2764 - val_loss: 5.8751\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 88us/step - loss: 5.2619 - val_loss: 5.8648\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 5.2465 - val_loss: 5.8565\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 5.2329 - val_loss: 5.8489\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 5.2219 - val_loss: 5.8407\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 5.2107 - val_loss: 5.8322\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 5.1984 - val_loss: 5.8254\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 5.1868 - val_loss: 5.8193\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 5.1790 - val_loss: 5.8128\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 5.1684 - val_loss: 5.8079\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 5.1614 - val_loss: 5.8029\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 5.1526 - val_loss: 5.7988\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 5.1442 - val_loss: 5.7949\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 5.1382 - val_loss: 5.7911\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 5.1308 - val_loss: 5.7880\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 191us/step - loss: 5.1263 - val_loss: 5.7853\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 5.1211 - val_loss: 5.7829\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 5.1149 - val_loss: 5.7810\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 5.1101 - val_loss: 5.7795\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 5.1071 - val_loss: 5.7777\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 5.1026 - val_loss: 5.7763\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 5.0986 - val_loss: 5.7751\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 5.0962 - val_loss: 5.7741\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 5.0921 - val_loss: 5.7735\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 5.0902 - val_loss: 5.7728\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 5.0866 - val_loss: 5.7725\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 5.0859 - val_loss: 5.7721\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 5.0827 - val_loss: 5.7719\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 5.0810 - val_loss: 5.7717\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 5.0794 - val_loss: 5.7716\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 5.0777 - val_loss: 5.7716\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 5.0765 - val_loss: 5.7717\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 5.0742 - val_loss: 5.7718\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 118us/step - loss: 5.0738 - val_loss: 5.7720\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 5.0724 - val_loss: 5.7722\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 5.0710 - val_loss: 5.7724\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 5.0708 - val_loss: 5.7727\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 5.0691 - val_loss: 5.7729\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 5.0688 - val_loss: 5.7734\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 5.0675 - val_loss: 5.7739\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 5.0673 - val_loss: 5.7746\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 5.0657 - val_loss: 5.7750\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 5.0656 - val_loss: 5.7757\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 70us/step - loss: 5.0643 - val_loss: 5.7759\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 5.0640 - val_loss: 5.7763\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 5.0642 - val_loss: 5.7770\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 5.0640 - val_loss: 5.7777\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 5.0628 - val_loss: 5.7780\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 5.0625 - val_loss: 5.7781\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 5.0624 - val_loss: 5.7786\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 5.0623 - val_loss: 5.7791\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 5.0619 - val_loss: 5.7794\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 5.0618 - val_loss: 5.7795\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 5.0617 - val_loss: 5.7802\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 5.0612 - val_loss: 5.7808\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 5.0613 - val_loss: 5.7813\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 5.0611 - val_loss: 5.7819\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 5.0610 - val_loss: 5.7820\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 5.0611 - val_loss: 5.7815\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 5.0610 - val_loss: 5.7820\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 5.0613 - val_loss: 5.7817\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 5.0609 - val_loss: 5.7817\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 5.0614 - val_loss: 5.7828\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 5.0605 - val_loss: 5.7830\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 5.0607 - val_loss: 5.7838\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 5.0604 - val_loss: 5.7839\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 5.0606 - val_loss: 5.7840\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 5.0603 - val_loss: 5.7848\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 5.0603 - val_loss: 5.7852\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 5.0600 - val_loss: 5.7851\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 5.0603 - val_loss: 5.7849\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 5.0601 - val_loss: 5.7849\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 5.0601 - val_loss: 5.7852\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 5.0599 - val_loss: 5.7855\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 5.0598 - val_loss: 5.7861\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 5.0599 - val_loss: 5.7866\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 5.0598 - val_loss: 5.7871\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 168us/step - loss: 5.0604 - val_loss: 5.7880\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 177us/step - loss: 5.0599 - val_loss: 5.7881\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 5.0598 - val_loss: 5.7882\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 5.0599 - val_loss: 5.7887\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 5.0600 - val_loss: 5.7887\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 5.0598 - val_loss: 5.7885\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 5.0600 - val_loss: 5.7882\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 5.0598 - val_loss: 5.7882\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 5.0598 - val_loss: 5.7884\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 5.0608 - val_loss: 5.7896\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 5.0599 - val_loss: 5.7898\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 5.0598 - val_loss: 5.7895\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 5.0605 - val_loss: 5.7888\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 5.0600 - val_loss: 5.7886\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 5.0598 - val_loss: 5.7889\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 5.0597 - val_loss: 5.7891\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 5.0599 - val_loss: 5.7895\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 5.0599 - val_loss: 5.7893\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 5.0599 - val_loss: 5.7893\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 5.0599 - val_loss: 5.7889\n",
      "\n",
      "\n",
      "ESV: 0.0002117688647340632\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 9.9265 - val_loss: 9.5285\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 170us/step - loss: 8.7460 - val_loss: 8.4366\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 7.5952 - val_loss: 7.3292\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 6.4427 - val_loss: 6.3424\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 5.4619 - val_loss: 5.6336\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 4.8132 - val_loss: 5.2347\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 4.4794 - val_loss: 5.1059\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 4.4328 - val_loss: 5.0886\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 4.4066 - val_loss: 5.0439\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 4.3772 - val_loss: 4.9958\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 4.3364 - val_loss: 4.9457\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 4.2978 - val_loss: 4.8989\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 4.2597 - val_loss: 4.8519\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 138us/step - loss: 4.2224 - val_loss: 4.8094\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 4.1949 - val_loss: 4.7611\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 4.1589 - val_loss: 4.7131\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 4.1230 - val_loss: 4.6731\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 4.0788 - val_loss: 4.6220\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 4.0395 - val_loss: 4.5735\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.9915 - val_loss: 4.5278\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.9408 - val_loss: 4.4746\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.8811 - val_loss: 4.4112\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 3.8236 - val_loss: 4.3565\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 71us/step - loss: 3.7869 - val_loss: 4.3104\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.7474 - val_loss: 4.2649\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.7074 - val_loss: 4.2320\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 3.6687 - val_loss: 4.1987\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.6430 - val_loss: 4.1649\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 3.6199 - val_loss: 4.1542\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.5947 - val_loss: 4.1273\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.5752 - val_loss: 4.0912\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.5345 - val_loss: 4.0706\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.5066 - val_loss: 4.0533\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.4898 - val_loss: 4.0399\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.4701 - val_loss: 4.0332\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 3.4488 - val_loss: 4.0246\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 3.4356 - val_loss: 4.0103\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.4168 - val_loss: 3.9947\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 3.4182 - val_loss: 3.9961\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.3887 - val_loss: 3.9749\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 3.3915 - val_loss: 3.9830\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 3.3896 - val_loss: 3.9596\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.3496 - val_loss: 3.9512\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.3318 - val_loss: 3.9494\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.3107 - val_loss: 3.9414\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.3167 - val_loss: 3.9382\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.2783 - val_loss: 3.9349\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.2846 - val_loss: 3.9363\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.2676 - val_loss: 3.9094\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.2477 - val_loss: 3.8998\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.2245 - val_loss: 3.9069\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 3.2137 - val_loss: 3.9213\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 3.1951 - val_loss: 3.9167\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 3.1855 - val_loss: 3.9042\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 3.1728 - val_loss: 3.8938\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.1816 - val_loss: 3.8869\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.1498 - val_loss: 3.9040\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.1472 - val_loss: 3.8781\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.1069 - val_loss: 3.8623\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.1398 - val_loss: 3.8654\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 3.0709 - val_loss: 3.8828\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0905 - val_loss: 3.8727\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 3.0572 - val_loss: 3.8546\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.0468 - val_loss: 3.8443\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.0234 - val_loss: 3.8444\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.0218 - val_loss: 3.8504\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.9961 - val_loss: 3.8549\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.9843 - val_loss: 3.8530\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.9715 - val_loss: 3.8370\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.9634 - val_loss: 3.8514\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.9481 - val_loss: 3.8468\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.9326 - val_loss: 3.8517\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.9460 - val_loss: 3.8602\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 2.9230 - val_loss: 3.8460\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.8996 - val_loss: 3.8356\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.8888 - val_loss: 3.8446\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.8719 - val_loss: 3.8425\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.8568 - val_loss: 3.8491\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 2.8464 - val_loss: 3.8597\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.8374 - val_loss: 3.8647\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 2.8985 - val_loss: 3.8578\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.9051 - val_loss: 3.8855\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.8059 - val_loss: 3.8707\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.8204 - val_loss: 3.8386\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.7776 - val_loss: 3.8350\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 2.7659 - val_loss: 3.8379\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.7887 - val_loss: 3.8637\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.7522 - val_loss: 3.8728\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.7336 - val_loss: 3.8940\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.7223 - val_loss: 3.8974\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 2.7297 - val_loss: 3.8750\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.7058 - val_loss: 3.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.7276 - val_loss: 3.8507\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.6811 - val_loss: 3.8643\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 2.6969 - val_loss: 3.8518\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 2.6593 - val_loss: 3.8672\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.6454 - val_loss: 3.8658\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.6362 - val_loss: 3.8624\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.6321 - val_loss: 3.8681\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.6156 - val_loss: 3.8680\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.6045 - val_loss: 3.8667\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.6089 - val_loss: 3.8744\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.5943 - val_loss: 3.8592\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.6026 - val_loss: 3.8591\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.5655 - val_loss: 3.8558\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 2.5681 - val_loss: 3.8654\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.5544 - val_loss: 3.8628\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.5338 - val_loss: 3.8644\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.5228 - val_loss: 3.8481\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 2.5153 - val_loss: 3.8659\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.5163 - val_loss: 3.8937\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.4981 - val_loss: 3.8656\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.4802 - val_loss: 3.8859\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.4740 - val_loss: 3.8867\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.4521 - val_loss: 3.8499\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.4604 - val_loss: 3.8455\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.4307 - val_loss: 3.8791\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.4421 - val_loss: 3.9238\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.4234 - val_loss: 3.8920\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.4103 - val_loss: 3.8748\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.4124 - val_loss: 3.9104\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 2.3894 - val_loss: 3.8907\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.3663 - val_loss: 3.9099\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.4268 - val_loss: 3.9125\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.3333 - val_loss: 3.9767\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.3616 - val_loss: 3.9157\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.3199 - val_loss: 3.9088\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.3243 - val_loss: 3.9142\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.3157 - val_loss: 3.9042\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.3000 - val_loss: 3.9127\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.2859 - val_loss: 3.9107\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.2929 - val_loss: 3.9041\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.2613 - val_loss: 3.9421\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.2693 - val_loss: 3.9541\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.2455 - val_loss: 3.9302\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.2624 - val_loss: 3.9040\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.2716 - val_loss: 3.9723\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.2301 - val_loss: 3.8915\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.2233 - val_loss: 3.9064\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.1992 - val_loss: 3.9151\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.2023 - val_loss: 3.8872\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.1783 - val_loss: 3.9122\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.1633 - val_loss: 3.9130\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.1538 - val_loss: 3.9462\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.1444 - val_loss: 3.9210\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.1324 - val_loss: 3.9111\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.1239 - val_loss: 3.9206\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.1469 - val_loss: 3.8949\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.1162 - val_loss: 3.9530\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.1177 - val_loss: 3.9081\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.1193 - val_loss: 3.9453\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.0785 - val_loss: 3.9211\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.0719 - val_loss: 3.9350\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.0607 - val_loss: 3.9220\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.0483 - val_loss: 3.9093\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 2.0313 - val_loss: 3.9168\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.0394 - val_loss: 3.9151\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.0313 - val_loss: 3.9521\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.0093 - val_loss: 3.9331\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.0152 - val_loss: 3.9420\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.0057 - val_loss: 3.9353\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.0367 - val_loss: 3.9175\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 1.9889 - val_loss: 4.0270\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 1.9887 - val_loss: 4.0034\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 1.9634 - val_loss: 3.9904\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.0032 - val_loss: 3.9609\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 1.9396 - val_loss: 3.9897\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 1.9514 - val_loss: 3.9554\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 1.9299 - val_loss: 3.9682\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 1.9262 - val_loss: 3.9871\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 108us/step - loss: 1.9165 - val_loss: 3.9601\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 1.9189 - val_loss: 3.9690\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 1.8966 - val_loss: 4.0152\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 1.8933 - val_loss: 3.9950\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 1.8932 - val_loss: 3.9741\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 1.8743 - val_loss: 4.0120\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 1.8801 - val_loss: 4.0299\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 1.8696 - val_loss: 3.9814\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 1.8597 - val_loss: 4.0287\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 1.8564 - val_loss: 3.9637\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 1.8930 - val_loss: 3.9334\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 1.8203 - val_loss: 4.0764\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 1.8758 - val_loss: 4.0329\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 1.8515 - val_loss: 4.0114\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 65us/step - loss: 1.8512 - val_loss: 4.0875\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 1.8269 - val_loss: 4.0022\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 1.8054 - val_loss: 4.0121\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 1.7963 - val_loss: 4.0253\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 1.7992 - val_loss: 4.0447\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 1.7782 - val_loss: 4.0447\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 1.7914 - val_loss: 4.0413\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 1.7720 - val_loss: 4.0089\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 1.7699 - val_loss: 4.0339\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 1.7552 - val_loss: 4.0609\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 1.7663 - val_loss: 4.0802\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 1.7520 - val_loss: 4.1049\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 1.7425 - val_loss: 4.0492\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 77us/step - loss: 1.7633 - val_loss: 4.0651\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 1.7392 - val_loss: 4.1098\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 1.7372 - val_loss: 4.0508\n",
      "\n",
      "\n",
      "ESV: 0.30158151175536085\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 12.1563 - val_loss: 12.0048\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 11.9205 - val_loss: 11.9394\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 11.8699 - val_loss: 11.8925\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 11.8272 - val_loss: 11.8502\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 11.7859 - val_loss: 11.8119\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 11.7468 - val_loss: 11.7727\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 11.7043 - val_loss: 11.7343\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 11.6646 - val_loss: 11.6943\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 171us/step - loss: 11.6223 - val_loss: 11.6534\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 11.5782 - val_loss: 11.6114\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 11.5338 - val_loss: 11.5682\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 11.4887 - val_loss: 11.5236\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 11.4418 - val_loss: 11.4782\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 181us/step - loss: 11.3936 - val_loss: 11.4327\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 11.3457 - val_loss: 11.3861\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 11.2967 - val_loss: 11.3391\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 11.2454 - val_loss: 11.2912\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 11.1942 - val_loss: 11.2420\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 11.1426 - val_loss: 11.1913\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 11.0884 - val_loss: 11.1399\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 11.0350 - val_loss: 11.0867\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 10.9776 - val_loss: 11.0339\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 10.9219 - val_loss: 10.9797\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 10.8643 - val_loss: 10.9243\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 10.8046 - val_loss: 10.8688\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 10.7469 - val_loss: 10.8116\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 10.6853 - val_loss: 10.7550\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 10.6238 - val_loss: 10.6966\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 10.5633 - val_loss: 10.6360\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 10.5005 - val_loss: 10.5758\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 10.4355 - val_loss: 10.5166\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 10.3727 - val_loss: 10.4565\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 10.3096 - val_loss: 10.3956\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 82us/step - loss: 10.2449 - val_loss: 10.3334\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 10.1798 - val_loss: 10.2654\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 10.1092 - val_loss: 10.1863\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 10.0153 - val_loss: 10.0764\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 9.8769 - val_loss: 9.9128\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 9.6807 - val_loss: 9.6496\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 9.3524 - val_loss: 9.2481\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 8.8803 - val_loss: 8.6841\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 8.2308 - val_loss: 7.9568\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 7.4048 - val_loss: 7.0932\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 6.4383 - val_loss: 6.1961\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 5.5743 - val_loss: 5.4264\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 4.9444 - val_loss: 5.0231\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 4.6431 - val_loss: 4.9461\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 106us/step - loss: 4.5816 - val_loss: 4.9171\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 4.5672 - val_loss: 4.8582\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 4.5060 - val_loss: 4.8143\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 4.4695 - val_loss: 4.7695\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 4.4352 - val_loss: 4.7217\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 4.3833 - val_loss: 4.6802\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 4.3477 - val_loss: 4.6375\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 4.3128 - val_loss: 4.5984\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 4.2834 - val_loss: 4.5625\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 4.2500 - val_loss: 4.5269\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 4.2177 - val_loss: 4.4939\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 4.1891 - val_loss: 4.4693\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 4.1671 - val_loss: 4.4415\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 4.1357 - val_loss: 4.4107\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 4.1070 - val_loss: 4.3785\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 4.0916 - val_loss: 4.3375\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 4.0534 - val_loss: 4.3055\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 4.0259 - val_loss: 4.2740\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.9910 - val_loss: 4.2482\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 3.9711 - val_loss: 4.2252\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.9471 - val_loss: 4.2002\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.9233 - val_loss: 4.1728\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.9011 - val_loss: 4.1428\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.8789 - val_loss: 4.1192\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 3.8573 - val_loss: 4.0964\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.8404 - val_loss: 4.0674\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.8143 - val_loss: 4.0458\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 3.7995 - val_loss: 4.0285\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 3.7777 - val_loss: 4.0027\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.7565 - val_loss: 3.9752\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 3.7346 - val_loss: 3.9544\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.7128 - val_loss: 3.9355\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.7005 - val_loss: 3.9129\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.6744 - val_loss: 3.8958\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.6574 - val_loss: 3.8753\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.6432 - val_loss: 3.8684\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 3.6269 - val_loss: 3.8497\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.6131 - val_loss: 3.8276\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 3.5978 - val_loss: 3.8185\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.5859 - val_loss: 3.8158\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.5684 - val_loss: 3.7961\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.5537 - val_loss: 3.7861\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 81us/step - loss: 3.5749 - val_loss: 3.7601\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.5250 - val_loss: 3.7559\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.5162 - val_loss: 3.7560\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.5106 - val_loss: 3.7471\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 3.4972 - val_loss: 3.7289\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.4953 - val_loss: 3.7085\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 3.4756 - val_loss: 3.7040\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.4690 - val_loss: 3.7030\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.4611 - val_loss: 3.7021\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 3.4567 - val_loss: 3.6895\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.4479 - val_loss: 3.6842\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 3.4370 - val_loss: 3.6610\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 3.4349 - val_loss: 3.6502\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 3.4280 - val_loss: 3.6534\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 3.4189 - val_loss: 3.6514\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.4057 - val_loss: 3.6354\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.4050 - val_loss: 3.6281\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.4012 - val_loss: 3.6175\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.3860 - val_loss: 3.6179\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.3914 - val_loss: 3.6324\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.3875 - val_loss: 3.6057\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.3682 - val_loss: 3.5970\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.3697 - val_loss: 3.5967\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 3.3627 - val_loss: 3.5896\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.3541 - val_loss: 3.5872\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.3501 - val_loss: 3.5975\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.3468 - val_loss: 3.5850\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.3368 - val_loss: 3.5919\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.3333 - val_loss: 3.5884\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.3279 - val_loss: 3.5780\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.3199 - val_loss: 3.5704\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.3189 - val_loss: 3.5668\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.3106 - val_loss: 3.5650\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.3147 - val_loss: 3.5738\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.3007 - val_loss: 3.5587\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.2942 - val_loss: 3.5562\n",
      "Epoch 126/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 118us/step - loss: 3.2922 - val_loss: 3.5538\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.2858 - val_loss: 3.5500\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.2842 - val_loss: 3.5457\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 3.2765 - val_loss: 3.5445\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 3.2712 - val_loss: 3.5423\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.2708 - val_loss: 3.5521\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.2717 - val_loss: 3.5373\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.2579 - val_loss: 3.5452\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.2517 - val_loss: 3.5543\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.2498 - val_loss: 3.5554\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.2449 - val_loss: 3.5467\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.2385 - val_loss: 3.5347\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.2361 - val_loss: 3.5359\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.2335 - val_loss: 3.5322\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 3.2247 - val_loss: 3.5307\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 3.2204 - val_loss: 3.5282\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 3.2184 - val_loss: 3.5279\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.2198 - val_loss: 3.5471\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.2128 - val_loss: 3.5406\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.2024 - val_loss: 3.5264\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.2025 - val_loss: 3.5232\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.2041 - val_loss: 3.5187\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.1952 - val_loss: 3.5324\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.1867 - val_loss: 3.5377\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.1804 - val_loss: 3.5345\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.1742 - val_loss: 3.5332\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.1882 - val_loss: 3.5254\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.1711 - val_loss: 3.5262\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.1588 - val_loss: 3.5353\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.1630 - val_loss: 3.5423\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.1542 - val_loss: 3.5387\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 3.1493 - val_loss: 3.5478\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.1450 - val_loss: 3.5445\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.1384 - val_loss: 3.5287\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.1297 - val_loss: 3.5264\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.1306 - val_loss: 3.5346\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.1192 - val_loss: 3.5241\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.1642 - val_loss: 3.5150\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.1169 - val_loss: 3.5260\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.1093 - val_loss: 3.5213\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.1056 - val_loss: 3.5320\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.1100 - val_loss: 3.5343\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.1031 - val_loss: 3.5079\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.1007 - val_loss: 3.5052\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 3.0888 - val_loss: 3.5087\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.0810 - val_loss: 3.5149\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.0753 - val_loss: 3.5226\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.0884 - val_loss: 3.5256\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.0767 - val_loss: 3.4985\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 3.0749 - val_loss: 3.5071\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.0596 - val_loss: 3.5076\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.0568 - val_loss: 3.4991\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 3.0542 - val_loss: 3.5012\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.0502 - val_loss: 3.5054\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 3.0434 - val_loss: 3.5071\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.0374 - val_loss: 3.5064\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0325 - val_loss: 3.4990\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.0342 - val_loss: 3.5037\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 3.0439 - val_loss: 3.5179\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.0166 - val_loss: 3.5125\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.0110 - val_loss: 3.5031\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.0288 - val_loss: 3.4938\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.0138 - val_loss: 3.5043\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.0029 - val_loss: 3.5045\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.9991 - val_loss: 3.5059\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 2.9960 - val_loss: 3.5137\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.9928 - val_loss: 3.5234\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.9912 - val_loss: 3.5184\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.9876 - val_loss: 3.5288\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.9776 - val_loss: 3.5130\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.9914 - val_loss: 3.4959\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 245us/step - loss: 2.9634 - val_loss: 3.5027\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.9719 - val_loss: 3.5245\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.9663 - val_loss: 3.5183\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 181us/step - loss: 2.9610 - val_loss: 3.4978\n",
      "\n",
      "\n",
      "ESV: 0.3969738685052503\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 10.0557 - val_loss: 9.6901\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 9.0304 - val_loss: 8.6600\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 142us/step - loss: 7.9907 - val_loss: 7.6610\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 7.0191 - val_loss: 6.7149\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 6.1024 - val_loss: 5.8582\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 5.3670 - val_loss: 5.1426\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 4.7240 - val_loss: 4.6924\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 4.3386 - val_loss: 4.4836\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 4.2410 - val_loss: 4.4058\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 4.2150 - val_loss: 4.3791\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 4.1821 - val_loss: 4.3384\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 4.1423 - val_loss: 4.2978\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 4.1013 - val_loss: 4.2643\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 4.0632 - val_loss: 4.2380\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 4.0347 - val_loss: 4.2136\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 86us/step - loss: 4.0096 - val_loss: 4.1812\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.9922 - val_loss: 4.1448\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.9531 - val_loss: 4.1230\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.9312 - val_loss: 4.1108\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.9152 - val_loss: 4.0917\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 76us/step - loss: 3.8903 - val_loss: 4.0561\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.8628 - val_loss: 4.0252\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.8372 - val_loss: 4.0054\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.8134 - val_loss: 3.9813\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.7946 - val_loss: 3.9576\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.7726 - val_loss: 3.9458\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 3.7516 - val_loss: 3.9241\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.7253 - val_loss: 3.8975\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.7124 - val_loss: 3.8744\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.6965 - val_loss: 3.8629\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 3.6713 - val_loss: 3.8501\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 3.6635 - val_loss: 3.8557\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 3.6438 - val_loss: 3.8200\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 92us/step - loss: 3.6108 - val_loss: 3.8144\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.5945 - val_loss: 3.8165\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 3.5823 - val_loss: 3.8027\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.5607 - val_loss: 3.7806\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.5517 - val_loss: 3.7709\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 85us/step - loss: 3.5352 - val_loss: 3.7481\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 3.5066 - val_loss: 3.7489\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.5027 - val_loss: 3.7445\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.4771 - val_loss: 3.7122\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.4617 - val_loss: 3.6994\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.4514 - val_loss: 3.6988\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.4319 - val_loss: 3.6902\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.4123 - val_loss: 3.6669\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.4237 - val_loss: 3.6543\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 3.3979 - val_loss: 3.6796\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.3780 - val_loss: 3.6680\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.3612 - val_loss: 3.6444\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.3467 - val_loss: 3.6544\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.3359 - val_loss: 3.6510\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 77us/step - loss: 3.3206 - val_loss: 3.6550\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 3.3121 - val_loss: 3.6489\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 3.3011 - val_loss: 3.6327\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.2936 - val_loss: 3.6422\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 3.2724 - val_loss: 3.6206\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.2786 - val_loss: 3.5908\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.2719 - val_loss: 3.5971\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.2444 - val_loss: 3.6346\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.2444 - val_loss: 3.6544\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 3.2418 - val_loss: 3.6171\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.2210 - val_loss: 3.6189\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.2248 - val_loss: 3.6529\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.2130 - val_loss: 3.6299\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.2069 - val_loss: 3.6362\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.1860 - val_loss: 3.6167\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 78us/step - loss: 3.1795 - val_loss: 3.6200\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.1703 - val_loss: 3.6124\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.1627 - val_loss: 3.6147\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 3.1566 - val_loss: 3.6152\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.1422 - val_loss: 3.5927\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.1427 - val_loss: 3.5959\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 3.1310 - val_loss: 3.6181\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 3.1277 - val_loss: 3.6348\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.1208 - val_loss: 3.5964\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.1094 - val_loss: 3.6108\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 3.0978 - val_loss: 3.6147\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0975 - val_loss: 3.6514\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.0867 - val_loss: 3.6284\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.0809 - val_loss: 3.6416\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 148us/step - loss: 3.0660 - val_loss: 3.6305\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 3.0634 - val_loss: 3.6292\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.0570 - val_loss: 3.6232\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 3.0509 - val_loss: 3.6205\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.0424 - val_loss: 3.6786\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 3.0348 - val_loss: 3.6591\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.0220 - val_loss: 3.6512\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.0142 - val_loss: 3.6377\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.0103 - val_loss: 3.6463\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.9994 - val_loss: 3.6923\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.9960 - val_loss: 3.6772\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.9952 - val_loss: 3.6645\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.9899 - val_loss: 3.6977\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.9726 - val_loss: 3.6771\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.9798 - val_loss: 3.6664\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 2.9490 - val_loss: 3.7023\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.9602 - val_loss: 3.7188\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.9331 - val_loss: 3.6725\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 73us/step - loss: 2.9269 - val_loss: 3.6558\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 2.9324 - val_loss: 3.6988\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.9079 - val_loss: 3.6995\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.9105 - val_loss: 3.7153\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.9117 - val_loss: 3.6659\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 2.8875 - val_loss: 3.7081\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.8886 - val_loss: 3.7636\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 2.9044 - val_loss: 3.7144\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.8585 - val_loss: 3.7506\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.8702 - val_loss: 3.7505\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.8509 - val_loss: 3.7374\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 2.8476 - val_loss: 3.7256\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 2.8448 - val_loss: 3.7322\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.8253 - val_loss: 3.7897\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.8229 - val_loss: 3.7723\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.8103 - val_loss: 3.7495\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.7986 - val_loss: 3.7556\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.7865 - val_loss: 3.7728\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.7830 - val_loss: 3.7747\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 105us/step - loss: 2.7764 - val_loss: 3.7545\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.7700 - val_loss: 3.7732\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.7818 - val_loss: 3.7577\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.7479 - val_loss: 3.7741\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.7554 - val_loss: 3.7896\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.7343 - val_loss: 3.7600\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.7460 - val_loss: 3.7782\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.7210 - val_loss: 3.7792\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.7438 - val_loss: 3.7724\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.7329 - val_loss: 3.7755\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.7123 - val_loss: 3.7806\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 2.7006 - val_loss: 3.7679\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.6836 - val_loss: 3.8068\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.6737 - val_loss: 3.7877\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.6641 - val_loss: 3.7722\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.6582 - val_loss: 3.7835\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 2.6791 - val_loss: 3.7798\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.6801 - val_loss: 3.8115\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.6400 - val_loss: 3.7748\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.6379 - val_loss: 3.8058\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.6218 - val_loss: 3.8557\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.6033 - val_loss: 3.8547\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.6020 - val_loss: 3.8569\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.5987 - val_loss: 3.8699\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 2.5729 - val_loss: 3.8955\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.5639 - val_loss: 3.9033\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.5646 - val_loss: 3.8826\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.5455 - val_loss: 3.8769\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.5367 - val_loss: 3.8716\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.5274 - val_loss: 3.9018\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.5372 - val_loss: 3.8457\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.5060 - val_loss: 3.8801\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 2.5033 - val_loss: 3.8858\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.4799 - val_loss: 3.8782\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.4768 - val_loss: 3.8823\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.4644 - val_loss: 3.8646\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.4693 - val_loss: 3.9022\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.4552 - val_loss: 3.9206\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.4335 - val_loss: 3.9026\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.4294 - val_loss: 3.9053\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.4364 - val_loss: 3.9102\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 99us/step - loss: 2.4141 - val_loss: 3.9189\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.4136 - val_loss: 3.9072\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.3932 - val_loss: 3.9605\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.3977 - val_loss: 3.9484\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.4127 - val_loss: 3.9600\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.4474 - val_loss: 3.9754\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 85us/step - loss: 2.3797 - val_loss: 3.9742\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 101us/step - loss: 2.3523 - val_loss: 3.9999\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 2.3488 - val_loss: 4.0179\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.3419 - val_loss: 3.9998\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.3244 - val_loss: 3.9468\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.3182 - val_loss: 3.9568\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.3140 - val_loss: 3.9866\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.2962 - val_loss: 3.9607\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.2969 - val_loss: 3.9541\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.2682 - val_loss: 3.9998\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.2734 - val_loss: 3.9996\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 77us/step - loss: 2.2763 - val_loss: 3.9975\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.2498 - val_loss: 4.0282\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.2516 - val_loss: 4.0425\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 77us/step - loss: 2.2330 - val_loss: 4.0116\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.2503 - val_loss: 4.0015\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.2124 - val_loss: 4.0025\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 2.2380 - val_loss: 4.0324\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.2203 - val_loss: 4.0594\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.1892 - val_loss: 4.0379\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 103us/step - loss: 2.1897 - val_loss: 4.0716\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.1696 - val_loss: 4.0709\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.1733 - val_loss: 4.0848\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.1427 - val_loss: 4.0711\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.1506 - val_loss: 4.0773\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.1507 - val_loss: 4.1130\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.1366 - val_loss: 4.0849\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 2.1255 - val_loss: 4.0515\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.0996 - val_loss: 4.1082\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.1027 - val_loss: 4.1458\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.0894 - val_loss: 4.1271\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.0865 - val_loss: 4.1657\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.0699 - val_loss: 4.1667\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.0712 - val_loss: 4.1937\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 2.0527 - val_loss: 4.1802\n",
      "\n",
      "\n",
      "ESV: 0.27618528768068595\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 0s 2ms/step - loss: 10.4027 - val_loss: 9.9628\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 205us/step - loss: 9.3477 - val_loss: 8.9392\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 8.2099 - val_loss: 7.8770\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 7.1293 - val_loss: 6.8548\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 6.0874 - val_loss: 5.9687\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 5.3274 - val_loss: 5.2915\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 4.7502 - val_loss: 4.9017\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 4.4252 - val_loss: 4.7267\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 4.3486 - val_loss: 4.6683\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 4.3384 - val_loss: 4.6472\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 4.3169 - val_loss: 4.6163\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 4.2743 - val_loss: 4.5789\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 4.2321 - val_loss: 4.5517\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 4.2020 - val_loss: 4.5191\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 4.1719 - val_loss: 4.4744\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 4.1335 - val_loss: 4.4301\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 4.0937 - val_loss: 4.3880\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 4.0603 - val_loss: 4.3557\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 4.0230 - val_loss: 4.3097\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.9848 - val_loss: 4.2700\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 3.9557 - val_loss: 4.2235\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.9202 - val_loss: 4.1840\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 3.8858 - val_loss: 4.1544\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.8526 - val_loss: 4.1296\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 3.8293 - val_loss: 4.0862\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 3.7885 - val_loss: 4.0665\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 3.7644 - val_loss: 4.0503\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.7371 - val_loss: 4.0215\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 3.7152 - val_loss: 3.9582\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 3.6844 - val_loss: 3.9460\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.6500 - val_loss: 3.9280\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 167us/step - loss: 3.6305 - val_loss: 3.8906\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 3.5969 - val_loss: 3.8735\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 3.5773 - val_loss: 3.8611\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 174us/step - loss: 3.5502 - val_loss: 3.8376\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.5311 - val_loss: 3.8069\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.5027 - val_loss: 3.8004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.5050 - val_loss: 3.7643\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 3.4541 - val_loss: 3.7617\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.4350 - val_loss: 3.7660\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.4318 - val_loss: 3.7627\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.3988 - val_loss: 3.7211\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 3.3868 - val_loss: 3.6816\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.3629 - val_loss: 3.6693\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.3385 - val_loss: 3.6581\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 3.3413 - val_loss: 3.6907\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.3199 - val_loss: 3.6343\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 3.2796 - val_loss: 3.6234\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 3.2677 - val_loss: 3.6108\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 3.2429 - val_loss: 3.6192\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.2343 - val_loss: 3.6363\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.2371 - val_loss: 3.6222\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 3.2118 - val_loss: 3.6106\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 180us/step - loss: 3.1943 - val_loss: 3.5974\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 3.2277 - val_loss: 3.5643\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.1675 - val_loss: 3.5772\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.1625 - val_loss: 3.6011\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 3.1576 - val_loss: 3.5568\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 3.1365 - val_loss: 3.5694\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.1155 - val_loss: 3.5653\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 3.1317 - val_loss: 3.5742\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 3.0939 - val_loss: 3.5512\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 3.1001 - val_loss: 3.5451\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.0994 - val_loss: 3.5349\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.0795 - val_loss: 3.5333\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.0791 - val_loss: 3.5949\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 3.0855 - val_loss: 3.5614\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.0368 - val_loss: 3.5590\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.0248 - val_loss: 3.5492\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.0173 - val_loss: 3.5445\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0169 - val_loss: 3.5595\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 3.0157 - val_loss: 3.5508\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 2.9886 - val_loss: 3.5720\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.9889 - val_loss: 3.5784\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 2.9713 - val_loss: 3.5557\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 2.9589 - val_loss: 3.5461\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.9561 - val_loss: 3.5398\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.9406 - val_loss: 3.5388\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.9265 - val_loss: 3.5479\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.9168 - val_loss: 3.5442\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 179us/step - loss: 2.9038 - val_loss: 3.5378\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.8842 - val_loss: 3.5587\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.8805 - val_loss: 3.5708\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.8641 - val_loss: 3.5588\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 2.8444 - val_loss: 3.5448\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 2.8336 - val_loss: 3.5466\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 2.8248 - val_loss: 3.5516\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.8245 - val_loss: 3.5364\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.8034 - val_loss: 3.5722\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.8095 - val_loss: 3.5289\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.7678 - val_loss: 3.5304\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.7681 - val_loss: 3.5255\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.7654 - val_loss: 3.5275\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.7432 - val_loss: 3.5218\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.7446 - val_loss: 3.5120\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.7084 - val_loss: 3.5170\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.7089 - val_loss: 3.5288\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.6836 - val_loss: 3.5453\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 2.6806 - val_loss: 3.5491\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.6683 - val_loss: 3.5467\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.6565 - val_loss: 3.5471\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.6609 - val_loss: 3.5407\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.6514 - val_loss: 3.5633\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.6309 - val_loss: 3.5504\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.6184 - val_loss: 3.5478\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.6020 - val_loss: 3.5603\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.5989 - val_loss: 3.5623\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.5790 - val_loss: 3.5420\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 2.5772 - val_loss: 3.5351\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.5797 - val_loss: 3.5745\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.5872 - val_loss: 3.5395\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 158us/step - loss: 2.5433 - val_loss: 3.5596\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 172us/step - loss: 2.5399 - val_loss: 3.5563\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.5276 - val_loss: 3.5522\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.5184 - val_loss: 3.5539\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 66us/step - loss: 2.5170 - val_loss: 3.5434\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.4990 - val_loss: 3.5354\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.4834 - val_loss: 3.5497\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 90us/step - loss: 2.4962 - val_loss: 3.5683\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 88us/step - loss: 2.4876 - val_loss: 3.5327\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.4535 - val_loss: 3.5391\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.4596 - val_loss: 3.5578\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.4552 - val_loss: 3.5295\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 2.4413 - val_loss: 3.5534\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 2.4109 - val_loss: 3.5472\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.4162 - val_loss: 3.5575\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.4254 - val_loss: 3.5703\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.3845 - val_loss: 3.5493\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.3832 - val_loss: 3.5333\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.3609 - val_loss: 3.5432\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.3630 - val_loss: 3.5500\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.3850 - val_loss: 3.5408\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.3619 - val_loss: 3.5748\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.3448 - val_loss: 3.5696\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.3133 - val_loss: 3.5568\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 2.3084 - val_loss: 3.5456\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.3043 - val_loss: 3.5636\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 2.3251 - val_loss: 3.5912\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 2.2955 - val_loss: 3.5552\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.2831 - val_loss: 3.5669\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.2800 - val_loss: 3.5875\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.2614 - val_loss: 3.5944\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.2554 - val_loss: 3.5740\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 2.2481 - val_loss: 3.5492\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.2711 - val_loss: 3.5527\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.2131 - val_loss: 3.5854\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.2502 - val_loss: 3.6218\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 2.2124 - val_loss: 3.5839\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.2507 - val_loss: 3.5820\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.2305 - val_loss: 3.6301\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.2075 - val_loss: 3.5917\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 2.1783 - val_loss: 3.5823\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.1720 - val_loss: 3.5956\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 128us/step - loss: 2.1598 - val_loss: 3.6018\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.1607 - val_loss: 3.5914\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 2.1524 - val_loss: 3.5744\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.1773 - val_loss: 3.6002\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.1261 - val_loss: 3.5819\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.1339 - val_loss: 3.5758\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.1355 - val_loss: 3.6039\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.1230 - val_loss: 3.5921\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 2.2058 - val_loss: 3.5718\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.1292 - val_loss: 3.6231\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.1009 - val_loss: 3.6009\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.1270 - val_loss: 3.6142\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 2.0834 - val_loss: 3.6653\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.1036 - val_loss: 3.6363\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.0683 - val_loss: 3.6450\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 2.0675 - val_loss: 3.6314\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.0584 - val_loss: 3.6462\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.0400 - val_loss: 3.6132\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.0414 - val_loss: 3.6281\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.0445 - val_loss: 3.6420\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 2.0310 - val_loss: 3.6495\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 2.0262 - val_loss: 3.6324\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 88us/step - loss: 2.0062 - val_loss: 3.6655\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.0099 - val_loss: 3.6843\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.0171 - val_loss: 3.6819\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 1.9903 - val_loss: 3.6649\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 1.9883 - val_loss: 3.6574\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 1.9715 - val_loss: 3.6694\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 1.9821 - val_loss: 3.7006\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 1.9686 - val_loss: 3.6970\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 1.9698 - val_loss: 3.7246\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 1.9558 - val_loss: 3.7273\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 1.9489 - val_loss: 3.7173\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 96us/step - loss: 1.9448 - val_loss: 3.7343\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 1.9369 - val_loss: 3.7281\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 1.9484 - val_loss: 3.7128\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 1.9239 - val_loss: 3.7256\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 1.9368 - val_loss: 3.7479\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 1.9149 - val_loss: 3.7665\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 1.9285 - val_loss: 3.7473\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 126us/step - loss: 1.8997 - val_loss: 3.7352\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 1.9098 - val_loss: 3.7575\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 149us/step - loss: 1.9007 - val_loss: 3.7696\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 98us/step - loss: 1.8881 - val_loss: 3.7948\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 1.8746 - val_loss: 3.7735\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 1.8936 - val_loss: 3.8014\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 1.8867 - val_loss: 3.8152\n",
      "\n",
      "\n",
      "ESV: 0.33989297733472634\n",
      "\n",
      "\n",
      "\n",
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 1s 2ms/step - loss: 11.3418 - val_loss: 11.2007\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 10.9623 - val_loss: 10.8735\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 10.6213 - val_loss: 10.5393\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 10.2667 - val_loss: 10.1741\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 9.8417 - val_loss: 9.7223\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 9.3195 - val_loss: 9.1610\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 8.6956 - val_loss: 8.4821\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 7.9497 - val_loss: 7.7292\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 163us/step - loss: 7.1160 - val_loss: 6.9792\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 6.3630 - val_loss: 6.2903\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 5.6721 - val_loss: 5.7353\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 5.1786 - val_loss: 5.3594\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 4.8718 - val_loss: 5.1841\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 4.7570 - val_loss: 5.1276\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 4.7285 - val_loss: 5.1001\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 4.7093 - val_loss: 5.0615\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 147us/step - loss: 4.6768 - val_loss: 5.0267\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 4.6477 - val_loss: 4.9912\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 151us/step - loss: 4.6171 - val_loss: 4.9637\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 97us/step - loss: 4.5967 - val_loss: 4.9394\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 4.5685 - val_loss: 4.9077\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 4.5442 - val_loss: 4.8753\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 4.5165 - val_loss: 4.8459\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 4.4908 - val_loss: 4.8191\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 4.4623 - val_loss: 4.7886\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 4.4347 - val_loss: 4.7604\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 4.4113 - val_loss: 4.7312\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 4.3798 - val_loss: 4.7029\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 4.3541 - val_loss: 4.6740\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 4.3284 - val_loss: 4.6484\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 4.2967 - val_loss: 4.6266\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 4.2738 - val_loss: 4.6095\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 4.2449 - val_loss: 4.5729\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 146us/step - loss: 4.2160 - val_loss: 4.5272\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 4.1867 - val_loss: 4.4980\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 108us/step - loss: 4.1510 - val_loss: 4.4607\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 4.1203 - val_loss: 4.4241\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 156us/step - loss: 4.1061 - val_loss: 4.3822\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 4.0617 - val_loss: 4.3580\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 104us/step - loss: 4.0319 - val_loss: 4.3356\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 3.9987 - val_loss: 4.3154\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 94us/step - loss: 3.9741 - val_loss: 4.2929\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.9639 - val_loss: 4.2884\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 111us/step - loss: 3.9238 - val_loss: 4.2393\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 3.8996 - val_loss: 4.2006\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.8711 - val_loss: 4.1839\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.8443 - val_loss: 4.1547\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 3.8203 - val_loss: 4.1399\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.7965 - val_loss: 4.1268\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 3.7622 - val_loss: 4.0759\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 3.7395 - val_loss: 4.0453\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 99us/step - loss: 3.7216 - val_loss: 4.0251\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.6869 - val_loss: 4.0313\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 3.6709 - val_loss: 4.0177\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 3.6628 - val_loss: 4.0098\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 3.6277 - val_loss: 3.9590\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.6101 - val_loss: 3.9389\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.5990 - val_loss: 3.9208\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.5694 - val_loss: 3.9196\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 3.5559 - val_loss: 3.9268\n",
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 134us/step - loss: 3.5382 - val_loss: 3.8867\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 160us/step - loss: 3.5213 - val_loss: 3.8564\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.5094 - val_loss: 3.8427\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.4974 - val_loss: 3.8679\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 3.4847 - val_loss: 3.8571\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.4872 - val_loss: 3.8024\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 3.4701 - val_loss: 3.7931\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.4458 - val_loss: 3.8190\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 176us/step - loss: 3.4367 - val_loss: 3.8218\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 3.4260 - val_loss: 3.8268\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.4142 - val_loss: 3.8263\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 116us/step - loss: 3.4079 - val_loss: 3.8103\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.4074 - val_loss: 3.8133\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.3908 - val_loss: 3.7615\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.3916 - val_loss: 3.7533\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.3814 - val_loss: 3.7571\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.3838 - val_loss: 3.8097\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 100us/step - loss: 3.3660 - val_loss: 3.7827\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 3.3592 - val_loss: 3.7519\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 3.3521 - val_loss: 3.7541\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 129us/step - loss: 3.3430 - val_loss: 3.7893\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 153us/step - loss: 3.3558 - val_loss: 3.8005\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.3394 - val_loss: 3.7750\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 3.3305 - val_loss: 3.7241\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 3.3175 - val_loss: 3.7426\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 150us/step - loss: 3.2931 - val_loss: 3.7506\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 161us/step - loss: 3.2849 - val_loss: 3.7786\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 200us/step - loss: 3.2831 - val_loss: 3.7672\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 119us/step - loss: 3.2604 - val_loss: 3.7276\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.2644 - val_loss: 3.7123\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 3.2432 - val_loss: 3.7290\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 3.2372 - val_loss: 3.7084\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 3.2205 - val_loss: 3.7019\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 91us/step - loss: 3.2102 - val_loss: 3.7040\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.1973 - val_loss: 3.6472\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.1860 - val_loss: 3.6674\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 165us/step - loss: 3.1695 - val_loss: 3.6667\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 3.1662 - val_loss: 3.6602\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 3.1593 - val_loss: 3.6332\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 3.1365 - val_loss: 3.6597\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 3.1221 - val_loss: 3.6691\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 3.1279 - val_loss: 3.6769\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.1054 - val_loss: 3.6237\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 3.1130 - val_loss: 3.6225\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 3.0788 - val_loss: 3.6556\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 3.0706 - val_loss: 3.6472\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 112us/step - loss: 3.0804 - val_loss: 3.6026\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 3.0345 - val_loss: 3.6430\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 3.0414 - val_loss: 3.6616\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 3.0147 - val_loss: 3.6121\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 3.0084 - val_loss: 3.5947\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.9977 - val_loss: 3.6189\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 3.0047 - val_loss: 3.7083\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.9832 - val_loss: 3.6294\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.9773 - val_loss: 3.6019\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 2.9578 - val_loss: 3.6312\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.9524 - val_loss: 3.6251\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.9499 - val_loss: 3.6414\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.9540 - val_loss: 3.6396\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 116us/step - loss: 2.9181 - val_loss: 3.5966\n",
      "Epoch 121/200\n",
      "207/207 [==============================] - 0s 131us/step - loss: 2.9029 - val_loss: 3.6061\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.9159 - val_loss: 3.6519\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.8876 - val_loss: 3.6040\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.8729 - val_loss: 3.6175\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.8509 - val_loss: 3.5973\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 155us/step - loss: 2.8533 - val_loss: 3.6034\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 86us/step - loss: 2.8325 - val_loss: 3.6472\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.8308 - val_loss: 3.6157\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.8182 - val_loss: 3.5935\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 162us/step - loss: 2.8108 - val_loss: 3.6164\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.8012 - val_loss: 3.6237\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.7899 - val_loss: 3.6067\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.7705 - val_loss: 3.6249\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.7830 - val_loss: 3.6100\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 148us/step - loss: 2.7694 - val_loss: 3.6554\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 117us/step - loss: 2.7407 - val_loss: 3.6129\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 124us/step - loss: 2.7448 - val_loss: 3.6081\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.7371 - val_loss: 3.6190\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 166us/step - loss: 2.7203 - val_loss: 3.6389\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 137us/step - loss: 2.7157 - val_loss: 3.5901\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 141us/step - loss: 2.6972 - val_loss: 3.6262\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 102us/step - loss: 2.6845 - val_loss: 3.6477\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 157us/step - loss: 2.6822 - val_loss: 3.6585\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 168us/step - loss: 2.6739 - val_loss: 3.6456\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 144us/step - loss: 2.6458 - val_loss: 3.6164\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.6624 - val_loss: 3.6085\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 152us/step - loss: 2.6456 - val_loss: 3.6644\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.6334 - val_loss: 3.6324\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 168us/step - loss: 2.6968 - val_loss: 3.5846\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 146us/step - loss: 2.6170 - val_loss: 3.7222\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 71us/step - loss: 2.6114 - val_loss: 3.6419\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.5485 - val_loss: 3.6021\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.5911 - val_loss: 3.5845\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.5699 - val_loss: 3.6214\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 138us/step - loss: 2.5602 - val_loss: 3.5527\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.5373 - val_loss: 3.6014\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.5833 - val_loss: 3.7645\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.5580 - val_loss: 3.6029\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 114us/step - loss: 2.4858 - val_loss: 3.6293\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.4662 - val_loss: 3.6307\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.4575 - val_loss: 3.6288\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 113us/step - loss: 2.4452 - val_loss: 3.5746\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 2.4428 - val_loss: 3.6370\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.4056 - val_loss: 3.5855\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.3907 - val_loss: 3.5918\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 89us/step - loss: 2.3747 - val_loss: 3.6356\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 130us/step - loss: 2.3941 - val_loss: 3.6430\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 143us/step - loss: 2.3574 - val_loss: 3.5923\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 107us/step - loss: 2.3426 - val_loss: 3.6342\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 125us/step - loss: 2.3497 - val_loss: 3.6405\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 135us/step - loss: 2.3128 - val_loss: 3.6192\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.3026 - val_loss: 3.6533\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 84us/step - loss: 2.3020 - val_loss: 3.6829\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.2966 - val_loss: 3.6477\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 123us/step - loss: 2.2710 - val_loss: 3.6306\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 83us/step - loss: 2.2626 - val_loss: 3.6282\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 106us/step - loss: 2.2532 - val_loss: 3.6194\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 110us/step - loss: 2.2618 - val_loss: 3.6207\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 154us/step - loss: 2.2568 - val_loss: 3.6832\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 120us/step - loss: 2.2650 - val_loss: 3.6242\n",
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 142us/step - loss: 2.2571 - val_loss: 3.6991\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.2123 - val_loss: 3.6355\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 132us/step - loss: 2.2126 - val_loss: 3.6426\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.2294 - val_loss: 3.6213\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 140us/step - loss: 2.2506 - val_loss: 3.6763\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.1953 - val_loss: 3.6302\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 145us/step - loss: 2.1777 - val_loss: 3.6503\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 126us/step - loss: 2.1698 - val_loss: 3.6549\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 139us/step - loss: 2.1691 - val_loss: 3.6204\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 2.1641 - val_loss: 3.6504\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 136us/step - loss: 2.1531 - val_loss: 3.6140\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 133us/step - loss: 2.1303 - val_loss: 3.6533\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 122us/step - loss: 2.1295 - val_loss: 3.6329\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 109us/step - loss: 2.1179 - val_loss: 3.6289\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 121us/step - loss: 2.1111 - val_loss: 3.6567\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 118us/step - loss: 2.1065 - val_loss: 3.6368\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 93us/step - loss: 2.0960 - val_loss: 3.6305\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 127us/step - loss: 2.1049 - val_loss: 3.6617\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 95us/step - loss: 2.0974 - val_loss: 3.6343\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 115us/step - loss: 2.0963 - val_loss: 3.6838\n",
      "\n",
      "\n",
      "ESV: 0.36424271147821086\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'winsound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c2960d23fae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mwinsound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m375\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'winsound' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    X_train.shape\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    model.fit(x=X_train, y= y_train, validation_data=(X_test,y_test),batch_size=32,epochs=200)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    print('\\n'+'\\n'+\"ESV: \" + str(explained_variance_score(y_test,predictions))+ '\\n'+'\\n'+'\\n')\n",
    "    if(explained_variance_score(y_test,predictions) > .58):\n",
    "        winsound.Beep(512, 375)\n",
    "        break\n",
    "\n",
    "winsound.Beep(512, 375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esv =np.empty(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
