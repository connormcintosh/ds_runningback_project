{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import winsound\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../../data/processed/oline_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"../../data/processed/oline_merge.csv\")\n",
    "df = df[df.year >= df.draft_yr+3]\n",
    "df = df[df['Percenthit (%)'] <= 6]\n",
    "# df =df[df.year != 2020]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>year</th>\n",
       "      <th>age</th>\n",
       "      <th>draft_yr</th>\n",
       "      <th>draft_pos</th>\n",
       "      <th>attempts</th>\n",
       "      <th>yards_run</th>\n",
       "      <th>tds_run</th>\n",
       "      <th>longgain_run</th>\n",
       "      <th>...</th>\n",
       "      <th>tds_rec</th>\n",
       "      <th>firstdowns</th>\n",
       "      <th>longgain_rec</th>\n",
       "      <th>yardspertarget</th>\n",
       "      <th>recpergame</th>\n",
       "      <th>yardspergame_rec</th>\n",
       "      <th>fumbles</th>\n",
       "      <th>team_adjusted_line_yards</th>\n",
       "      <th>team_running_back_yards</th>\n",
       "      <th>team_stuffed_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>304.513011</td>\n",
       "      <td>304.513011</td>\n",
       "      <td>2015.156134</td>\n",
       "      <td>27.460967</td>\n",
       "      <td>2009.903346</td>\n",
       "      <td>114.553903</td>\n",
       "      <td>132.527881</td>\n",
       "      <td>558.007435</td>\n",
       "      <td>3.866171</td>\n",
       "      <td>36.167286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921933</td>\n",
       "      <td>9.405204</td>\n",
       "      <td>28.386617</td>\n",
       "      <td>5.827881</td>\n",
       "      <td>2.112268</td>\n",
       "      <td>16.586989</td>\n",
       "      <td>1.587361</td>\n",
       "      <td>16.903346</td>\n",
       "      <td>17.609665</td>\n",
       "      <td>16.130112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>163.572650</td>\n",
       "      <td>163.572650</td>\n",
       "      <td>3.266429</td>\n",
       "      <td>2.496894</td>\n",
       "      <td>3.845664</td>\n",
       "      <td>93.322592</td>\n",
       "      <td>89.170578</td>\n",
       "      <td>396.784764</td>\n",
       "      <td>3.598913</td>\n",
       "      <td>20.313722</td>\n",
       "      <td>...</td>\n",
       "      <td>1.247925</td>\n",
       "      <td>7.007148</td>\n",
       "      <td>15.419240</td>\n",
       "      <td>1.777976</td>\n",
       "      <td>1.209986</td>\n",
       "      <td>10.729167</td>\n",
       "      <td>1.500245</td>\n",
       "      <td>9.207291</td>\n",
       "      <td>9.091577</td>\n",
       "      <td>9.050531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>863.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>599.000000</td>\n",
       "      <td>599.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>1845.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>58.700000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1         year         age     draft_yr  \\\n",
       "count  269.000000    269.000000   269.000000  269.000000   269.000000   \n",
       "mean   304.513011    304.513011  2015.156134   27.460967  2009.903346   \n",
       "std    163.572650    163.572650     3.266429    2.496894     3.845664   \n",
       "min      0.000000      0.000000  2005.000000   22.000000  1999.000000   \n",
       "25%    178.000000    178.000000  2013.000000   26.000000  2007.000000   \n",
       "50%    303.000000    303.000000  2015.000000   27.000000  2010.000000   \n",
       "75%    449.000000    449.000000  2018.000000   29.000000  2013.000000   \n",
       "max    599.000000    599.000000  2020.000000   37.000000  2017.000000   \n",
       "\n",
       "        draft_pos    attempts    yards_run     tds_run  longgain_run  ...  \\\n",
       "count  269.000000  269.000000   269.000000  269.000000    269.000000  ...   \n",
       "mean   114.553903  132.527881   558.007435    3.866171     36.167286  ...   \n",
       "std     93.322592   89.170578   396.784764    3.598913     20.313722  ...   \n",
       "min      2.000000    2.000000    -3.000000    0.000000      3.000000  ...   \n",
       "25%     36.000000   51.000000   223.000000    1.000000     20.000000  ...   \n",
       "50%     73.000000  119.000000   473.000000    3.000000     32.000000  ...   \n",
       "75%    227.000000  206.000000   863.000000    6.000000     48.000000  ...   \n",
       "max    257.000000  392.000000  1845.000000   18.000000     97.000000  ...   \n",
       "\n",
       "          tds_rec  firstdowns  longgain_rec  yardspertarget  recpergame  \\\n",
       "count  269.000000  269.000000    269.000000      269.000000  269.000000   \n",
       "mean     0.921933    9.405204     28.386617        5.827881    2.112268   \n",
       "std      1.247925    7.007148     15.419240        1.777976    1.209986   \n",
       "min      0.000000    0.000000      3.000000       -0.400000    0.100000   \n",
       "25%      0.000000    4.000000     17.000000        4.700000    1.100000   \n",
       "50%      1.000000    8.000000     25.000000        5.800000    2.000000   \n",
       "75%      1.000000   13.000000     35.000000        6.600000    2.900000   \n",
       "max      7.000000   32.000000     80.000000       12.200000    6.300000   \n",
       "\n",
       "       yardspergame_rec     fumbles  team_adjusted_line_yards  \\\n",
       "count        269.000000  269.000000                269.000000   \n",
       "mean          16.586989    1.587361                 16.903346   \n",
       "std           10.729167    1.500245                  9.207291   \n",
       "min           -0.300000    0.000000                  1.000000   \n",
       "25%            8.400000    0.000000                  9.000000   \n",
       "50%           14.500000    1.000000                 17.000000   \n",
       "75%           23.400000    3.000000                 24.000000   \n",
       "max           58.700000    6.000000                 32.000000   \n",
       "\n",
       "       team_running_back_yards  team_stuffed_rate  \n",
       "count               269.000000         269.000000  \n",
       "mean                 17.609665          16.130112  \n",
       "std                   9.091577           9.050531  \n",
       "min                   1.000000           1.000000  \n",
       "25%                  10.000000           8.000000  \n",
       "50%                  18.000000          16.000000  \n",
       "75%                  25.000000          24.000000  \n",
       "max                  32.000000          32.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>draft_pos</th>\n",
       "      <th>attempts</th>\n",
       "      <th>yards_run</th>\n",
       "      <th>tds_run</th>\n",
       "      <th>longgain_run</th>\n",
       "      <th>Percenthit (%)</th>\n",
       "      <th>g</th>\n",
       "      <th>gs</th>\n",
       "      <th>tgt</th>\n",
       "      <th>rec</th>\n",
       "      <th>yards_rec</th>\n",
       "      <th>tds_rec</th>\n",
       "      <th>firstdowns</th>\n",
       "      <th>longgain_rec</th>\n",
       "      <th>fumbles</th>\n",
       "      <th>team_adjusted_line_yards</th>\n",
       "      <th>team_running_back_yards</th>\n",
       "      <th>team_stuffed_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>27.460967</td>\n",
       "      <td>114.553903</td>\n",
       "      <td>132.527881</td>\n",
       "      <td>558.007435</td>\n",
       "      <td>3.866171</td>\n",
       "      <td>36.167286</td>\n",
       "      <td>2.016015</td>\n",
       "      <td>12.472119</td>\n",
       "      <td>7.312268</td>\n",
       "      <td>35.271375</td>\n",
       "      <td>26.483271</td>\n",
       "      <td>210.040892</td>\n",
       "      <td>0.921933</td>\n",
       "      <td>9.405204</td>\n",
       "      <td>28.386617</td>\n",
       "      <td>1.587361</td>\n",
       "      <td>16.903346</td>\n",
       "      <td>17.609665</td>\n",
       "      <td>16.130112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.496894</td>\n",
       "      <td>93.322592</td>\n",
       "      <td>89.170578</td>\n",
       "      <td>396.784764</td>\n",
       "      <td>3.598913</td>\n",
       "      <td>20.313722</td>\n",
       "      <td>1.460674</td>\n",
       "      <td>3.843506</td>\n",
       "      <td>5.576393</td>\n",
       "      <td>22.832877</td>\n",
       "      <td>17.482526</td>\n",
       "      <td>149.632816</td>\n",
       "      <td>1.247925</td>\n",
       "      <td>7.007148</td>\n",
       "      <td>15.419240</td>\n",
       "      <td>1.500245</td>\n",
       "      <td>9.207291</td>\n",
       "      <td>9.091577</td>\n",
       "      <td>9.050531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.100644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.727987</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.658375</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>863.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.886179</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>1845.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>5.944167</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age   draft_pos    attempts    yards_run     tds_run  \\\n",
       "count  269.000000  269.000000  269.000000   269.000000  269.000000   \n",
       "mean    27.460967  114.553903  132.527881   558.007435    3.866171   \n",
       "std      2.496894   93.322592   89.170578   396.784764    3.598913   \n",
       "min     22.000000    2.000000    2.000000    -3.000000    0.000000   \n",
       "25%     26.000000   36.000000   51.000000   223.000000    1.000000   \n",
       "50%     27.000000   73.000000  119.000000   473.000000    3.000000   \n",
       "75%     29.000000  227.000000  206.000000   863.000000    6.000000   \n",
       "max     37.000000  257.000000  392.000000  1845.000000   18.000000   \n",
       "\n",
       "       longgain_run  Percenthit (%)           g          gs         tgt  \\\n",
       "count    269.000000      269.000000  269.000000  269.000000  269.000000   \n",
       "mean      36.167286        2.016015   12.472119    7.312268   35.271375   \n",
       "std       20.313722        1.460674    3.843506    5.576393   22.832877   \n",
       "min        3.000000        0.100644    1.000000    0.000000    2.000000   \n",
       "25%       20.000000        0.727987   10.000000    2.000000   16.000000   \n",
       "50%       32.000000        1.658375   14.000000    7.000000   30.000000   \n",
       "75%       48.000000        2.886179   16.000000   12.000000   51.000000   \n",
       "max       97.000000        5.944167   16.000000   16.000000  104.000000   \n",
       "\n",
       "              rec   yards_rec     tds_rec  firstdowns  longgain_rec  \\\n",
       "count  269.000000  269.000000  269.000000  269.000000    269.000000   \n",
       "mean    26.483271  210.040892    0.921933    9.405204     28.386617   \n",
       "std     17.482526  149.632816    1.247925    7.007148     15.419240   \n",
       "min      2.000000   -2.000000    0.000000    0.000000      3.000000   \n",
       "25%     12.000000   87.000000    0.000000    4.000000     17.000000   \n",
       "50%     22.000000  180.000000    1.000000    8.000000     25.000000   \n",
       "75%     38.000000  314.000000    1.000000   13.000000     35.000000   \n",
       "max     77.000000  704.000000    7.000000   32.000000     80.000000   \n",
       "\n",
       "          fumbles  team_adjusted_line_yards  team_running_back_yards  \\\n",
       "count  269.000000                269.000000               269.000000   \n",
       "mean     1.587361                 16.903346                17.609665   \n",
       "std      1.500245                  9.207291                 9.091577   \n",
       "min      0.000000                  1.000000                 1.000000   \n",
       "25%      0.000000                  9.000000                10.000000   \n",
       "50%      1.000000                 17.000000                18.000000   \n",
       "75%      3.000000                 24.000000                25.000000   \n",
       "max      6.000000                 32.000000                32.000000   \n",
       "\n",
       "       team_stuffed_rate  \n",
       "count         269.000000  \n",
       "mean           16.130112  \n",
       "std             9.050531  \n",
       "min             1.000000  \n",
       "25%             8.000000  \n",
       "50%            16.000000  \n",
       "75%            24.000000  \n",
       "max            32.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('year',axis=1)\n",
    "df = df.drop('playername',axis=1)\n",
    "df = df.drop('team',axis=1)\n",
    "df = df.drop('basesalarycap (10^8)',axis=1)\n",
    "df = df.drop('cashspent (M)',axis=1)\n",
    "df = df.drop('caphit (M)',axis=1)\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.drop('Unnamed: 0.1',axis=1)\n",
    "df = df.drop('catchpercent',axis=1)\n",
    "df = df.drop('draft_yr',axis=1)\n",
    "\n",
    "df = df.drop('yardspergame_run',axis=1)\n",
    "df = df.drop('yardsperatt',axis=1)\n",
    "df = df.drop('yardspertarget',axis=1)\n",
    "df = df.drop('yardsperrec',axis=1)\n",
    "df = df.drop('recpergame',axis=1)\n",
    "df = df.drop('yardspergame_rec',axis=1)\n",
    "\n",
    "# df = df.drop('yards_run',axis=1)\n",
    "# df = df.drop('yards_rec',axis=1)\n",
    "# df = df.drop('attempts',axis=1)\n",
    "# df = df.drop('rec',axis=1)\n",
    "# df = df.drop('recpergame',axis=1)\n",
    "# df = df.drop('yardspergame_rec',axis=1)\n",
    "# df = df.drop('yardspergame_run',axis=1)\n",
    "# df = df.drop('tgt',axis=1)\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Percenthit (%)',axis=1).values\n",
    "y = df['Percenthit (%)'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=  41 , b=  47 , c=  42\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.6866 - val_loss: 2.9217\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.3152 - val_loss: 1.9262\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.8604 - val_loss: 1.9064\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.8326 - val_loss: 1.8821\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8072 - val_loss: 1.8813\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7761 - val_loss: 1.8594\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7775 - val_loss: 1.8575\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7486 - val_loss: 1.8320\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7634 - val_loss: 1.8130\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.7130 - val_loss: 1.7981\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7156 - val_loss: 1.7941\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6756 - val_loss: 1.7643\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6706 - val_loss: 1.7503\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6557 - val_loss: 1.7392\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6709 - val_loss: 1.7795\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.6350 - val_loss: 1.7455\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6167 - val_loss: 1.6893\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6129 - val_loss: 1.6891\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6123 - val_loss: 1.6705\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5855 - val_loss: 1.6464\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5848 - val_loss: 1.6345\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5947 - val_loss: 1.6330\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6093 - val_loss: 1.8185\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6353 - val_loss: 1.7071\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5687 - val_loss: 1.6638\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5802 - val_loss: 1.5928\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5910 - val_loss: 1.7272\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5750 - val_loss: 1.5919\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5835 - val_loss: 1.5627\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5416 - val_loss: 1.5669\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5391 - val_loss: 1.8188\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5455 - val_loss: 1.6020\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5361 - val_loss: 1.6444\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5499 - val_loss: 1.5133\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5168 - val_loss: 1.5093\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5180 - val_loss: 1.5776\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5500 - val_loss: 1.6770\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5859 - val_loss: 1.5133\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5232 - val_loss: 1.5104\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5480 - val_loss: 1.6386\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5048 - val_loss: 1.4911\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5631 - val_loss: 1.5015\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5093 - val_loss: 1.4833\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5229 - val_loss: 1.5373\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5395 - val_loss: 1.5642\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5016 - val_loss: 1.4725\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5108 - val_loss: 1.5935\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5069 - val_loss: 1.4925\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5047 - val_loss: 1.5173\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4973 - val_loss: 1.4670\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_375 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 41)                697       \n",
      "_________________________________________________________________\n",
      "dense_377 (Dense)            (None, 47)                1974      \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 42)                2016      \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 1)                 43        \n",
      "=================================================================\n",
      "Total params: 5,034\n",
      "Trainable params: 5,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F66BB59D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.3251016485600462\n",
      "\n",
      "\n",
      "\n",
      "a=  40 , b=  42 , c=  37\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 4.8518 - val_loss: 3.4464\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.9014 - val_loss: 2.0084\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.9551 - val_loss: 1.7661\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7978 - val_loss: 1.7407\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7965 - val_loss: 1.6897\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7419 - val_loss: 1.6844\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7037 - val_loss: 1.6717\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7046 - val_loss: 1.6230\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6608 - val_loss: 1.6047\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.6383 - val_loss: 1.7095\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6038 - val_loss: 1.5835\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6063 - val_loss: 1.6813\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6087 - val_loss: 1.6095\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5870 - val_loss: 1.6766\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6133 - val_loss: 1.5316\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5623 - val_loss: 1.5455\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5584 - val_loss: 1.5399\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5597 - val_loss: 1.6069\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5496 - val_loss: 1.4992\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5318 - val_loss: 1.5079\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5511 - val_loss: 1.5206\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5399 - val_loss: 1.4995\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5394 - val_loss: 1.4603\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5204 - val_loss: 1.5046\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5223 - val_loss: 1.4448\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5370 - val_loss: 1.5260\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5574 - val_loss: 1.4339\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4910 - val_loss: 1.5404\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5363 - val_loss: 1.4704\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4962 - val_loss: 1.4491\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5011 - val_loss: 1.4216\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5056 - val_loss: 1.4198\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5031 - val_loss: 1.4516\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5080 - val_loss: 1.4441\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4852 - val_loss: 1.6752\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5259 - val_loss: 1.4305\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5030 - val_loss: 1.4856\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4932 - val_loss: 1.8553\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5403 - val_loss: 1.4427\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4872 - val_loss: 1.4073\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4959 - val_loss: 1.3938\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5140 - val_loss: 1.3986\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4513 - val_loss: 1.6257\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5204 - val_loss: 1.4015\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4832 - val_loss: 1.4657\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4652 - val_loss: 1.4341\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4758 - val_loss: 1.3905\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4720 - val_loss: 1.4048\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4792 - val_loss: 1.4122\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4830 - val_loss: 1.3902\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_380 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 40)                680       \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 42)                1722      \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 37)                1591      \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 1)                 38        \n",
      "=================================================================\n",
      "Total params: 4,335\n",
      "Trainable params: 4,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F608EE948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.35880571042146503\n",
      "\n",
      "\n",
      "\n",
      "a=  39 , b=  36 , c=  37\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4.6179 - val_loss: 2.9098\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.3908 - val_loss: 2.0792\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9125 - val_loss: 2.1310\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8783 - val_loss: 2.0227\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8462 - val_loss: 2.0405\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8125 - val_loss: 1.9556\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7912 - val_loss: 1.9638\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7599 - val_loss: 1.8646\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7399 - val_loss: 1.8452\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7598 - val_loss: 1.8582\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6875 - val_loss: 1.8424\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6825 - val_loss: 1.8097\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6580 - val_loss: 1.7735\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6896 - val_loss: 1.7930\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.6500 - val_loss: 1.7148\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6351 - val_loss: 1.8052\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6376 - val_loss: 1.6775\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6210 - val_loss: 1.8124\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6284 - val_loss: 1.6511\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6203 - val_loss: 1.7008\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5871 - val_loss: 1.6446\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6311 - val_loss: 1.6364\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6226 - val_loss: 1.6254\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5880 - val_loss: 1.7202\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6199 - val_loss: 1.6422\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5730 - val_loss: 1.6044\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5759 - val_loss: 1.6409\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5954 - val_loss: 1.5836\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5530 - val_loss: 1.6350\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5654 - val_loss: 1.5767\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5529 - val_loss: 1.5657\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5747 - val_loss: 1.6843\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5828 - val_loss: 1.7500\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5932 - val_loss: 1.5481\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5416 - val_loss: 1.5748\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5679 - val_loss: 1.5588\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5499 - val_loss: 1.5795\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5507 - val_loss: 1.5751\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5587 - val_loss: 1.6801\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5810 - val_loss: 1.5907\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5690 - val_loss: 1.5604\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5641 - val_loss: 1.5556\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5735 - val_loss: 1.6179\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5502 - val_loss: 1.5318\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5555 - val_loss: 1.7006\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5907 - val_loss: 1.5366\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5283 - val_loss: 1.7174\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5702 - val_loss: 1.6252\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5193 - val_loss: 1.5221\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5286 - val_loss: 1.5107\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_385 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 39)                663       \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 36)                1440      \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 37)                1369      \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 1)                 38        \n",
      "=================================================================\n",
      "Total params: 3,814\n",
      "Trainable params: 3,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F608BAE58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.3015218004428927\n",
      "\n",
      "\n",
      "\n",
      "a=  44 , b=  45 , c=  35\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4.1421 - val_loss: 2.4150\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.2236 - val_loss: 1.9361\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9793 - val_loss: 1.8882\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9095 - val_loss: 1.9102\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8655 - val_loss: 1.8455\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8490 - val_loss: 1.8652\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8163 - val_loss: 1.8846\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8437 - val_loss: 1.8133\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7693 - val_loss: 1.8290\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7699 - val_loss: 1.7536\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7242 - val_loss: 1.7258\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7054 - val_loss: 1.7044\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7141 - val_loss: 1.7001\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7048 - val_loss: 1.6927\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6722 - val_loss: 1.6845\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6463 - val_loss: 1.7144\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6344 - val_loss: 1.7065\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6055 - val_loss: 1.7178\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5819 - val_loss: 1.5818\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5870 - val_loss: 1.5625\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6994 - val_loss: 1.5720\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5641 - val_loss: 1.5383\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5683 - val_loss: 1.5309\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6015 - val_loss: 1.5218\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5483 - val_loss: 1.5957\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5526 - val_loss: 1.6001\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5329 - val_loss: 1.5254\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5227 - val_loss: 1.5078\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5584 - val_loss: 1.5833\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5150 - val_loss: 1.5390\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5038 - val_loss: 1.4716\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5332 - val_loss: 1.4623\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5089 - val_loss: 1.4968\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5179 - val_loss: 1.4460\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5178 - val_loss: 1.4997\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4714 - val_loss: 1.8745\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4975 - val_loss: 1.4229\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5151 - val_loss: 1.4801\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4976 - val_loss: 1.5123\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4772 - val_loss: 1.4307\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4835 - val_loss: 1.4330\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5224 - val_loss: 1.7069\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5388 - val_loss: 1.5349\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4901 - val_loss: 1.4696\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4482 - val_loss: 1.5139\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4641 - val_loss: 1.4121\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4808 - val_loss: 1.4735\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4537 - val_loss: 1.4256\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4555 - val_loss: 1.4912\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4400 - val_loss: 1.5710\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_390 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 44)                748       \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 45)                2025      \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 35)                1610      \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 4,723\n",
      "Trainable params: 4,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F6563B288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.34434486762075167\n",
      "\n",
      "\n",
      "\n",
      "a=  44 , b=  39 , c=  41\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 5.2343 - val_loss: 3.5722\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.9686 - val_loss: 2.3138\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.1892 - val_loss: 2.1744\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.0881 - val_loss: 2.1474\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.0482 - val_loss: 2.1329\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.0201 - val_loss: 2.1049\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9855 - val_loss: 2.0933\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9697 - val_loss: 2.0765\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9338 - val_loss: 2.0805\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8971 - val_loss: 2.0792\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8730 - val_loss: 2.0320\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8549 - val_loss: 2.0032\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8094 - val_loss: 1.9720\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7917 - val_loss: 2.0393\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7981 - val_loss: 2.0183\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7640 - val_loss: 2.0129\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7414 - val_loss: 1.9060\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7331 - val_loss: 1.9930\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6945 - val_loss: 1.8699\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7170 - val_loss: 1.8625\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6795 - val_loss: 1.9315\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6502 - val_loss: 1.9665\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6562 - val_loss: 1.8369\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6198 - val_loss: 1.8072\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6146 - val_loss: 1.8453\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6909 - val_loss: 1.8549\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5911 - val_loss: 1.7419\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5994 - val_loss: 1.7998\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6124 - val_loss: 1.7084\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5948 - val_loss: 1.7936\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6010 - val_loss: 1.8175\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5512 - val_loss: 1.7358\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5626 - val_loss: 1.7648\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5436 - val_loss: 1.6579\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5954 - val_loss: 1.7102\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5829 - val_loss: 1.7018\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5381 - val_loss: 1.8407\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5202 - val_loss: 1.8153\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5336 - val_loss: 1.6313\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5509 - val_loss: 1.7125\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5112 - val_loss: 1.6860\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5374 - val_loss: 1.7771\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5400 - val_loss: 1.7339\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5296 - val_loss: 1.6498\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5090 - val_loss: 1.8583\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5406 - val_loss: 1.6245\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5255 - val_loss: 1.7197\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5389 - val_loss: 1.8108\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4736 - val_loss: 1.6196\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5109 - val_loss: 1.6810\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_395 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_396 (Dense)            (None, 44)                748       \n",
      "_________________________________________________________________\n",
      "dense_397 (Dense)            (None, 39)                1755      \n",
      "_________________________________________________________________\n",
      "dense_398 (Dense)            (None, 41)                1640      \n",
      "_________________________________________________________________\n",
      "dense_399 (Dense)            (None, 1)                 42        \n",
      "=================================================================\n",
      "Total params: 4,489\n",
      "Trainable params: 4,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F6034E0D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ESV: 0.23896085982264337\n",
      "\n",
      "\n",
      "\n",
      "a=  39 , b=  39 , c=  41\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4.5762 - val_loss: 2.6954\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.2771 - val_loss: 1.9526\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9768 - val_loss: 1.9415\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9541 - val_loss: 1.9221\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9408 - val_loss: 1.9202\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9194 - val_loss: 1.8970\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9074 - val_loss: 1.9552\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8699 - val_loss: 1.8761\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8354 - val_loss: 1.8934\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8858 - val_loss: 1.8523\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8146 - val_loss: 1.8379\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7940 - val_loss: 1.8385\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7704 - val_loss: 1.8471\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7793 - val_loss: 1.8052\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7875 - val_loss: 1.7985\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7683 - val_loss: 1.7920\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7380 - val_loss: 1.8285\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7069 - val_loss: 1.8693\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7132 - val_loss: 1.7601\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6841 - val_loss: 1.7396\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7015 - val_loss: 1.8352\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6917 - val_loss: 1.7166\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6667 - val_loss: 1.7106\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6659 - val_loss: 1.6908\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6541 - val_loss: 1.6790\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6631 - val_loss: 1.7064\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6172 - val_loss: 1.6689\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6134 - val_loss: 1.6495\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5937 - val_loss: 1.6844\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6294 - val_loss: 1.6114\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6054 - val_loss: 1.6148\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5830 - val_loss: 1.6128\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5906 - val_loss: 1.5908\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6214 - val_loss: 1.5758\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5720 - val_loss: 1.7033\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5695 - val_loss: 1.5664\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5572 - val_loss: 1.5369\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5726 - val_loss: 1.5805\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5510 - val_loss: 1.5148\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5781 - val_loss: 1.5127\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5811 - val_loss: 1.5184\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5933 - val_loss: 1.4909\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6007 - val_loss: 1.5404\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5545 - val_loss: 1.5919\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5503 - val_loss: 1.5597\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5632 - val_loss: 1.5828\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5393 - val_loss: 1.5069\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5257 - val_loss: 1.5942\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5352 - val_loss: 1.5410\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5497 - val_loss: 1.4530\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_400 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_401 (Dense)            (None, 39)                663       \n",
      "_________________________________________________________________\n",
      "dense_402 (Dense)            (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_403 (Dense)            (None, 41)                1640      \n",
      "_________________________________________________________________\n",
      "dense_404 (Dense)            (None, 1)                 42        \n",
      "=================================================================\n",
      "Total params: 4,209\n",
      "Trainable params: 4,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F64373EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.32928336743739484\n",
      "\n",
      "\n",
      "\n",
      "a=  35 , b=  48 , c=  43\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.6650 - val_loss: 3.1331\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.5174 - val_loss: 1.9936\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.8869 - val_loss: 1.9363\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8006 - val_loss: 1.9211\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7927 - val_loss: 1.9310\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8024 - val_loss: 1.9908\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7541 - val_loss: 1.8939\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7329 - val_loss: 1.8814\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7108 - val_loss: 1.9358\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7067 - val_loss: 1.8403\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6808 - val_loss: 1.8780\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6750 - val_loss: 1.8323\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6831 - val_loss: 1.8856\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6641 - val_loss: 1.8176\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6569 - val_loss: 1.8687\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6386 - val_loss: 1.7775\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6339 - val_loss: 1.7394\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6358 - val_loss: 1.7590\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6142 - val_loss: 1.7740\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5978 - val_loss: 1.6722\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6296 - val_loss: 1.7418\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6091 - val_loss: 1.6761\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5790 - val_loss: 1.7518\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6274 - val_loss: 1.6430\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5748 - val_loss: 1.5945\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5865 - val_loss: 1.5867\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5751 - val_loss: 1.6736\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5674 - val_loss: 1.6466\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5708 - val_loss: 1.6504\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5444 - val_loss: 1.5579\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5345 - val_loss: 1.5941\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5278 - val_loss: 1.6393\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5097 - val_loss: 1.5355\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5508 - val_loss: 1.5324\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5301 - val_loss: 1.5421\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5080 - val_loss: 1.5054\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5375 - val_loss: 1.6044\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5223 - val_loss: 1.5279\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5113 - val_loss: 1.5251\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5337 - val_loss: 1.5711\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5015 - val_loss: 1.4691\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5050 - val_loss: 1.5373\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4886 - val_loss: 1.4698\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4758 - val_loss: 1.5528\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4865 - val_loss: 1.4641\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4920 - val_loss: 1.4475\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4656 - val_loss: 1.4473\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4849 - val_loss: 1.4349\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4493 - val_loss: 1.5129\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4751 - val_loss: 1.4802\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_405 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_406 (Dense)            (None, 35)                595       \n",
      "_________________________________________________________________\n",
      "dense_407 (Dense)            (None, 48)                1728      \n",
      "_________________________________________________________________\n",
      "dense_408 (Dense)            (None, 43)                2107      \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 1)                 44        \n",
      "=================================================================\n",
      "Total params: 4,778\n",
      "Trainable params: 4,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F6A65CB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.3363995430391429\n",
      "\n",
      "\n",
      "\n",
      "a=  39 , b=  47 , c=  40\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.0962 - val_loss: 3.2891\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.7480 - val_loss: 1.9770\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.0607 - val_loss: 1.8227\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9719 - val_loss: 1.8367\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9368 - val_loss: 1.7554\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.9048 - val_loss: 1.7182\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8590 - val_loss: 1.7304\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8558 - val_loss: 1.6894\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7996 - val_loss: 1.6762\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.8015 - val_loss: 1.6461\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7616 - val_loss: 1.6291\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7511 - val_loss: 1.6180\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7131 - val_loss: 1.6225\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6933 - val_loss: 1.5968\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7116 - val_loss: 1.5897\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6700 - val_loss: 1.5874\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6517 - val_loss: 1.5842\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6882 - val_loss: 1.6240\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6456 - val_loss: 1.6155\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6398 - val_loss: 1.6235\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6195 - val_loss: 1.6372\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6353 - val_loss: 1.6172\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6006 - val_loss: 1.5687\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5993 - val_loss: 1.6908\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6035 - val_loss: 1.5520\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5818 - val_loss: 1.5480\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5789 - val_loss: 1.6423\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.6055 - val_loss: 1.5419\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5713 - val_loss: 1.5240\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5766 - val_loss: 1.5274\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5531 - val_loss: 1.5186\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5800 - val_loss: 1.5544\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5841 - val_loss: 1.5561\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5778 - val_loss: 1.5338\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5450 - val_loss: 1.5968\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5355 - val_loss: 1.5292\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5678 - val_loss: 1.5056\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5276 - val_loss: 1.4910\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5505 - val_loss: 1.4891\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5499 - val_loss: 1.5648\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5121 - val_loss: 1.4847\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5410 - val_loss: 1.5002\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5177 - val_loss: 1.5428\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5088 - val_loss: 1.5773\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5247 - val_loss: 1.4818\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5176 - val_loss: 1.6285\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5019 - val_loss: 1.5786\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4962 - val_loss: 1.5575\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5181 - val_loss: 1.4954\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4922 - val_loss: 1.4719\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_410 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_411 (Dense)            (None, 39)                663       \n",
      "_________________________________________________________________\n",
      "dense_412 (Dense)            (None, 47)                1880      \n",
      "_________________________________________________________________\n",
      "dense_413 (Dense)            (None, 40)                1920      \n",
      "_________________________________________________________________\n",
      "dense_414 (Dense)            (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 4,808\n",
      "Trainable params: 4,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F6A749798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.32097751175573275\n",
      "\n",
      "\n",
      "\n",
      "a=  41 , b=  38 , c=  38\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5.1254 - val_loss: 3.2040\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.4825 - val_loss: 1.8274\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8409 - val_loss: 1.7470\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8435 - val_loss: 1.7273\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7776 - val_loss: 1.7110\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7630 - val_loss: 1.6903\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7305 - val_loss: 1.6809\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7471 - val_loss: 1.7124\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7041 - val_loss: 1.6492\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7152 - val_loss: 1.6418\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6902 - val_loss: 1.7085\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6709 - val_loss: 1.6356\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6447 - val_loss: 1.5888\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6434 - val_loss: 1.5858\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6830 - val_loss: 1.5667\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6735 - val_loss: 1.5513\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6384 - val_loss: 1.5413\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6119 - val_loss: 1.5278\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6005 - val_loss: 1.5188\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5790 - val_loss: 1.5088\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5710 - val_loss: 1.5011\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5625 - val_loss: 1.4890\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6065 - val_loss: 1.4838\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5871 - val_loss: 1.6353\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5925 - val_loss: 1.4623\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5644 - val_loss: 1.4572\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5775 - val_loss: 1.4552\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5638 - val_loss: 1.4450\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5678 - val_loss: 1.4462\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5851 - val_loss: 1.4575\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5228 - val_loss: 1.4480\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5561 - val_loss: 1.4203\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5196 - val_loss: 1.4144\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5050 - val_loss: 1.4505\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5287 - val_loss: 1.4086\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5677 - val_loss: 1.4266\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4978 - val_loss: 1.4799\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5144 - val_loss: 1.4641\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4992 - val_loss: 1.3949\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5505 - val_loss: 1.3968\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5000 - val_loss: 1.3926\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5478 - val_loss: 1.4140\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4960 - val_loss: 1.4018\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5074 - val_loss: 1.3943\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4788 - val_loss: 1.3909\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5043 - val_loss: 1.4107\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4950 - val_loss: 1.3885\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4857 - val_loss: 1.3788\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4639 - val_loss: 1.3843\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.4928 - val_loss: 1.3831\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_415 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_416 (Dense)            (None, 41)                697       \n",
      "_________________________________________________________________\n",
      "dense_417 (Dense)            (None, 38)                1596      \n",
      "_________________________________________________________________\n",
      "dense_418 (Dense)            (None, 38)                1482      \n",
      "_________________________________________________________________\n",
      "dense_419 (Dense)            (None, 1)                 39        \n",
      "=================================================================\n",
      "Total params: 4,118\n",
      "Trainable params: 4,118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F6A8754C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "ESV: 0.3606268220954971\n",
      "\n",
      "\n",
      "\n",
      "a=  39 , b=  47 , c=  44\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 5.3521 - val_loss: 4.0729\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.5164 - val_loss: 2.7067\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.3696 - val_loss: 2.0243\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.9745 - val_loss: 1.8855\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.8837 - val_loss: 1.8403\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.8451 - val_loss: 1.7923\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8110 - val_loss: 1.7546\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7804 - val_loss: 1.7204\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7294 - val_loss: 1.6878\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.7072 - val_loss: 1.6635\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.7068 - val_loss: 1.6468\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6701 - val_loss: 1.6315\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6482 - val_loss: 1.6291\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6482 - val_loss: 1.6104\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6545 - val_loss: 1.6029\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6376 - val_loss: 1.5934\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6442 - val_loss: 1.5905\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6295 - val_loss: 1.6150\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6596 - val_loss: 1.5742\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6085 - val_loss: 1.5678\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6111 - val_loss: 1.5641\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.6141 - val_loss: 1.5541\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5814 - val_loss: 1.5726\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5953 - val_loss: 1.5464\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6010 - val_loss: 1.5444\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5791 - val_loss: 1.5351\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5771 - val_loss: 1.5333\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5815 - val_loss: 1.5291\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5537 - val_loss: 1.5446\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.5610 - val_loss: 1.5299\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.5328 - val_loss: 1.6467\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5841 - val_loss: 1.6802\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5860 - val_loss: 1.5622\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5469 - val_loss: 1.5355\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5471 - val_loss: 1.5086\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5529 - val_loss: 1.5045\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5323 - val_loss: 1.4991\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5308 - val_loss: 1.4957\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.5011 - val_loss: 1.6088\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5390 - val_loss: 1.4944\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5575 - val_loss: 1.5074\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5562 - val_loss: 1.4949\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5327 - val_loss: 1.4827\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5388 - val_loss: 1.5546\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5159 - val_loss: 1.4799\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5068 - val_loss: 1.4834\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5459 - val_loss: 1.5323\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5082 - val_loss: 1.4742\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5146 - val_loss: 1.5346\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.5281 - val_loss: 1.4668\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_420 (Dense)            (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_421 (Dense)            (None, 39)                663       \n",
      "_________________________________________________________________\n",
      "dense_422 (Dense)            (None, 47)                1880      \n",
      "_________________________________________________________________\n",
      "dense_423 (Dense)            (None, 44)                2112      \n",
      "_________________________________________________________________\n",
      "dense_424 (Dense)            (None, 1)                 45        \n",
      "=================================================================\n",
      "Total params: 5,004\n",
      "Trainable params: 5,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F6BE42318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ESV: 0.32172525828418375\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MAX: 0.3606268220954971\n",
      "AVG: 0.323774738947975\n",
      "\n",
      "\n",
      "[0.3606268220954971, 0.35880571042146503, 0.34434486762075167, 0.3363995430391429, 0.32928336743739484, 0.3251016485600462, 0.32172525828418375, 0.32097751175573275, 0.3015218004428927, 0.23896085982264337]\n"
     ]
    }
   ],
   "source": [
    "esv = []\n",
    "# while True: \n",
    "for i in range(10):\n",
    "    \n",
    "    X_train.shape\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    \n",
    "#     a=random.randint(18,21)\n",
    "#     b=random.randint(35,37)\n",
    "#     c=random.randint(28,30)\n",
    "\n",
    "\n",
    "    a=random.randint(35,45)\n",
    "    b=random.randint(35,49)\n",
    "    c=random.randint(35,45)\n",
    "\n",
    "    \n",
    "    model.add(Dense(a, activation=\"relu\"))\n",
    "    model.add(Dense(b, activation=\"relu\"))\n",
    "    model.add(Dense(c, activation=\"relu\"))\n",
    "    \n",
    "    \n",
    "    print('a= ',a,', b= ',b,', c= ',c)\n",
    "    \n",
    "#     model.add(Dense(random.randint(30,40), activation=\"relu\"))\n",
    "    \n",
    "#     model.add(Dense(random.randint(30,40), activation=\"relu\"))\n",
    "    \n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"sgd\", loss=\"mse\")\n",
    "    model.fit(x=X_train, y= y_train, validation_data=(X_test,y_test),batch_size=32,epochs=50)\n",
    "    \n",
    "    print(model.summary())\n",
    "    predictions = model.predict(X_test)\n",
    "    esv.append(explained_variance_score(y_test,predictions))\n",
    "    print('\\n'+'\\n'+\"ESV: \" + str(explained_variance_score(y_test,predictions))+ '\\n'+'\\n'+'\\n')\n",
    "    if(explained_variance_score(y_test,predictions) > .40):\n",
    "        winsound.Beep(1047, 62)\n",
    "        break\n",
    "#     winsound.Beep(1397,250)\n",
    "\n",
    "esv.sort(reverse=True)\n",
    "print(\"\\n\")\n",
    "print(\"MAX: \" + str(max(esv)))\n",
    "print(\"AVG: \" + str(sum(esv)/len(esv)))\n",
    "print('\\n')\n",
    "print(esv)\n",
    "winsound.Beep(784, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \",mean_squared_error(y_test,predictions) )\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,predictions)) )\n",
    "print(\"MAE: \",mean_absolute_error(y_test,predictions) )\n",
    "print(\"ESV: \", explained_variance_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 19)                323       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 35)                700       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 29)                1044      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 2,401\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'real')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2de5hcZZWv31WdCqkkmoZDZoY0CYnMnOQRiURaQeONiyIPENugQQYFwTF6vHCJE42jgwne4mRUdBQ1chFGUPpwaXNRwUMQBQeYjgkNkTAOYCCNjmEgGQkN6XSv80fV7lRV78u3q/au63qfB0nv2rW/bzfmt9de3+9bS1QVwzAMo/XI1HsChmEYRjqYwBuGYbQoJvCGYRgtigm8YRhGi2ICbxiG0aJMqPcEijn00EN19uzZ9Z6GYRhG07B58+anVXW632cNJfCzZ8+mv7+/3tMwDMNoGkRkR9BnlqIxDMNoUUzgDcMwWhQTeMMwjBbFBN4wDKNFMYE3DMNoUUzgDcMwWpRUBV5EOkXkJhHZLiIPi8hr0xzPMAzDOEDaPvivAz9T1XeKyERgcsrjGYZhGAVSE3gReSnwRuB9AKq6D9iX1niG0Yj0bRlkzW2P8NTuIWZ05lh+ylx6FnTVe1pGIzDQC3dcBnt2wrTD4aRLYf6SRIdIM0XzMmAXcI2IbBGRK0VkSvlJIrJURPpFpH/Xrl0pTscwakvflkE+dcuDDO4eQoHB3UN86pYH6dsyWO+pGfVmoBfWXwh7ngQ0/+/1F+aPJ0iaAj8BeBXwbVVdAOwFVpSfpKprVbVbVbunT/ctp2AYTcma2x5haHik5NjQ8AhrbnukTjMyGoY7LoPhodJjw0P54wmSpsDvBHaq6n2Fn28iL/iG0RY8tXso1nGjjdizM97xCkktB6+qfxSRJ0Vkrqo+ApwE/Dat8Qwjilrnw2d05hj0EfMZnbnUxjQajKA8+7TDC+mZMqYdnujwabtoPgZcX3DQPAacn/J4huGLlw/3UiZePhwIFPlqHwjLT5lbMiZALtvB8lPmVnEnRtOwYRn0Xw1o/mcvzw55oV9/YWmaJpvLH0+QVAVeVbcC3WmOYRguhOXD/US7kgdCOd555qJpQwZ6S8Xdw8uzX/JQ/ueUXTQNVQ/eMNIibj487gMhiJ4FXSbo7cgdlzFO3D28PPv8JYkLejlWqsBoC4Ly3kHHbYHUqIqwxdKE8+xhmMAbbcHyU+aSy3aUHAvLh8d9IBhGCYEiLonn2cMwgTfagp4FXXxp8dF0deYQoKszx5cWHx2YPon7QDCMEk66NL9oWoJA9wWpp2WKsRy80XRU6m6Jkw+3BVIjlKgyA96fU15EjUJUAxYC6kB3d7da020jjHJ3C+Qj67Bo3O8aJtxGRQz0wk8/CUPPlB7P5uCMb9RcwAFEZLOq+roVLUVjVEzflkEWrt7EnBUbWbh6U01qrFS7/d/qwxgVs2EZ3LJ0vLhDKmUGksBSNEZFJOETr4Rq3S1J2R/jYG8MTU5Q1F5OwmUGksAE3qiIegglVL/93+UBkaQg1+tBaCRE+W7UMGpof3TFUjRGRdTLJ16tuyXK/ph0CscqSjYxQbtR/UihzEASmMAbFVEvn3hcu2M5UQ+IpAXZNkw1MWG7UYvJHVK3BdYoLEVjVEQ9C2lVs/0/yv6YtCBbRckmJjKnXvC1n/7VmkynEkzgjYpoZp942AMiaUG2ipJNTFBJX8hH7ad+uSGj9mJM4I2KacVCWkkLcjM/CNsev5K+TRC1F2MCb9SFRrUOpiHIrfggbHpcGl43yG7UarCdrEbNSWI3qmFUjNfwurzZRoMulEZhO1mNhsKsg/Gox47hlmSgF772CrjlAzVpeN0IWIrGqDlmHXTHNkolhF/UXk4D7kStFovgjZoT5EhRsAi1DHvbqQIvYl/ZCbd+KFzcoSF3olaLCbxRc/w2G3lY8a9S7G2nQryIfc+TgIKOhJ/foDtRq8UE3qg5xbtR/bAI9QDWWapC7rgsOmL3mDazaRdYozCBN+pCz4Iu7llxIhLwuUWoeayzVIW45NOzOVj8PbjkoZYUd7BFVqPO1Hsrf6P68T1so5QDfp72oF2o0gE62pSe9kowH7xRV+rpia/H2I3+QGk6/Mr5ZnPwyr+FB25oGa97GGE++FQjeBH5PfBnYATYHzQJo32pZ4Ra65r2lVgeix8InZOzqMKeoWF7OAz0wvqLYXjv+M+Gh+B3t+fFvIl3oSZBLVI0J6jq0zUYx2hSar2V3xNNv9QQpJf/j/NA6dsyyKr123j2+eGxY8V/bms//LWL4PG7ws/ZszMv5m0m6OXYIqvRVhQ39Agirfy/q+XRm2OxoPvRdm6jgV74woxocYeW9LRXQtoCr8DtIrJZRJb6nSAiS0WkX0T6d+3alfJ0jHbHL4ouJk2HSufkrNPxqDkW0zZuo7GdqD4pmXFIS3raKyHtFM1CVX1KRP4C+LmIbFfVXxafoKprgbWQX2RNeT5GmxMmiF0p57WD/Azlx8PeLsppeT/8mEMmoC67H90XtH1qxiPVCF5Vnyr8+0/ArcBr0hzPMKIIEkRP3Nfc9khqRb32DPmnXIqP920ZDNwbUE5L++EHeuHLc/KFwWKJ+/ubplZ7LUhN4EVkioi8xPsz8FbgobTGMwwXgjYOnTBveqLNtv1w2ZW65rZHAruATs5m6MxlK+pF21R46ZihZ9y/k5mY37Rk4l5CmimavwRuFRFvnBtU9WcpjmcYkQTZMmthmXTpFhWWQvrt505NZB4NT5wyA5KBY883YQ8gNYFX1ceAV6Z1fcOoFD9b5iU3bvU91xPcJDYouXj+g3b2BtXtaRmKd6MGvsMUMW1mW/ra42KlCgyD8JIJSdZkj/L8t2WTbpda7R4tuhs1LcwHbxiEF/WqZU324kqbLZ9r93BNyeQOMXGPiUXwhkF4+iQqfeOKa5qn5Zt0lxcHC3XJSNuWGUgCE3jDKBAkrElUvLTWewXK0zF7ngQE37z7tJn5Ur5GxViKxjAiSKImu7XeK+CbjlEod/+3aIelWmMCbxgRJJEXt9Z7BQIbcWg+YkdausNSrbEUjWE4UG1evN6NTRqGoJy7pWNSwSJ4o+b0bRlk4epNqZUEaETaqvXeQC987RWwsjP/74HeA5+ddGk+/VKMpWNSwyJ4o6a062Jj27Te81tEXX9h/s/F9dnbvBFHrbCWfUZNWbh6U+BOzXtWnFjVta0dXh2JqvpoKZjUqFvLPsMoJ63FxnZ9M2gIXHaiBi6uGmliOXijprhUVKwEsyHWEZedqNZhqS6YwBs1Ja3FRrMh1pGo6NwWUeuGCbxRU9KqtZLWm4HhQFh0bp72umI5eKPmpFFrpS2rMDYKJ106PgdvVR8bAovgjZbA783gzGO7Um3B1zaE+dohL+JnfMN2ojYgZpM0WpJyVw3kI/qWL72bNH4OGYvOG4owm6RF8EZLYq6ahPBzyAwP5Y8bDY8JvNGSmKsmIYIcMuZrbwpskdVoSSop7hW2E7bld8kO9MJPPwlDz+R/zh0Cp345pDiY+dqbARN4oyoaVfiiXDXl8z5h3nRu3jzouxMWaN1dsgO9sP5iGN5benzoGej7MLzqXHjghvE5ePO1NwW2yGpUTKMvZAY9fPzmHdBTiK5CxB9UP8fr2dpoD7hIyiP2IKbNzIu5FQdrWMIWWU3gjYpJs3BYmgTN2w+vz1DQ35JctqNhH3CBDPTCjz8CI/scThZYuTv1KRmVY8XGjFRo1oXMOPObERLBd4gEOnVcBb7mKa6BXrj1g6Cjbudbrr2pSd1FIyIdIrJFRDakPZZRW5q1PIDr/IR8Lj+ofs5IwNuv6wOkb8sgy296gMHdQyj5h8jymx5Ib0PWhmVwy1J3cc9kLdfe5NTCJnkR8HANxjFqTLN2KfKbtx/KgbIKfvVzuqp8wK1av43hkdKHxPCIsmr9Nqfvx2KgF/qvJjjZVEZ2CvRcYbn2JifVFI2IHA6cBnwBWJbmWEbtadYuReXzzoj4RuPFAh5UP6ea+jfPPj8c63hV3HEZTuLu2SNN2FuCtHPwlwOfAF4SdIKILAWWAsyaNSvl6RhJk0bhsFpQPO8gN1CUUDfVAy5qY1ImaxF7C5KawIvI6cCfVHWziLw56DxVXQushbyLJq35tDuN6ldPE9d7rkaoyx9wXkNxl+t05rLsHhofrXfmsq63WMqGZbD5+6BFC7+ezTFowxJY1N7CpJmDXwgsEpHfAz8CThSRH6Q4nhGAF6EWL+Z96pYHW7q6Ytx77lnQxfJT5jKjM8dTu4dYc9sjsX8/ccdcuegoshkpOZbNCCsXHRVrXACuXQT9V5WKOxxoev03b81vUCpBoPv98MnHTdxblNQEXlU/paqHq+ps4N3AJlV9T1rjGcG0Y+GtuPfsKs5ehO5XgjjumD0LuljzrleWLN6uedcr479ZbVgGj98V/PnwEPzu9vElfRevhdO/Gm8so6kwH3wb0Kx+9WqIe89h4hyUqy8vWVDJ77nqNYwxd0wEe3bmo3SL1NuKmgi8qv4C+EUtxjLGU0nhrbRJe00g7j27iHPUQ6Bmv+eB3gOlAySDkzvGNiy1JVYuuA1I2q8elqZw/X7aawJx79ll01bUQ6Am+wK8MgN7ngR0fM7dDysO1raYwLcBSTa6TkKca7EmEPeeXcQ56CGQEWHOio2sue0Rzjy2K/GG4mN4ZQacasgUsPZ5bY0VGzNikUSBsTkrNgYmFX6/+rQqZlcdUWkjP798OakVG9uwLMZOVIHuC2wBtU2wYmNGYiSxYBuUqxbyIpqUOMbN80cteLrsgI0qNhZ77SGusFs5X6MIE/g60awbj5JYSFx+ylwuuXHrOMlSiFWJMYwox0ulFD8E5qzY6HtO0MPOeU5ji6gBG5P8yB2S97MbRhGWg68DzbzxKImFxJ4FXYHxaFLWzVrk+eNW03Sa00BvfmNSHHHPZPM7UQ2jDIvg64CL57pRSar+SlfKlsKoVFLxG1Tn5CyqsGdoONb9RLUFjDUn1w5L5ViZASMEE/g60Owbj5IoMBZXHOMSlkoqT5UUV2+Mk8qJ87Dr2zIYWLXyvKn3Q9+/wKiD5XEMW0g1ojGBrwONuPGo1qRdiTHsAeL3BlVMnLcpl4ed90DxE/d3Tvw1n93/LZzrtANMnAKnX25RuxGJCXwdSDt6bRaqeROIWqQOe4BccuPWyOv7pXIqfQj5PVBWTbia93TcQUYUCfjeOKQDjn2fRe2GMybwdSCJ6LVZXThJ4OpGCXqABL1BlZ+TlBOnPPV2XfYLvCGzDXFRdumAd3zHonWjIkzg60S10WsaFsBmodpF6uWnzOXikCg+LJVTyWJ48QNlUeZud3EHE3ejKswm2YS0Y/nfYqpdpO5Z0BXYVKNDZGwnatxxgmr0LD9lLu+c+GvunnghX89e4S7u3e83cTeqwgS+CWl2F061xPWf+7Fy0VG+fv6vLDlQjz3OOGF7G3o67mF19koOzzztJu65Q2Dx9yzXblSNCXwTkoTANTNJbbaKKkYWZ5zQt6o7LmPCyAvRk8pOyQu7dVgyEsJy8E1Iu7twkrJYxq09EzZO6FvVpIiG1wBz3gTnrXObuGE4YgLfhIQJT7u4a5LYbJXkODM6cxz7Pz/nExN6mSFP85Qeyj/tX8Lml74FDgppeO01xbaI3UgBKxfcQviVs02tfG2LUdWDcaCXF9f/PRP37SnJsQ/pRB469vO8evbB+foyw0VRfjZnddqNRLBywW1CLWrctOIbQsW206L6MQcB5TuWcrKPVz/6L7DoofwBr82elfQ1aoQtsrYQabtrmrkKZhgr122Lbzu9dhHc8oHo4mB7Cvn3+Uvgkofoe/s2Fr74DebcMKWidoeGEYfQCF5EFod9rqq3JDsdoxrSrnFTqyqYtXxL6NsyyO6hYd/PAh+M3zwOnt7uNkBRs2u/N4VLbtxK/45n+HzP0bHmbRguRKVozgj5TAET+AYibXdNLfz3td6lGxal+z4YNyxzF/eyZtd+D0gFrr/3CbqPOKTpU11G4xEq8Kp6fq0mYsQjLMpNK/qtRRXMWtfKD3s4jT0Yxzos7cS56qNPnfagsZLsZGUYxTgvsorIacBRwCTvmKpelsakjHCioty0hKIW/vta79INemhNmdiR/z3G6olKaAOOsCJn7bIL2agtTousIvId4CzgY+S9Au8Cjoj4ziQRuV9EHhCRbSKyqurZGkD9atH0LOjizGO76Ch4ATtEOPPY8Q+UoJosLtR6l+7yU+aS7RhfP+Cto79k+HOHQf9VOIv7nDeF7kJdfsrcwNLA7bIL2agtri6a16nqucCzqroKeC0wM+I7LwInquorgWOAt4nI8ZVP1fBII8p1EeW+LYPcvHlwrHHFiCo3bx4sObdap00SZQji0LOgiykTS19kF2XuZk3Ht8mOPO9+oUPnRe5E7VnQxTnHzxon8u20C9moLa4C7ynH8yIyAxgG5oR9QfM8V/gxW/incXZVNTFJR7muouzy5lDt24VLjZik2VNw0SzK3D1W8XGCjLp9WTryVR8/ep/T6Z/vOZqvnXVMTe/PaF9cc/AbRKQTWAP8hrxQXxn1JRHpADYDfw18S1XH/S0QkaXAUoBZs2Y5Tqe9SToX7rqwGZQ/Htw9xJwVGxPLMdeqDIHHeVPv52P7ruQQec6xlK/A4rUVb1Sq9f0Z7YuTwKvq5wp/vFlENgCTVHWPw/dGgGMKD4dbReQVqvpQ2TlrgbWQL1UQa/ZtSqVumSDnjWvKpyOgaTQwFvkL/q9pDZtjHujlH0e+SUdmv/t3ui+wXahGU+Ak8CIyGfg4MEtVPyAis0TkDaq6weX7qrpbRH4BvA14KOJ0w4G4UWCY88bV/hgk7sUojBP5JHLMqWx+2rAM+q+iI/rMA3S/P5E67a1Y8sFoPFxz8NeQXzR9beHnncDnw74gItMLkTsikgNOBhx3iBhJE5aGcVnY7NsyOOaeiUIh0RxzKiUSrl1UcMjEIEFxb8WSD0bj4ZqDP1JVzxKRswFUdUgk8m/7YcC1hTx8Buh1jfiN5AlLw0SlfDxBcongIS/q96w4MZmJk+Dmpw3LYPM1oKNjbxrhFN5Fikr6FkfenZOzqOYXaeNE4bXezGW0L64Cv68QhSuAiBxJPqIPRFUHgAXVTc9Iiqg0TFjKx0+QADKSz8sPjx4Q/mrTMX6pi0RsodcugsfvGvsxUtwlA+/4bkmuvTzN9ezzB2rYxCmp0O4tF43aESnwhUj9O8DPgJkicj2wEHhfulMzkqQa503gFnuFNUtemVguOWidoHNytkRMPaIWbvu2DDKybhk9o7eREXWI2At0TIS3f2tcxJ4JWWQG9yi8FiUfDAMcBF5VVUQuAt4KHE8++LlIVZ9Oe3JGclRTpyZMkJK0/AWlLg6akCGX7Yj1cOrbMsihty5hoTzkaH3MP7Bk4hQ44/IxcS9+4LikqFyi8HZvuWjUDtcUzb3Ay1R1Y5qTaVdq5aioVIxPmDedH9z7hO/xJAkSxz1Dw3ztrGPcf0cDvbz5x8uYJn+OJe73y3yO+/Svxo4FpabCcInC0y4KZxgergJ/AvBBEdkB7KWw+qSq81ObWZtQ6/K4YfMIEpw7t+/y/U7Q8UpJ5E2hkGvP27eiT1eFZ5nKF/V9vL7nwyWfxc2Jx4nCi+/H+91fcuNWE3sjUVwF/tRUZ9HGNIKjIuohU6tFwapTFxuWlSykhqHAi0zgk8NL6X/pW3xFNeiB0yHCqGrFLppiGuUBb7QmrjtZd6Q9kXalERwVUQ+ZWi0KVpq6ePSaD3LEjl46dDQyJaOAZKcgZ1zOpPlL+HrIuUEPnCRrxzTCA95oXazpdp1JWzxd8vtRD5laLgo6p2IKTTh0z5O8TMkLu4u4z3lTZNXH4rlAurnyRnjAG62LCXydSVM8XV//XTzy0ECLggO9sP5CGM7XvnES9twhSEAjjjDSLgxmlkkjTUzg60ya4un6+u/ykAkTuprWVRnohVs/BOrmblGFX40exRs/+et05lMlZpk00sQEvgFIKkosF9qw8r4LV28qEeQvLT6aleu2sbtQG31S1q1MUc0WCQuFwVxRhREyXD9yImunfoR7yj5vlGJfDfd2ZLQUoo71RWpBd3e39vf313saTUm50IbhV+3xzGO7uHnzYOwFxYWrN/k+SCqtR1MuvJe//He8+sFVMLzX+RqqcN3IyXx2/wW+9+D3u0p68dQwaoWIbFbVbr/PLIJvEeJsyil/pA8Nj/DD+54ct1PTxc0RtUgYJ1IuFt5FmbtZOXQdB29+zs3PXvifEclwk7yFlfvPoytgvKjUVaNE94ZRLSbwLUK1rougbfhR1w1bJIybvvGEd1HmblZnr2Sy7HObvHQg7/gOzF/CBODdhX+CCHsomS/daCVc68EbDY6r6yKopnvQ8ajrhtWSX7V+W6z+rE/tHmJR5m4uz17hLu4ABXF3JaynbbU9ZQ2jkTCBbxH8hNaP4192sK8gn33czMimH34ENckGfCtAQkAEPdDLQ5PO5+vZK8jEqB+zZ8qRsa2Py0+ZS7ZskGxGkitNbBgNgqVoWoRyN4YIjPpkXX7/30N8afHRvjnm7iMOqSj37OcCWrh6U+D5JRH0QC/89JMw9AxTwLl+DOTtj58a+fw4h4wT5ePIgbmZL91oFUzgWwhPaPu2DHLxjVt9z/E6OPkJd5KbesIi3rG3gqINS67s0wn8/fBS1o2+HgCpILJec9sjDI+UPv2GR3SsfaH50o1WwQS+hfDcH0H+d6guEi13l5wwbzp3bt/lG/EHRcKduSw9g1+BH8frh6oKg3oo/7R/yZi4+91PtaUZ4vjSzW1jNDom8C2Cqw++0kjUz11SXCO+3G0SFAnfdshXoP/eWGMX+9qLkbL7SbI0Q5RQm9vGaAZskbVFcPHBd+ayFYuPy/WL3Sbli6/vm3o/Wyct5S//213cVfPrCNeNnMxKH3E/5/hZJffj6oAJc/64Ym4boxmwCL5FiHJ55LIdrFx01NjPcdMLri6S4si4Z0FXPh2z+RrYP+r0fW8B9VmmsnL43JJ0TFdnrmS+QEnJhaDUVPnckygPYG4boxkwgW8RwgSufEena3ohTsNpjxI/faG7kiuqcNHwh0tEvfgeiksf+N1DeQkGD791h2oXlM1tYzQDlqKpI31bBlm4ehNzVmxk4epN9G0ZrPhaQWmHy886hntWnBg7leEJ6ODuIRS3htN45w30whdnxBJ3gEdlpq+4l+fag+5BGe9+TMsBk0SaxzDSJrUIXkRmAtcBfwWMAmtVNayBTluR9CJdnLRDUBphcPcQc1ZsZEZnjr0v7g/NuXcERPRfyV0Ht/zMed5aUOVHmcnJL3x5XBTul2sPuwdlfCqnmkg9KJVlVSCNZiDNFM1+4OOq+hsReQmwWUR+rqq/TXHMpiGNVm2uaYewdI5CqM0SgqtPvnPir1msbuLuNbteOXwu60dfPybqXhTuCXWQaAbdQ6VVLP2Iegin3QzEMKoltRSNqv5BVX9T+POfgYcB+9tQoJ6LdK5lDYIYGh7hzu27xlwyb8/czb2TLmJN5puRG1FV4c+jB3HR8Id51YtrWVck7mPncECogwS0FikSc8oYzU5NFllFZDawALivFuM1A/VcpCtPL1TSEeCp3UP0DH6Fnheugolu31Hy5QXOHf600/XDiEqRRLmEijeFeemm8jcGc8oYzU7qDT9EZCpwF/AFVb3F5/OlwFKAWbNmHbtjx45U59MoNFLTiaCmHVMmdrB3n38e/iu56zjTMR3j8egR7+b0x95Rcs9BzhcIT9GEEfW7DdsUVnxe0s1MDCMNwhp+pOqiEZEscDNwvZ+4A6jqWlXtVtXu6dOnpzmdhiKoCmMa4u65dWav2MiRn/oJs8tcO0HpjmyH//89LptwtXOuHQoLqYfO48jzvzvuns85flZgusjLecd1F0WlVsI2bRWfZ04Zo9lJ00UjwFXAw6r61bTGaWZqsUhXHq16zhc/1055SuMSn4JlqyZczXs7/p9Trt3jfpnPcR/91dhY5ffsVbH0i5YrWXiOSq1EpVi8z80pYzQ7aebgFwLvBR4UEU8p/kFVf5LimEYZLtFqkCPEa8K9KHM3X5hwFVPlRQACeoOMUbxhyUt5hOXEvbHnrNjom66Jm/OOWt8IcxEVn1c8N8NoRlITeFW9G6fq3kaauEar5fRtGeTj+9dyzkE/R4gWdQ8lH7GvH339WA4d8LUb9u94pqQaZefkrG+TkM7JWbfBC0SV/PX73O88P6yCpNFMWKmCFicqWp2Wy5bUc/EEa2TdMt6T+bmzsEPBwz7nTRx33joeLzq+cPUm35z49fc+MRaxD+4eIpsROjLCSFmnkude2E/flkFnIY1KrRR/HuaiKccqSBrNRuoumjh0d3drf39/vafRUoQ5RrIZAaGk+UUu28F1r95B9+ZPuEftCnuZxKeHL+DrX/zSuCg3auNUMUGumkZwrpirxmhEwlw0FsG3OGHR6vP79pekRFZNuJr3ZO4gs1mdxF0VRshw/ciJfHb/BXR15mIVAfO9ZsDxRvCemy/eaDZM4FOk3vna8vEvP+uYko1AXlu/n05czjzJWxHjRO3FTTi8ptVhRcDKa8zEeXfMiMRK06SBVZA0mg2rJpkS5dUYK/V0pzG+99mizN3850F/yzwZRMRN3JUDTTiKOyxNnTSBngVdkUXAXPzvfoyopvr7c6ns6eeLF/K/22qrgRpGGlgEnxJpFBNLanyAt4zcxVez32ZCTJ/Tv+4/mUvLuisB7C6keuIUAes+4hA+3vuAcynitH5/roun5emu4rcQW3A1GhGL4FMiLF+bZB34Ssb/0t7P8PXsFUwQN2FVgNwhsPh7fHfqR3zP8dIUrrs/vfSRq7gXzz9J+rYM8vHeB5yLivUs6OKeFSfS1Zkbl2KyQmRGo2ERfEoERbKdk7POVrs4Ofzyc6flsuweKvWUr5pwNe+dkN+F6hK4590xB/HwsZ/j1Ys+CMDyEf86L56Au+z+dG0Q7keS+W5vHkEPmbCHiS24Gs2ACXxKBG22UcUpdRPHc923ZZDlNz0wZncc3D1ER0bIZoThgqf8uuwXeENmm/POM9X8hqU/vP1HgWAEVl4AABTpSURBVGmKIAGP2v3p0sDbj1qUAy4m7GFiC65GM2ACnxJx6rvA+MgvKIf+8d4HSq4PsGr9thIvO8DIqDJpYgfnTPw3Lhy+koPluRjbigU5cy3HzV8SeG/V5JnDotygTlEdIokXYwubR9TDJGq3rGE0AibwKeInhEFFtcojvyDx8dwk3vUB3+39izJ3s1Ku45D9z8UrGNH9fji9stpwrimlsIXYIOFMo9Jm0DxcHiZWiMxoBkzga4xr5Be2AzTMTVKJpx2AzETo+RbMX1JR7j+OqyTsd5CmcJbf1wnzpo9rO5jNCFMnTeCSG7ey5rZHQse2QmRGo2MCX2NcFyGf37c/9DrFEf7kbIbnh0e5Z+KHmSG74wk7wJw3wXnrxsaOk/svPtfPVbJq/bZx33OpFVMLK+TNmwc589iusYJn03JZ9hbt7jXro9HsmMDXgTABc3WYeCmdvi2DvLB/lEWZu+OLe5Gwe8Tx77sslj77/LDvDtRaR79B93Xn9l1j/vyFqzeNcx7Vcu+CYSSNCXyD4SKaxSmdkXXL+I+Jt9PBqLu45w6BU78MPouocex/rpbARhDIoLkO7h5izoqNoSkxl/usd1kKw/DDBL7BCBMTgVLx2LCMxaM/S0TYPeLY/1wrRTaCNzxsrl4ph6D6OFHWRysjbDQqtpO1wQgSk67OHI+vPo17Vpx4QDQ2f99d3Bd/Dz75eKi4Q7w+pH7n+tEI3nCXuXpF0YpxsT5GlYUwjHphAt9gxGr0rI6bhea8KVLYPeI0Ay8/tzOXJdtRKpGN4g0vn2sQ5UXRXOyZtqvVaFQsRdNg9Czoon/HM/zwvicZUaVDhDOP7aJn8Cuw7vt5UZcOOPZ9+X/7iLyXZhDwXUh1mUOc7knlDqBGzUUXzzVu846w+7JdrUajYgJfI1yFr2/LIDdvHhzbzfnL7P9hxpbdqBSlD3QE7b+KR5nJkfrkuDTNY0e8m3P/66z8WH/IsTyFOurF/vfylnfN0N0ozk7UqBx7UI/X5/fFazVoGEljLftqgJ/10W93plfZcETVacPSfs13UzqnYxMdjKKZDI/PWsLpj72jZCxv8TCq52g19xN2X+XfjdsLNS1cH7ou0X7flkFWrts2zmaZ1i5cw/AIa9lnAp8QYWLhKhCfuuVB3jJyF1/LXkGG6J2oqjDnxRvGfj54cpbJEyeEOluyHcKUiRPYMzQ8bp7VCp7ffRVfN8yp0shCOGfFRt85C/D46tPGfq62Z2sjp7eMxsV6sqZM1Cu8yyLcmtse4btcxhuy25ydMSNla+TPPj881ngjiOERHYsyi+cJOFv9ohYPiz//TN+DXH/vE2MCGRRONPKGItccezWLrWa1NNLAXDQJEGWTC1psKz7+pb2fyZfzjdET9fqR8VFh3IU9b55xrH5RYxTvsi0W9yga1XXi6mxy+e8chFktjTRITeBF5GoR+ZOIPJTWGI1CVOQWKhAblsHKTt7QEU/cfzV6VElPVMjbFF296eXzjBN9Lj9lbqDVUAqfQ1604iQA03CdJNE9q9xiefDkLAdNyHDJjVtLrhnL4lqGWS2NNEgzgv8+8LYUr98wREVugd7yexZD/1WAOndYGlW4aPjDnDv86ZLPMsDKRUcBcNCEeP9ZZ3TmYkWfPQu6AoVbOZBSiCNOafjlk2x87rXq+9pZx/DC8Ci7h4bHXTPOHoJyqon+DSOI1HLwqvpLEZmd1vUbCRfLXYlffKAX+s6C0X3OY6jCU9rJwn1XAJDNwPBo/jMR+NvjZgE4FSorJpuRsXnGaWDRFVLT3SOqlEHaLpo4hdNcFzijrllpETVrIGKkQd0XWUVkKbAUYNasWXWeTWXEqmE+0At9H4bR8MVQD9V8VPyvIyePpWQ6c1le3D/K8OjI2Dk3bx5kwwN/iN0Kb+qkCSXzdHVxuAiS3zkCnHP8LD7fc3SseVaCa9ojzgJnWqkUayBipEHdBV5V1wJrIW+TrPN0KiYychvohTsugz1POl9TgRv0LXx63/ljx3LZDkT8+7pW0ue02HUTdwcrUOJpL14ULL5WvUTL1f0SJ9JPc9eqNRAxkqbuAt/qPHrNB5m940Yyqk6LqArIxClw+uXI/CVM2TJIl2Nf1yC8tEnSwuSJUVj0W0/Rck17xF1gtlSK0SyYwKfInjWv4mXPPZoXdhdxV3hED2f7qbfTMz+4u1FQX9eDJ2d5YXh0XEpkcPcQB0/Oks0Iw6MHXpKSEKag6HfluvGdnGqN6xtEnKi83m8lhhGH1AReRH4IvBk4VER2Ap9V1avSGq/huHYRL937aGzr47nDn6YrYsNPUBT52TPyLhq/XaPPPj9MtkPozGV9d7FWSlD0u3vIv5NTFEnv5nR5g4gblVsqxWgW0nTRnJ3WtRuaDctg8zWgo5FBu1cl4lmmsnL4XNaNvh7Ii2aY0Ln0ND1m1e3j6qIMjyhTDprA1s++NbHbDXPKxN2ZWq/dnBaVG62KpWiSYsOygqfdkWyOVfpBvv/ca8Z91Dk5Gyl0UX1dy8Xdo1q3R/mD54R50/nBvU8kMlacxc6484wS7EqjcqsfYzQyJvDV8s3j4Ontzqerwv6OHNkzvsExIwvJ+aQGVP1dMq557bDt7cVlBOIKk1+E/YN7nwhtdRdnnGp6okbNM4k3Ab+H282bB61+jNGwWC2aavjneWgccQf+Z+qRZC/9I8xfQs+CLs48touOQqLea+6xJyD69vLaUYQJ4vJT5la8wzOoIXhQdcgT5k13Hqdvy2BgSiuu0yeNui5+v7Pr733C6scYDY0JfCUM9MLnpqPP/cGpxADkI/f7mc+dJ68fO1be3GNElZs3DzItlw28jot4RAlipQIY1WC7Q6Rki/6d23c5jxNUt6a4to0rUbbHSurT+P3OgjZtWP0Yo1GwFE1c/nkePPcHwMn5iCo8pwfx6f3vZ93o68kVvcIHCe2kbPBzN2oBFhjzyvsJkPe9oGsH4UXYYTvRRlV5fPVpJfXfXccJGjvOzjdv3KDveCmjStI3cUTb6scYjYJF8HH45nFj4u6CZ308et81Yw6Z4gg20GL4/DAHT/aP4qflspFpj7BiYE/tHqIz4NpBx8GtMmSxgIZF+34CGCaKLumjqHE922Olby9B8yt/yNumJ6ORMIF34dpFsHKa82KqKuxT8a36CAeEPUg0glI0YWUKygWqK+DaMzpzBDXxCmvuFRXBhgmo33nlhJU5dhHgsHGLqzrGeXspTuU8v28/2UypnOeyHZxz/KyKqkcaRi2wFE0Y1y6Cx++K9RVV2K5dnLpvTeA5nrD7bbDJZoS9+/YzPFKqtp25LCsXHRVYpqA4vxzUHs8T14sDrhG0uOvNOSg6Lq4GGVZG4eDJWVTh4hu3jvWeLa8kGTS3OF2kihEoaZfnumu1PJWT1kYxw0gTi+D9GOiFlZ2xxN2r1X7dyMmh4u65Sxau3sQlN27loAkZDp6cHYsAp06aME7cAaYclK/6GFY3vDxNoRxIIXjRJQSvHYSlSYKaWVx+1jHcs+LEMaELuoZXRsHz53sLy+X11MPePMJwrafu2pTD743A2yj2+OrTSu7ZMBoVE/hiPGG/5QPEWd7zovaXvXjDuC5L5bw4PMIP7n1iLIe+e2iYZ58fLikp4Icn2mECFeb0+OOeF7j4xq2Bi69RbhXXZhZB8/Pz9nsUp2CiBDjIAeMq3K73YR2WjFZANCzxWmO6u7u1v7+/PoPH3LDkMQr86/6TI4U9SQSYPLGD5/eNlKQKZq/YWNV1f7/6tETm5+fyCXqwFOOla8C/bEB52gTyIu4JdJK7Sheu3hTY0KQ45WMY9UZENqtqt+9nbS/wcUsMeMx5E5y3zld0asV7ihpn9G0ZdBLRIMqFK+kt+EGCWU6xYLteI67outxb1MPEMBqFMIFv7xTNtYsqE/eph8F564ADr/wdrmUjE+SG+w7UgInb4Lqc4lRGkr1Mi6/v0gw8zDGTRNrE9d6q6a9qGI1C+7poKkzJ0P1+OP2rJYei3CNpMar5qHb5KXOryg0fPDk7rm1fUkW/PPw6QI0EvD0G3UsS3ZTi3FsjlQW2omZGJbSnwFci7h05+Mc/Bn4c1WDalWwmL3yjjuG4F4F2Ts4GLtCGITBWR94jzb6jxaIUlHIJEuwkuik14+JpvcooG81Pe6VoBnrhy3Pii/vUw0LFHeLXS/GjqzPHmne9kq8uOYbOkHo05QwNj/DC8IhTCqQcZbxIuFoOq8XV+eKRRNqkVveWJGkUTzPag9aP4Ctodj3G1MPg790eBj0Luujf8UxgbfQwBHi8yMHymb4H+fML+2NdY2h4lPccP4s7t++K9Sbh5zuvVd/RShptVJs2acaeqs341mE0Bq0t8BuWQf/VxCtZVWDx92D+klhf8Rwt19/7RKwRMyLMWbGRGZ05Zv+vHPc8+kyscT3u3L6Le1ac6OsAyQjj0j5BwlbLDke1znM3Y/emJNYejPakdW2SA71wy1Jii3vB/lgNfo0hvMg6qiJjNRS/CfgtykFzCZuRxyybRhjt54Mf6IVbPwQaw5t+6Dz46H3Vjx1BsfBmQpwkldBqm3DMOXIA+10YQbSXwA/0wvoLYdgxP5k7BE79cux0TBLMWbHROZoXgRnTcjy1e4hpuey4gmStFtFZ1GoYboQJfGvk4McWUneCZBwjd4HuC8Z52mtF35bBWBH8Occd2LXqfb+VI7o0vPiG0W40v8CXR+wu4j5tJpx0aV2idjgQnfqJey7bwatmTePex55lRJUOEc4+bmaJuENjbcJJA3OOGEb1NL/A33GZWzpGOuAd36mbqBcT1JyiQ8RSEAXMOWIY1ZPqRicReZuIPCIi/ykiK1IZZM/O6HOyuYYRdwiOQkdVTdwLxN0EZRjGeFITeBHpAL4FnAq8HDhbRF6e+EDTDg+YQAcg+XTMGd9oGHGH5txNWWus2JdhVE+aKZrXAP+pqo8BiMiPgLcDv010lJMuHe+ayeYaTtSLacbdlPWg1dcZDCNt0kzRdAHF9QF2Fo6VICJLRaRfRPp37doVf5T5S/JiPm0mjRqxl2PRqWEYtSDNCN6vQPo424iqrgXWQt4HX9FI85c0tKD7YdGpYRhpk2YEvxOYWfTz4cBTKY5nGIZhFJGmwP878DciMkdEJgLvBqor8mIYhmE4k1qKRlX3i8hHgduADuBqVd2W1niGYRhGKaludFLVnwA/SXMMwzAMw5/26uhkGIbRRpjAG4ZhtCgm8IZhGC2KCbxhGEaL0lANP0RkF7DD8fRDgadTnE6taaX7aaV7gda6n1a6F2it+6n0Xo5Q1el+HzSUwMdBRPqDupg0I610P610L9Ba99NK9wKtdT9p3IulaAzDMFoUE3jDMIwWpZkFfm29J5AwrXQ/rXQv0Fr300r3Aq11P4nfS9Pm4A3DMIxwmjmCNwzDMEIwgTcMw2hRmlLga9LMu0aIyNUi8icReajec6kWEZkpIneKyMMisk1ELqr3nCpFRCaJyP0i8kDhXlbVe05JICIdIrJFRDbUey7VIiK/F5EHRWSriPTXez7VICKdInKTiGwv/P15bSLXbbYcfKGZ938AbyHfVOTfgbNVNdlerzVCRN4IPAdcp6qvqPd8qkFEDgMOU9XfiMhLgM1ATzP+txERAaao6nMikgXuBi5S1XvrPLWqEJFlQDfwUlU9vd7zqQYR+T3QrapNv9FJRK4FfqWqVxb6Z0xW1d3VXrcZI/ixZt6qug/wmnk3Jar6S+CZes8jCVT1D6r6m8Kf/ww8jE8f3mZA8zxX+DFb+Ke5oqEyRORw4DTgynrPxTiAiLwUeCNwFYCq7ktC3KE5Bd6pmbdRX0RkNrAAuK++M6mcQjpjK/An4Oeq2rT3UuBy4BPAaL0nkhAK3C4im0Vkab0nUwUvA3YB1xTSZ1eKyJQkLtyMAu/UzNuoHyIyFbgZuFhV/6fe86kUVR1R1WPI9xN+jYg0bQpNRE4H/qSqm+s9lwRZqKqvAk4FPlJIdzYjE4BXAd9W1QXAXiCRtcVmFHhr5t3AFPLVNwPXq+ot9Z5PEhRel38BvK3OU6mGhcCiQt76R8CJIvKD+k6pOlT1qcK//wTcSj5924zsBHYWvSHeRF7wq6YZBd6aeTcohYXJq4CHVfWr9Z5PNYjIdBHpLPw5B5wMbK/vrCpHVT+lqoer6mzyf2c2qep76jytihGRKYWFfArpjLcCTelEU9U/Ak+KyNzCoZOARIwJqfZkTYNWa+YtIj8E3gwcKiI7gc+q6lX1nVXFLATeCzxYyF0D/EOhN2+zcRhwbcG1lQF6VbXprYUtxF8Ct+ZjCiYAN6jqz+o7par4GHB9IWh9DDg/iYs2nU3SMAzDcKMZUzSGYRiGAybwhmEYLYoJvGEYRotiAm8YhtGimMAbhmG0KCbwRssgIiOFyoIPicj/FZHJVVzrzSKyQUTOL1xzq4jsK6peuDrkuyeKyPEOY/ydiFxe6RwNIwoTeKOVGFLVYwpVOfcBHyr+UPLE+v+8ql5TuOYx5HdMn1D4OWwr+YlApMAbRtqYwButyq+AvxaR2YX62lcAvwFmishbReTfROQ3hUh/Koz1GdguIncDi6MGEJFDRWSdiAyIyK9F5BUiciTwd8DyQqT/OhF5u4jcVygkdbuI/EWaN24YHibwRsshIhPIF6B6sHBoLvl6+14hp88AJxcKVfUDy0RkEvA94AzgDcBfOQz1OeA+VZ0PrAS+r6qPki/Hu6YQ6f8a+CVwfGH8W4CPJ3OnhhFO05UqMIwQckUlEn5Fvi7ODGBHUaOO44GXA/cUtrlPBP4NmAc8rqq/AygU4ooqQft68vXVUdXbReT7AWVeZwG9IvJXwEHkG9YYRuqYwButxFAhVz5GQcT3Fh8iX9v97LLzjiF+2eny0tV+pawBvgV8UVV/IiInk1ApWMOIwlI0RrtxL7BQRP4aQEQmi8j/Jl8pck4hhw5wdtAFivglcE7hOieTL/m6F/gz8JKi86YBg4Vqm+clcxuGEY0JvNFWqOou4H3AD0VkgLzgz1PVF8inZDYWFll3OFzuUuB1hetcxoEKgD8GlhQWVV9HPj9/K3AX8F8J3o5hhGLVJA3DMFoUi+ANwzBaFBN4wzCMFsUE3jAMo0UxgTcMw2hRTOANwzBaFBN4wzCMFsUE3jAMo0X5//juCxyn8nYPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_scl = scaler.fit_transform(X)\n",
    "predTotal = model.predict(X_scl)\n",
    "plt.scatter(predTotal,y)\n",
    "plt.scatter(y,y)\n",
    "plt.xlabel(\"PredTotal\")\n",
    "plt.ylabel('real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predTotal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5d1f5a0561d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbigpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredTotal\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbigpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predTotal' is not defined"
     ]
    }
   ],
   "source": [
    "bigpred = [i for i in predTotal if i >= 5] \n",
    "bigpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs =pd.read_csv(\"../../data/rbs_to_predict.csv\")\n",
    "names = rbs['playername'].values\n",
    "rbs = rbs.drop('tm',axis=1)\n",
    "rbs = rbs.drop('playername',axis=1)\n",
    "for i in range(len(names)):\n",
    "    print(names[i])\n",
    "    print(rbs.iloc[i].to_numpy())\n",
    "    print(model.predict(scaler.transform(rbs.iloc[i].to_numpy().reshape(1, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./savedmodels/April27-norookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  4.500784850833322\n",
      "RMSE:  2.1215053266096984\n",
      "MAE:  1.5205602826493432\n",
      "ESV:  -0.46056120636169307\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('./savedmodels/norookiednn/')\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \",mean_squared_error(y_test,predictions) )\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,predictions)) )\n",
    "print(\"MAE: \",mean_absolute_error(y_test,predictions) )\n",
    "print(\"ESV: \", explained_variance_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9596240325094597\n",
      "MSE: 1.525682738916666\n",
      "RMSE: 1.235185305497384\n",
      "ESV:  0.28225617919754986\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "X_scl = scaler.fit_transform(X)\n",
    "predictions = loaded_model.predict(X_scl)\n",
    "plt.scatter(predictions,y)\n",
    "plt.ylabel(\"Y Test (True Values)\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "print('MAE:', metrics.mean_absolute_error(y, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y, predictions)))\n",
    "\n",
    "print(\"ESV: \", explained_variance_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,x_vars=['age','draft_pos','attempts','yards_run','tds_run','longgain_run','g','gs','tgt','rec','yards_rec','tds_rec','firstdowns','longgain_rec','fumbles','team_adjusted_line_yards','team_running_back_yards','team_stuffed_rate'],y_vars=['Percenthit (%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
