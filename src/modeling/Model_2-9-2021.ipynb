{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../../data/processed/oline_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>year</th>\n",
       "      <th>playername</th>\n",
       "      <th>team</th>\n",
       "      <th>age</th>\n",
       "      <th>draft_yr</th>\n",
       "      <th>draft_pos</th>\n",
       "      <th>attempts</th>\n",
       "      <th>yards_run</th>\n",
       "      <th>...</th>\n",
       "      <th>tds_rec</th>\n",
       "      <th>firstdowns</th>\n",
       "      <th>longgain_rec</th>\n",
       "      <th>yardspertarget</th>\n",
       "      <th>recpergame</th>\n",
       "      <th>yardspergame_rec</th>\n",
       "      <th>fumbles</th>\n",
       "      <th>team_adjusted_line_yards</th>\n",
       "      <th>team_running_back_yards</th>\n",
       "      <th>team_stuffed_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>Aaron Jones</td>\n",
       "      <td>GNB</td>\n",
       "      <td>26</td>\n",
       "      <td>2017</td>\n",
       "      <td>182</td>\n",
       "      <td>75</td>\n",
       "      <td>389</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>Aaron Jones</td>\n",
       "      <td>GNB</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>182</td>\n",
       "      <td>236</td>\n",
       "      <td>1084</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>67</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>Aaron Jones</td>\n",
       "      <td>GNB</td>\n",
       "      <td>24</td>\n",
       "      <td>2017</td>\n",
       "      <td>182</td>\n",
       "      <td>133</td>\n",
       "      <td>728</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>Aaron Jones</td>\n",
       "      <td>GNB</td>\n",
       "      <td>23</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>Adrian Peterson</td>\n",
       "      <td>DET</td>\n",
       "      <td>35</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>314</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>598</td>\n",
       "      <td>598</td>\n",
       "      <td>2016</td>\n",
       "      <td>Wendell Smallwood</td>\n",
       "      <td>PHI</td>\n",
       "      <td>22</td>\n",
       "      <td>2016</td>\n",
       "      <td>153</td>\n",
       "      <td>77</td>\n",
       "      <td>312</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>2013</td>\n",
       "      <td>Willis McGahee</td>\n",
       "      <td>CLE</td>\n",
       "      <td>32</td>\n",
       "      <td>2004</td>\n",
       "      <td>23</td>\n",
       "      <td>138</td>\n",
       "      <td>377</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>2015</td>\n",
       "      <td>Zac Stacy</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>24</td>\n",
       "      <td>2013</td>\n",
       "      <td>160</td>\n",
       "      <td>31</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "      <td>2014</td>\n",
       "      <td>Zac Stacy</td>\n",
       "      <td>STL</td>\n",
       "      <td>23</td>\n",
       "      <td>2013</td>\n",
       "      <td>160</td>\n",
       "      <td>76</td>\n",
       "      <td>293</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>602</td>\n",
       "      <td>602</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zac Stacy</td>\n",
       "      <td>STL</td>\n",
       "      <td>22</td>\n",
       "      <td>2013</td>\n",
       "      <td>160</td>\n",
       "      <td>250</td>\n",
       "      <td>973</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  year         playername team  age  draft_yr  \\\n",
       "0             0             0  2020        Aaron Jones  GNB   26      2017   \n",
       "1             1             1  2019        Aaron Jones  GNB   25      2017   \n",
       "2             2             2  2018        Aaron Jones  GNB   24      2017   \n",
       "3             3             3  2017        Aaron Jones  GNB   23      2017   \n",
       "4             4             4  2020    Adrian Peterson  DET   35      2007   \n",
       "..          ...           ...   ...                ...  ...  ...       ...   \n",
       "583         598           598  2016  Wendell Smallwood  PHI   22      2016   \n",
       "584         599           599  2013     Willis McGahee  CLE   32      2004   \n",
       "585         600           600  2015          Zac Stacy  NYJ   24      2013   \n",
       "586         601           601  2014          Zac Stacy  STL   23      2013   \n",
       "587         602           602  2013          Zac Stacy  STL   22      2013   \n",
       "\n",
       "     draft_pos  attempts  yards_run  ...  tds_rec  firstdowns  longgain_rec  \\\n",
       "0          182        75        389  ...        2          12            30   \n",
       "1          182       236       1084  ...        3          18            67   \n",
       "2          182       133        728  ...        1          12            24   \n",
       "3            7        81        448  ...        0           2             9   \n",
       "4            7        80        314  ...        0           3            18   \n",
       "..         ...       ...        ...  ...      ...         ...           ...   \n",
       "583        153        77        312  ...        0           1            18   \n",
       "584         23       138        377  ...        0           0             9   \n",
       "585        160        31         89  ...        0           4            16   \n",
       "586        160        76        293  ...        0           9            17   \n",
       "587        160       250        973  ...        1           8            25   \n",
       "\n",
       "     yardspertarget  recpergame  yardspergame_rec  fumbles  \\\n",
       "0               5.8         3.7              29.0        2   \n",
       "1               7.0         3.1              29.6        3   \n",
       "2               5.9         2.2              17.2        1   \n",
       "3               1.2         0.8               1.8        0   \n",
       "4               4.9         1.0               7.1        0   \n",
       "..              ...         ...               ...      ...   \n",
       "583             4.2         0.5               4.2        1   \n",
       "584             1.8         0.7               1.7        1   \n",
       "585             5.4         1.1               8.1        0   \n",
       "586             6.6         1.4              11.7        2   \n",
       "587             4.0         1.9              10.1        1   \n",
       "\n",
       "     team_adjusted_line_yards  team_running_back_yards  team_stuffed_rate  \n",
       "0                         5.0                      3.0                1.0  \n",
       "1                         5.0                     13.0                6.0  \n",
       "2                         7.0                      6.0               12.0  \n",
       "3                         5.0                     13.0                2.0  \n",
       "4                        19.0                     22.0               18.0  \n",
       "..                        ...                      ...                ...  \n",
       "583                      13.0                     13.0               12.0  \n",
       "584                      18.0                     29.0               18.0  \n",
       "585                      26.0                     15.0               26.0  \n",
       "586                      18.0                     17.0               29.0  \n",
       "587                      12.0                     21.0               13.0  \n",
       "\n",
       "[588 rows x 35 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('year',axis=1)\n",
    "df = df.drop('playername',axis=1)\n",
    "df = df.drop('team',axis=1)\n",
    "df = df.drop('basesalarycap (10^8)',axis=1)\n",
    "df = df.drop('cashspent (M)',axis=1)\n",
    "df = df.drop('caphit (M)',axis=1)\n",
    "df = df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      73.30%\n",
       "1      72.10%\n",
       "2      74.30%\n",
       "3      50.00%\n",
       "4      68.80%\n",
       "        ...  \n",
       "583    46.20%\n",
       "584    72.70%\n",
       "585    75.00%\n",
       "586    78.30%\n",
       "587    74.30%\n",
       "Name: catchpercent, Length: 588, dtype: object"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"catchpercent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['catchpercent'] = df['catchpercent'].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0.1',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('draft_yr',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>draft_pos</th>\n",
       "      <th>attempts</th>\n",
       "      <th>yards_run</th>\n",
       "      <th>tds_run</th>\n",
       "      <th>longgain_run</th>\n",
       "      <th>yardsperatt</th>\n",
       "      <th>yardspergame_run</th>\n",
       "      <th>Percenthit (%)</th>\n",
       "      <th>g</th>\n",
       "      <th>...</th>\n",
       "      <th>tds_rec</th>\n",
       "      <th>firstdowns</th>\n",
       "      <th>longgain_rec</th>\n",
       "      <th>yardspertarget</th>\n",
       "      <th>recpergame</th>\n",
       "      <th>yardspergame_rec</th>\n",
       "      <th>fumbles</th>\n",
       "      <th>team_adjusted_line_yards</th>\n",
       "      <th>team_running_back_yards</th>\n",
       "      <th>team_stuffed_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>182</td>\n",
       "      <td>75</td>\n",
       "      <td>389</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>5.2</td>\n",
       "      <td>77.8</td>\n",
       "      <td>1.100909</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>182</td>\n",
       "      <td>236</td>\n",
       "      <td>1084</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>4.6</td>\n",
       "      <td>67.8</td>\n",
       "      <td>0.369547</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>67</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>182</td>\n",
       "      <td>133</td>\n",
       "      <td>728</td>\n",
       "      <td>8</td>\n",
       "      <td>67</td>\n",
       "      <td>5.5</td>\n",
       "      <td>60.7</td>\n",
       "      <td>0.293098</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>448</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>5.5</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.308675</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>314</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>3.9</td>\n",
       "      <td>52.3</td>\n",
       "      <td>0.529768</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>22</td>\n",
       "      <td>153</td>\n",
       "      <td>77</td>\n",
       "      <td>312</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4.1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.329198</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>138</td>\n",
       "      <td>377</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>31.4</td>\n",
       "      <td>0.398135</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>24</td>\n",
       "      <td>160</td>\n",
       "      <td>31</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.408291</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>23</td>\n",
       "      <td>160</td>\n",
       "      <td>76</td>\n",
       "      <td>293</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>3.9</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.405357</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>22</td>\n",
       "      <td>160</td>\n",
       "      <td>250</td>\n",
       "      <td>973</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>3.9</td>\n",
       "      <td>69.5</td>\n",
       "      <td>0.365142</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  draft_pos  attempts  yards_run  tds_run  longgain_run  yardsperatt  \\\n",
       "0     26        182        75        389        5            75          5.2   \n",
       "1     25        182       236       1084       16            56          4.6   \n",
       "2     24        182       133        728        8            67          5.5   \n",
       "3     23          7        81        448        4            46          5.5   \n",
       "4     35          7        80        314        2            27          3.9   \n",
       "..   ...        ...       ...        ...      ...           ...          ...   \n",
       "583   22        153        77        312        1            19          4.1   \n",
       "584   32         23       138        377        2            16          2.7   \n",
       "585   24        160        31         89        1            18          2.9   \n",
       "586   23        160        76        293        1            16          3.9   \n",
       "587   22        160       250        973        7            40          3.9   \n",
       "\n",
       "     yardspergame_run  Percenthit (%)   g  ...  tds_rec  firstdowns  \\\n",
       "0                77.8        1.100909   9  ...        2          12   \n",
       "1                67.8        0.369547  16  ...        3          18   \n",
       "2                60.7        0.293098  12  ...        1          12   \n",
       "3                37.3        0.308675  12  ...        0           2   \n",
       "4                52.3        0.529768  11  ...        0           3   \n",
       "..                ...             ...  ..  ...      ...         ...   \n",
       "583              24.0        0.329198  13  ...        0           1   \n",
       "584              31.4        0.398135  12  ...        0           0   \n",
       "585              11.1        0.408291   8  ...        0           4   \n",
       "586              22.5        0.405357  13  ...        0           9   \n",
       "587              69.5        0.365142  14  ...        1           8   \n",
       "\n",
       "     longgain_rec yardspertarget  recpergame  yardspergame_rec  fumbles  \\\n",
       "0              30            5.8         3.7              29.0        2   \n",
       "1              67            7.0         3.1              29.6        3   \n",
       "2              24            5.9         2.2              17.2        1   \n",
       "3               9            1.2         0.8               1.8        0   \n",
       "4              18            4.9         1.0               7.1        0   \n",
       "..            ...            ...         ...               ...      ...   \n",
       "583            18            4.2         0.5               4.2        1   \n",
       "584             9            1.8         0.7               1.7        1   \n",
       "585            16            5.4         1.1               8.1        0   \n",
       "586            17            6.6         1.4              11.7        2   \n",
       "587            25            4.0         1.9              10.1        1   \n",
       "\n",
       "     team_adjusted_line_yards  team_running_back_yards  team_stuffed_rate  \n",
       "0                         5.0                      3.0                1.0  \n",
       "1                         5.0                     13.0                6.0  \n",
       "2                         7.0                      6.0               12.0  \n",
       "3                         5.0                     13.0                2.0  \n",
       "4                        19.0                     22.0               18.0  \n",
       "..                        ...                      ...                ...  \n",
       "583                      13.0                     13.0               12.0  \n",
       "584                      18.0                     29.0               18.0  \n",
       "585                      26.0                     15.0               26.0  \n",
       "586                      18.0                     17.0               29.0  \n",
       "587                      12.0                     21.0               13.0  \n",
       "\n",
       "[588 rows x 26 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('Percenthit (%)',axis=1).values\n",
    "y = df['Percenthit (%)'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 6.9099 - val_loss: 6.3918\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 6.8108 - val_loss: 6.2909\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 6.7029 - val_loss: 6.1843\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 6.5883 - val_loss: 6.0710\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 6.4673 - val_loss: 5.9525\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 6.3447 - val_loss: 5.8206\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 6.2173 - val_loss: 5.6840\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 6.0848 - val_loss: 5.5464\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 5.9427 - val_loss: 5.4078\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 5.7957 - val_loss: 5.2683\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 5.6545 - val_loss: 5.1288\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 5.5078 - val_loss: 4.9874\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.3759 - val_loss: 4.8448\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 5.2315 - val_loss: 4.7037\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 5.0900 - val_loss: 4.5610\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 4.9462 - val_loss: 4.4196\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.8021 - val_loss: 4.2786\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 4.6563 - val_loss: 4.1407\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 4.5311 - val_loss: 4.0052\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 4.3861 - val_loss: 3.8762\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.2676 - val_loss: 3.7521\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 4.1638 - val_loss: 3.6404\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.0510 - val_loss: 3.5445\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.9780 - val_loss: 3.4633\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.9164 - val_loss: 3.4001\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8587 - val_loss: 3.3504\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.8225 - val_loss: 3.3147\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.7989 - val_loss: 3.2906\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.7755 - val_loss: 3.2731\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.7709 - val_loss: 3.2592\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.7641 - val_loss: 3.2464\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.7554 - val_loss: 3.2329\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.7379 - val_loss: 3.2151\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.7189 - val_loss: 3.1958\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.6964 - val_loss: 3.1760\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.6693 - val_loss: 3.1560\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.6440 - val_loss: 3.1370\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.6140 - val_loss: 3.1201\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.5925 - val_loss: 3.1017\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.5698 - val_loss: 3.0842\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.5458 - val_loss: 3.0639\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.5285 - val_loss: 3.0450\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 3.5146 - val_loss: 3.0268\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.4995 - val_loss: 3.0071\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.4836 - val_loss: 2.9859\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.4664 - val_loss: 2.9645\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.4459 - val_loss: 2.9405\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.4234 - val_loss: 2.9173\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4046 - val_loss: 2.8963\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.3864 - val_loss: 2.8781\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.3702 - val_loss: 2.8614\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.3537 - val_loss: 2.8456\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.3367 - val_loss: 2.8305\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.3198 - val_loss: 2.8163\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 3.3058 - val_loss: 2.8036\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.2932 - val_loss: 2.7921\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2796 - val_loss: 2.7794\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.2670 - val_loss: 2.7653\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.2532 - val_loss: 2.7516\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.2398 - val_loss: 2.7357\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.2243 - val_loss: 2.7218\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.2105 - val_loss: 2.7088\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.1986 - val_loss: 2.6963\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1828 - val_loss: 2.6847\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.1694 - val_loss: 2.6737\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.1549 - val_loss: 2.6636\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.1412 - val_loss: 2.6535\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.1291 - val_loss: 2.6425\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.1165 - val_loss: 2.6304\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.1011 - val_loss: 2.6191\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.0895 - val_loss: 2.6083\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.0770 - val_loss: 2.5979\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.0642 - val_loss: 2.5878\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.0522 - val_loss: 2.5787\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.0404 - val_loss: 2.5701\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.0273 - val_loss: 2.5609\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.0161 - val_loss: 2.5503\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.0027 - val_loss: 2.5389\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.9908 - val_loss: 2.5304\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9795 - val_loss: 2.5236\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.9767 - val_loss: 2.5173\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.9622 - val_loss: 2.5073\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9485 - val_loss: 2.4986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.9341 - val_loss: 2.4918\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.9244 - val_loss: 2.4870\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9160 - val_loss: 2.4825\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.9050 - val_loss: 2.4750\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8947 - val_loss: 2.4665\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8806 - val_loss: 2.4599\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8693 - val_loss: 2.4541\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8613 - val_loss: 2.4486\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8494 - val_loss: 2.4430\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8399 - val_loss: 2.4370\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8288 - val_loss: 2.4308\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8174 - val_loss: 2.4244\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8083 - val_loss: 2.4191\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7979 - val_loss: 2.4132\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7878 - val_loss: 2.4069\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7790 - val_loss: 2.4005\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7700 - val_loss: 2.3946\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7538 - val_loss: 2.3888\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7454 - val_loss: 2.3844\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7358 - val_loss: 2.3790\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.7307 - val_loss: 2.3727\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7166 - val_loss: 2.3688\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.7069 - val_loss: 2.3668\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.6990 - val_loss: 2.3641\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6949 - val_loss: 2.3610\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6789 - val_loss: 2.3568\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6750 - val_loss: 2.3538\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6705 - val_loss: 2.3498\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6627 - val_loss: 2.3464\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.6542 - val_loss: 2.3457\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6432 - val_loss: 2.3446\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.636 - 0s 10ms/step - loss: 2.6374 - val_loss: 2.3410\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6337 - val_loss: 2.3350\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.6260 - val_loss: 2.3255\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.6156 - val_loss: 2.3204\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 2.6086 - val_loss: 2.3174\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.6040 - val_loss: 2.3148\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.5980 - val_loss: 2.3159\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.5895 - val_loss: 2.3128\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.5861 - val_loss: 2.3084\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.5796 - val_loss: 2.2994\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.5700 - val_loss: 2.2916\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5634 - val_loss: 2.2860\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5587 - val_loss: 2.2822\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5531 - val_loss: 2.2809\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5478 - val_loss: 2.2837\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5414 - val_loss: 2.2843\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5372 - val_loss: 2.2864\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.5317 - val_loss: 2.2809\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5251 - val_loss: 2.2732\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.5186 - val_loss: 2.2624\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5144 - val_loss: 2.2531\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5086 - val_loss: 2.2473\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5047 - val_loss: 2.2401\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5015 - val_loss: 2.2392\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4969 - val_loss: 2.2485\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4869 - val_loss: 2.2509\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4869 - val_loss: 2.2533\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4794 - val_loss: 2.2411\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4726 - val_loss: 2.2305\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4691 - val_loss: 2.2210\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4628 - val_loss: 2.2198\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4576 - val_loss: 2.2178\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4518 - val_loss: 2.2165\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4460 - val_loss: 2.2170\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4435 - val_loss: 2.2152\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4352 - val_loss: 2.2065\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4318 - val_loss: 2.2001\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4271 - val_loss: 2.1945\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4236 - val_loss: 2.1937\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4196 - val_loss: 2.1986\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.4131 - val_loss: 2.2019\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4075 - val_loss: 2.1988\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4030 - val_loss: 2.1955\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4030 - val_loss: 2.1852\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3984 - val_loss: 2.1867\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3889 - val_loss: 2.1790\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3844 - val_loss: 2.1775\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3796 - val_loss: 2.1753\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3748 - val_loss: 2.1782\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3788 - val_loss: 2.1860\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.3669 - val_loss: 2.1712\n"
=======
      "Epoch 1/250\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.4945WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.9010 - val_loss: 6.3155\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 6.8280 - val_loss: 6.2515\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 6.7581 - val_loss: 6.1856\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 6.6865 - val_loss: 6.1168\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 6.6167 - val_loss: 6.0460\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 6.5429 - val_loss: 5.9729\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 6.4675 - val_loss: 5.8963\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 6.3908 - val_loss: 5.8169\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 6.3096 - val_loss: 5.7350\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 6.2242 - val_loss: 5.6502\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 6.1344 - val_loss: 5.5624\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 6.0432 - val_loss: 5.4697\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.9490 - val_loss: 5.3735\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 5.8511 - val_loss: 5.2743\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 5.7531 - val_loss: 5.1723\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.6495 - val_loss: 5.0676\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.5423 - val_loss: 4.9604\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.4378 - val_loss: 4.8509\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 5.3275 - val_loss: 4.7401\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 5.2225 - val_loss: 4.6285\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 5.1115 - val_loss: 4.5167\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 5.0008 - val_loss: 4.4049\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 4.8885 - val_loss: 4.2944\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 4.7816 - val_loss: 4.1854\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.6784 - val_loss: 4.0791\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.5761 - val_loss: 3.9770\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 4.4742 - val_loss: 3.8798\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.3854 - val_loss: 3.7868\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 4.2995 - val_loss: 3.7004\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.2274 - val_loss: 3.6223\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 4.1522 - val_loss: 3.5544\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 4.0948 - val_loss: 3.4947\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 4.0510 - val_loss: 3.4445\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 4.0092 - val_loss: 3.4043\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.9778 - val_loss: 3.3736\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.9600 - val_loss: 3.3508\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.9464 - val_loss: 3.3347\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.9363 - val_loss: 3.3231\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.9410 - val_loss: 3.3147\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 3.9289 - val_loss: 3.3079\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 3.9278 - val_loss: 3.3013\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.9229 - val_loss: 3.2944\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.9177 - val_loss: 3.2870\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.9105 - val_loss: 3.2792\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.9028 - val_loss: 3.2710\n",
      "Epoch 46/250\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.8934 - val_loss: 3.2627\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.8842 - val_loss: 3.2545\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.8752 - val_loss: 3.2468\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8657 - val_loss: 3.2392\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8567 - val_loss: 3.2317\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8490 - val_loss: 3.2247\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8413 - val_loss: 3.2177\n",
      "Epoch 53/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.8333 - val_loss: 3.2102\n",
      "Epoch 54/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.8258 - val_loss: 3.2028\n",
      "Epoch 55/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.8185 - val_loss: 3.1955\n",
      "Epoch 56/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.8109 - val_loss: 3.1879\n",
      "Epoch 57/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.8043 - val_loss: 3.1799\n",
      "Epoch 58/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.7961 - val_loss: 3.1724\n",
      "Epoch 59/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.7882 - val_loss: 3.1644\n",
      "Epoch 60/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.7804 - val_loss: 3.1558\n",
      "Epoch 61/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.7734 - val_loss: 3.1469\n",
      "Epoch 62/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.7643 - val_loss: 3.1382\n",
      "Epoch 63/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.7555 - val_loss: 3.1293\n",
      "Epoch 64/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.7479 - val_loss: 3.1199\n",
      "Epoch 65/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.7394 - val_loss: 3.1108\n",
      "Epoch 66/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.7300 - val_loss: 3.1019\n",
      "Epoch 67/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.7223 - val_loss: 3.0929\n",
      "Epoch 68/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.7135 - val_loss: 3.0840\n",
      "Epoch 69/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.7049 - val_loss: 3.0751\n",
      "Epoch 70/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.6966 - val_loss: 3.0659\n",
      "Epoch 71/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.6877 - val_loss: 3.0567\n",
      "Epoch 72/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.6791 - val_loss: 3.0474\n",
      "Epoch 73/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.6699 - val_loss: 3.0380\n",
      "Epoch 74/250\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.6600 - val_loss: 3.0283\n",
      "Epoch 75/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.6504 - val_loss: 3.0186\n",
      "Epoch 76/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.6414 - val_loss: 3.0089\n",
      "Epoch 77/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.6317 - val_loss: 2.9995\n",
      "Epoch 78/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.6222 - val_loss: 2.9900\n",
      "Epoch 79/250\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.6123 - val_loss: 2.9805\n",
      "Epoch 80/250\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.6023 - val_loss: 2.9713\n",
      "Epoch 81/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.5923 - val_loss: 2.9627\n",
      "Epoch 82/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.5826 - val_loss: 2.9546\n",
      "Epoch 83/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.5737 - val_loss: 2.9463\n",
      "Epoch 84/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.5657 - val_loss: 2.9375\n",
      "Epoch 85/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.5543 - val_loss: 2.9269\n",
      "Epoch 86/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.5436 - val_loss: 2.9167\n",
      "Epoch 87/250\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.5331 - val_loss: 2.9065\n",
      "Epoch 88/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.5231 - val_loss: 2.8963\n",
      "Epoch 89/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.5132 - val_loss: 2.8863\n",
      "Epoch 90/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.5033 - val_loss: 2.8773\n",
      "Epoch 91/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.4925 - val_loss: 2.8682\n",
      "Epoch 92/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.4824 - val_loss: 2.8590\n",
      "Epoch 93/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.4725 - val_loss: 2.8509\n",
      "Epoch 94/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.4634 - val_loss: 2.8429\n",
      "Epoch 95/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.4515 - val_loss: 2.8339\n",
      "Epoch 96/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4413 - val_loss: 2.8242\n",
      "Epoch 97/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4312 - val_loss: 2.8156\n",
      "Epoch 98/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4206 - val_loss: 2.8078\n",
      "Epoch 99/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4114 - val_loss: 2.8005\n",
      "Epoch 100/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4000 - val_loss: 2.7922\n",
      "Epoch 101/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.3896 - val_loss: 2.7836\n",
      "Epoch 102/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.3797 - val_loss: 2.7760\n",
      "Epoch 103/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.3688 - val_loss: 2.7694\n",
      "Epoch 104/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.3596 - val_loss: 2.7613\n",
      "Epoch 105/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.3500 - val_loss: 2.7525\n",
      "Epoch 106/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.3394 - val_loss: 2.7423\n",
      "Epoch 107/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.3292 - val_loss: 2.7339\n",
      "Epoch 108/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.3201 - val_loss: 2.7269\n",
      "Epoch 109/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.3084 - val_loss: 2.7186\n",
      "Epoch 110/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.2990 - val_loss: 2.7108\n",
      "Epoch 111/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.2879 - val_loss: 2.7034\n",
      "Epoch 112/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2774 - val_loss: 2.6997\n",
      "Epoch 113/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2641 - val_loss: 2.6959\n",
      "Epoch 114/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2535 - val_loss: 2.6936\n",
      "Epoch 115/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.2374 - val_loss: 2.6864\n",
      "Epoch 116/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2259 - val_loss: 2.6858\n",
      "Epoch 117/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2109 - val_loss: 2.6806\n",
      "Epoch 118/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1986 - val_loss: 2.6804\n",
      "Epoch 119/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.1843 - val_loss: 2.6735\n",
      "Epoch 120/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1704 - val_loss: 2.6620\n",
      "Epoch 121/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1566 - val_loss: 2.6553\n",
      "Epoch 122/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1426 - val_loss: 2.6530\n",
      "Epoch 123/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1276 - val_loss: 2.6513\n",
      "Epoch 124/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1161 - val_loss: 2.6500\n",
      "Epoch 125/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.1028 - val_loss: 2.6438\n",
      "Epoch 126/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.0910 - val_loss: 2.6377\n",
      "Epoch 127/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.0829 - val_loss: 2.6326\n",
      "Epoch 128/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.0685 - val_loss: 2.6151\n",
      "Epoch 129/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.0574 - val_loss: 2.6058\n",
      "Epoch 130/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.0470 - val_loss: 2.6020\n",
      "Epoch 131/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.0361 - val_loss: 2.6065\n",
      "Epoch 132/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.0255 - val_loss: 2.6216\n",
      "Epoch 133/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.0138 - val_loss: 2.6306\n",
      "Epoch 134/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.0056 - val_loss: 2.6363\n",
      "Epoch 135/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.9976 - val_loss: 2.6340\n",
      "Epoch 136/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.9881 - val_loss: 2.6158\n",
      "Epoch 137/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.9774 - val_loss: 2.6038\n",
      "Epoch 138/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.9680 - val_loss: 2.5981\n",
      "Epoch 139/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.9579 - val_loss: 2.6012\n",
      "Epoch 140/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9474 - val_loss: 2.5977\n",
      "Epoch 141/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9402 - val_loss: 2.5926\n",
      "Epoch 142/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.9304 - val_loss: 2.5938\n",
      "Epoch 143/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9243 - val_loss: 2.6054\n",
      "Epoch 144/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.9132 - val_loss: 2.5953\n",
      "Epoch 145/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.9040 - val_loss: 2.5876\n",
      "Epoch 146/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8947 - val_loss: 2.5703\n",
      "Epoch 147/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8867 - val_loss: 2.5586\n",
      "Epoch 148/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8804 - val_loss: 2.5443\n",
      "Epoch 149/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8718 - val_loss: 2.5482\n",
      "Epoch 150/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8611 - val_loss: 2.5670\n",
      "Epoch 151/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8514 - val_loss: 2.5902\n",
      "Epoch 152/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8440 - val_loss: 2.6187\n",
      "Epoch 153/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8391 - val_loss: 2.6313\n",
      "Epoch 154/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8319 - val_loss: 2.6206\n",
      "Epoch 155/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8205 - val_loss: 2.5851\n",
      "Epoch 156/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8123 - val_loss: 2.5470\n",
      "Epoch 157/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8035 - val_loss: 2.5356\n",
      "Epoch 158/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.7966 - val_loss: 2.5197\n",
      "Epoch 159/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7873 - val_loss: 2.5348\n",
      "Epoch 160/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.7756 - val_loss: 2.5501\n",
      "Epoch 161/250\n"
>>>>>>> 05f78e54f0e75aa908a25de4444b70e6d87ca310
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.3629 - val_loss: 2.1635\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3558 - val_loss: 2.1629\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3594 - val_loss: 2.1652\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3586 - val_loss: 2.1501\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3430 - val_loss: 2.1562\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3396 - val_loss: 2.1615\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3347 - val_loss: 2.1560\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3282 - val_loss: 2.1465\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3245 - val_loss: 2.1375\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3236 - val_loss: 2.1329\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3210 - val_loss: 2.1350\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3176 - val_loss: 2.1485\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3084 - val_loss: 2.1547\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3037 - val_loss: 2.1679\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3018 - val_loss: 2.1664\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2981 - val_loss: 2.1593\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2921 - val_loss: 2.1389\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2856 - val_loss: 2.1264\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2819 - val_loss: 2.1197\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2793 - val_loss: 2.1203\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.506 - 0s 14ms/step - loss: 2.2735 - val_loss: 2.1254\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2712 - val_loss: 2.1339\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2656 - val_loss: 2.1298\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2607 - val_loss: 2.1220\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2561 - val_loss: 2.1165\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2568 - val_loss: 2.1136\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2486 - val_loss: 2.1300\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2439 - val_loss: 2.1387\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2422 - val_loss: 2.1361\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2391 - val_loss: 2.1255\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2316 - val_loss: 2.1224\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2373 - val_loss: 2.1218\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2214 - val_loss: 2.0943\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2214 - val_loss: 2.0800\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2224 - val_loss: 2.0875\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2115 - val_loss: 2.0940\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2036 - val_loss: 2.1079\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1998 - val_loss: 2.1256\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1915 - val_loss: 2.1206\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1861 - val_loss: 2.1040\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1833 - val_loss: 2.0954\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1819 - val_loss: 2.1059\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1721 - val_loss: 2.0981\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1690 - val_loss: 2.0779\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1622 - val_loss: 2.0741\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1565 - val_loss: 2.0766\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1506 - val_loss: 2.0862\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1514 - val_loss: 2.0990\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1438 - val_loss: 2.0852\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1360 - val_loss: 2.0649\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1373 - val_loss: 2.0466\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1356 - val_loss: 2.0501\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.1224 - val_loss: 2.0785\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1237 - val_loss: 2.1124\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1230 - val_loss: 2.0976\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1179 - val_loss: 2.0570\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.1089 - val_loss: 2.0346\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.1048 - val_loss: 2.0333\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0990 - val_loss: 2.0401\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0949 - val_loss: 2.0495\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0924 - val_loss: 2.0661\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.0869 - val_loss: 2.0589\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.0816 - val_loss: 2.0608\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0779 - val_loss: 2.0541\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0737 - val_loss: 2.0350\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0686 - val_loss: 2.0350\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0686 - val_loss: 2.0362\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.0624 - val_loss: 2.0634\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0571 - val_loss: 2.0600\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0521 - val_loss: 2.0362\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0461 - val_loss: 2.0210\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0468 - val_loss: 2.0037\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.0394 - val_loss: 2.0182\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0392 - val_loss: 2.0336\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0299 - val_loss: 2.0164\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0291 - val_loss: 1.9925\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0203 - val_loss: 1.9993\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0141 - val_loss: 2.0121\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0123 - val_loss: 2.0318\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0070 - val_loss: 2.0224\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0031 - val_loss: 2.0110\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0040 - val_loss: 1.9876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9939 - val_loss: 1.9985\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9880 - val_loss: 2.0015\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9843 - val_loss: 2.0031\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9820 - val_loss: 2.0056\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.9769 - val_loss: 1.9875\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9726 - val_loss: 1.9847\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9691 - val_loss: 1.9899\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9671 - val_loss: 1.9954\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9641 - val_loss: 1.9870\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9569 - val_loss: 1.9960\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9537 - val_loss: 1.9987\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9482 - val_loss: 1.9788\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9444 - val_loss: 1.9581\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9431 - val_loss: 1.9594\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9385 - val_loss: 1.9812\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9323 - val_loss: 1.9951\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9309 - val_loss: 2.0001\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9302 - val_loss: 1.9777\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9220 - val_loss: 1.9378\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.9334 - val_loss: 1.9301\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9260 - val_loss: 1.9675\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9111 - val_loss: 1.9909\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.9102 - val_loss: 1.9846\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.9100 - val_loss: 1.9691\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9069 - val_loss: 1.9309\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8992 - val_loss: 1.9365\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8916 - val_loss: 1.9601\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8874 - val_loss: 1.9824\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8937 - val_loss: 1.9934\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8888 - val_loss: 1.9510\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8775 - val_loss: 1.9346\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8761 - val_loss: 1.9337\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8715 - val_loss: 1.9318\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8674 - val_loss: 1.9312\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8662 - val_loss: 1.9343\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8585 - val_loss: 1.9637\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8604 - val_loss: 1.9684\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8531 - val_loss: 1.9401\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8571 - val_loss: 1.9104\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8473 - val_loss: 1.9289\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.8465 - val_loss: 1.9578\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8394 - val_loss: 1.9452\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.8334 - val_loss: 1.9310\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8301 - val_loss: 1.9201\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8286 - val_loss: 1.9204\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8257 - val_loss: 1.9207\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.8207 - val_loss: 1.9426\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8174 - val_loss: 1.9628\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8182 - val_loss: 1.9683\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.8149 - val_loss: 1.9352\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.8091 - val_loss: 1.9015\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.8096 - val_loss: 1.8868\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.8076 - val_loss: 1.8868\n"
=======
      "2/2 [==============================] - 0s 8ms/step - loss: 2.7627 - val_loss: 2.5864\n",
      "Epoch 162/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7578 - val_loss: 2.6267\n",
      "Epoch 163/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7484 - val_loss: 2.6327\n",
      "Epoch 164/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7408 - val_loss: 2.6258\n",
      "Epoch 165/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7323 - val_loss: 2.5966\n",
      "Epoch 166/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.7200 - val_loss: 2.5864\n",
      "Epoch 167/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7107 - val_loss: 2.5725\n",
      "Epoch 168/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7011 - val_loss: 2.5613\n",
      "Epoch 169/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6889 - val_loss: 2.5554\n",
      "Epoch 170/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6774 - val_loss: 2.5579\n",
      "Epoch 171/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6641 - val_loss: 2.5831\n",
      "Epoch 172/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6509 - val_loss: 2.5807\n",
      "Epoch 173/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6321 - val_loss: 2.5535\n",
      "Epoch 174/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6228 - val_loss: 2.5094\n",
      "Epoch 175/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6172 - val_loss: 2.5239\n",
      "Epoch 176/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5996 - val_loss: 2.5938\n",
      "Epoch 177/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5921 - val_loss: 2.6870\n",
      "Epoch 178/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5942 - val_loss: 2.7014\n",
      "Epoch 179/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5865 - val_loss: 2.6241\n",
      "Epoch 180/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5670 - val_loss: 2.5469\n",
      "Epoch 181/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5613 - val_loss: 2.4938\n",
      "Epoch 182/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5520 - val_loss: 2.4973\n",
      "Epoch 183/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5428 - val_loss: 2.5419\n",
      "Epoch 184/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5341 - val_loss: 2.5644\n",
      "Epoch 185/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5283 - val_loss: 2.5513\n",
      "Epoch 186/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5197 - val_loss: 2.5146\n",
      "Epoch 187/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5135 - val_loss: 2.4833\n",
      "Epoch 188/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5065 - val_loss: 2.4859\n",
      "Epoch 189/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4985 - val_loss: 2.4981\n",
      "Epoch 190/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4925 - val_loss: 2.5172\n",
      "Epoch 191/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4888 - val_loss: 2.5236\n",
      "Epoch 192/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4806 - val_loss: 2.4805\n",
      "Epoch 193/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4749 - val_loss: 2.4646\n",
      "Epoch 194/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4694 - val_loss: 2.4907\n",
      "Epoch 195/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4643 - val_loss: 2.5585\n",
      "Epoch 196/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4630 - val_loss: 2.5478\n",
      "Epoch 197/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4514 - val_loss: 2.4586\n",
      "Epoch 198/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4500 - val_loss: 2.3976\n",
      "Epoch 199/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4424 - val_loss: 2.4320\n",
      "Epoch 200/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4322 - val_loss: 2.4876\n",
      "Epoch 201/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4398 - val_loss: 2.5413\n",
      "Epoch 202/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4289 - val_loss: 2.4704\n",
      "Epoch 203/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4265 - val_loss: 2.3907\n",
      "Epoch 204/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4163 - val_loss: 2.3970\n",
      "Epoch 205/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4094 - val_loss: 2.4013\n",
      "Epoch 206/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4038 - val_loss: 2.4212\n",
      "Epoch 207/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3973 - val_loss: 2.4510\n",
      "Epoch 208/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3941 - val_loss: 2.4821\n",
      "Epoch 209/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3889 - val_loss: 2.5296\n",
      "Epoch 210/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3909 - val_loss: 2.5246\n",
      "Epoch 211/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3807 - val_loss: 2.4346\n",
      "Epoch 212/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3848 - val_loss: 2.3396\n",
      "Epoch 213/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3781 - val_loss: 2.3755\n",
      "Epoch 214/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3676 - val_loss: 2.4238\n",
      "Epoch 215/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3594 - val_loss: 2.4952\n",
      "Epoch 216/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3607 - val_loss: 2.5237\n",
      "Epoch 217/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3584 - val_loss: 2.4713\n",
      "Epoch 218/250\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.3496 - val_loss: 2.4419\n",
      "Epoch 219/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3486 - val_loss: 2.3792\n",
      "Epoch 220/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3448 - val_loss: 2.3859\n",
      "Epoch 221/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3388 - val_loss: 2.3985\n",
      "Epoch 222/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.3328 - val_loss: 2.4419\n",
      "Epoch 223/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3278 - val_loss: 2.4597\n",
      "Epoch 224/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3247 - val_loss: 2.4260\n",
      "Epoch 225/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3194 - val_loss: 2.4037\n",
      "Epoch 226/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3166 - val_loss: 2.3789\n",
      "Epoch 227/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3143 - val_loss: 2.3318\n",
      "Epoch 228/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.3095 - val_loss: 2.3581\n",
      "Epoch 229/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3042 - val_loss: 2.3974\n",
      "Epoch 230/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3023 - val_loss: 2.4727\n",
      "Epoch 231/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2984 - val_loss: 2.4564\n",
      "Epoch 232/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2928 - val_loss: 2.4207\n",
      "Epoch 233/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2876 - val_loss: 2.3754\n",
      "Epoch 234/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2839 - val_loss: 2.3675\n",
      "Epoch 235/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2834 - val_loss: 2.3810\n",
      "Epoch 236/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2766 - val_loss: 2.3572\n",
      "Epoch 237/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2743 - val_loss: 2.3156\n",
      "Epoch 238/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.781 - 0s 9ms/step - loss: 2.2714 - val_loss: 2.3498\n",
      "Epoch 239/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2633 - val_loss: 2.3704\n",
      "Epoch 240/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2587 - val_loss: 2.3757\n",
      "Epoch 241/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2583 - val_loss: 2.3897\n",
      "Epoch 242/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2524 - val_loss: 2.3495\n",
      "Epoch 243/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2461 - val_loss: 2.3580\n",
      "Epoch 244/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2429 - val_loss: 2.3825\n",
      "Epoch 245/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2502 - val_loss: 2.4355\n",
      "Epoch 246/250\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.2355 - val_loss: 2.3724\n",
      "Epoch 247/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2283 - val_loss: 2.3362\n",
      "Epoch 248/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2268 - val_loss: 2.3136\n",
      "Epoch 249/250\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.2236 - val_loss: 2.3400\n",
      "Epoch 250/250\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.2171 - val_loss: 2.3973\n"
>>>>>>> 05f78e54f0e75aa908a25de4444b70e6d87ca310
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2726beb5088>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(x=X_train, y= y_train, validation_data=(X_test,y_test),batch_size=256,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2726be56408>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1.886780414916811\n",
      "RMSE:  1.3736012576132897\n",
      "MAE:  0.9162257351604476\n",
      "EVS:  0.47321461231332496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(\"MSE: \",mean_squared_error(y_test,predictions) )\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,predictions)) )\n",
    "print(\"MAE: \",mean_absolute_error(y_test,predictions) )\n",
    "\n",
    "print(\"EVS: \", explained_variance_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
